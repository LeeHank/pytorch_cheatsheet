

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>1. Pytorch Cheatsheet &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'cheatsheet/pytorch_cheatsheet';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. VGG16" href="../classical_network/vgg/vgg16.html" />
    <link rel="prev" title="Welcome to your Jupyter Book" href="../intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    <p class="title logo__title">My sample book</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Cheat Sheet</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">1. Pytorch Cheatsheet</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Classical Network</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../classical_network/vgg/vgg16.html">2. VGG16</a></li>
<li class="toctree-l1"><a class="reference internal" href="../classical_network/resnet/resnet.html">3. Resnet</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fcheatsheet/pytorch_cheatsheet.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/cheatsheet/pytorch_cheatsheet.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Pytorch Cheatsheet</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors">1.1. Tensors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-attribute">1.1.1. tensor 與四個重要 attribute</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-type">1.1.1.1. data type</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#device">1.1.1.2. device</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient">1.1.1.3. 關閉 gradient 計算</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-obj-detach">1.1.1.3.1. tensor_obj.detach()</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#with-no-grad">1.1.1.3.2. with no_grad</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor">1.1.2. 100 種 建立 tensor 的方式</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-empty-torch-zeros-torch-ones-torch-rand">1.1.2.1. torch.empty(), torch.zeros(), torch.ones(), torch.rand()</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-manual-seed">1.1.2.2. torch.manual_seed()</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-empty-like-torch-zeros-like-torch-ones-like-torch-rand-like">1.1.2.3. torch.empty_like(), torch.zeros_like(), torch.ones_like(), torch.rand_like()</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-obj-clone">1.1.3. tensor_obj.clone()</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-from-numpy-np-array-and-tensor-obj-clone-numpy">1.1.4. <code class="docutils literal notranslate"><span class="pre">torch.from_numpy(np_array)</span></code> and <code class="docutils literal notranslate"><span class="pre">tensor_obj.clone().numpy()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reshape">1.1.5. reshape</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-reshape">1.1.5.1. torch.reshape()</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#unsqueeze-squeeze">1.1.5.2. unsqueeze (增軸) 與 squeeze (減軸)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stack-tensor">1.1.5.3. stack 組合 tensor (會增軸)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#permute">1.1.5.4. permute 換通道</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-element">1.1.6. 對 tensor 的每個 element 做運算</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">1.1.6.1. 加, 減, 乘, 除, 開根號, 次方, …</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.1.6.2. 取絕對值, 取整數, 截斷, …</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1.1.6.3. 三角函數 與 反函數</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">1.1.6.4. 比較兩 tensor 是否相等</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-summarise">1.1.7. 對 tensor 做 summarise</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">1.1.7.1. 取最大最小值, 平均, 標準差…</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-operation">1.1.8. matrix operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-linear-algebra">1.1.9. 對 tensor 做 linear algebra</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-loss-fn-optimizer">1.2. 自動微分(model, loss_fn 和 optimizer 如何協作)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">1.3. Data preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-class">1.3.1. Dataset - 自訂 class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-tensordataset">1.3.2. Dataset - 直接用 <code class="docutils literal notranslate"><span class="pre">TensorDataset</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataloader">1.3.3. DataLoader</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">1.3.4. 內建 dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">1.3.4.1. 圖片類</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#transform">1.3.4.1.1. 無 transform</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">1.3.4.1.2. 有 transform</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transforms">1.4. Transforms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">1.5. activation functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">1.5.1. 內建</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#relu">1.5.1.1. ReLU</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid">1.5.1.2. Sigmoid</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">1.5.2. 自訂</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-layers-block">1.6. custom layers &amp; block</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-layer">1.6.1. custom layer (不帶參數)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">1.6.2. custom layer (帶參數)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-block-nn-sequential-layer1-block2">1.6.3. sequential block (<code class="docutils literal notranslate"><span class="pre">nn.Sequential(layer1,</span> <span class="pre">block2,</span> <span class="pre">...)</span></code>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-for-tips">1.6.4. sequential <code class="docutils literal notranslate"><span class="pre">for</span></code> tips</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-block">1.6.5. custom block</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model">1.6.6. 經典 model 自己寫系列</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#vgg11">1.6.6.1. VGG11</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#vgg-block">1.6.6.1.1. VGG block</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">1.6.6.1.2. 讓 VGG block 疊高高</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#flatten-layer">1.6.6.1.3. flatten layer</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#classifier-layer">1.6.6.1.4. classifier layer</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">1.6.6.1.5. 全部組起來</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet18">1.6.6.2. Resnet18</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet-block">1.6.6.2.1. resnet block</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet-network">1.6.6.2.2. resnet network</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">1.6.7. model 手術</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#module">1.6.7.1. 往後疊加 module</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#seq-obj-append-module-module">1.6.7.1.1. 用 <code class="docutils literal notranslate"><span class="pre">Seq_obj.append(module)</span></code> 來增加 module</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#seq-obj-add-module-name-module-module">1.6.7.1.2. 用 <code class="docutils literal notranslate"><span class="pre">Seq_obj.add_module("name",</span> <span class="pre">module)</span></code> 來增加帶有名稱的module</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">1.7. model 結構/參數管理</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-structure">1.7.1. 看 model structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-bias-gradient">1.7.2. 看單一層的 weight, bias, gradient</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#state-dict">1.7.2.1. <code class="docutils literal notranslate"><span class="pre">.state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-weight-data-weight-grad">1.7.2.2. <code class="docutils literal notranslate"><span class="pre">.weight</span></code>, <code class="docutils literal notranslate"><span class="pre">.weight.data</span></code>, <code class="docutils literal notranslate"><span class="pre">.weight.grad</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-bias-data-bias-grad">1.7.2.3. <code class="docutils literal notranslate"><span class="pre">.bias</span></code>, <code class="docutils literal notranslate"><span class="pre">.bias.data</span></code>, <code class="docutils literal notranslate"><span class="pre">.bias.grad</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">1.7.2.4. <code class="docutils literal notranslate"><span class="pre">.parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#named-parameters">1.7.2.5. <code class="docutils literal notranslate"><span class="pre">.named_parameters</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-weight-bias-gradient">1.7.3. 看所有的 parameters, weight, bias, gradient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#block-factory">1.7.4. block factory</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">1.8. 參數初始化</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">1.9. Transfer learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-classification">1.9.1. image classification</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">1.9.1.1. vgg11</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">1.9.1.1.1. 直接用</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#featrue-extraction">1.9.1.1.2. 僅作 featrue extraction</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">1.9.1.1.3. 把最後一層分類層，換掉</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classical-layers">1.10. Classical Layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nn">1.10.1. NN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-linear-in-dim-out-dim">1.10.1.1. <code class="docutils literal notranslate"><span class="pre">nn.Linear(in_dim,</span> <span class="pre">out_dim)</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flatten">1.10.2. Flatten</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-nn-flatten-start-dim-1-end-dim-1">1.10.2.1. <code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten(start_dim=1,</span> <span class="pre">end_dim=-</span> <span class="pre">1)</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">1.10.3. Dropout</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-dropout-p-0-2">1.10.3.1. <code class="docutils literal notranslate"><span class="pre">nn.Dropout(p=0.2)</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-dropout2d-p-0-2">1.10.3.2. <code class="docutils literal notranslate"><span class="pre">nn.Dropout2d(p=0.2)</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">1.10.4. Batch Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-batchnorm2d">1.10.4.1. <code class="docutils literal notranslate"><span class="pre">nn.BatchNorm2d()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">1.10.5. Layer Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn">1.10.6. CNN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution">1.10.6.1. convolution</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-conv2d">1.10.6.1.1. <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#d-convolution">1.10.6.1.2. 1d convolution</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling">1.10.6.2. pooling</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-maxpool2d">1.10.6.2.1. <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-avgpool2d">1.10.6.2.2. <code class="docutils literal notranslate"><span class="pre">nn.AvgPool2d()</span></code>.</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-adaptiveavgpool2d">1.10.6.2.3. <code class="docutils literal notranslate"><span class="pre">nn.AdaptiveAvgPool2d()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-adaptivemaxpool2d">1.10.6.2.4. <code class="docutils literal notranslate"><span class="pre">nn.AdaptiveMaxPool2d()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn">1.10.7. RNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">1.10.8. Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">1.10.9. Transformer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss">1.11. Loss</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">1.11.1. overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mse">1.11.2. mse</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#class">1.11.2.1. class 版本</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#function">1.11.2.2. function 版本</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mae">1.11.3. mae</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy">1.11.4. binary cross entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">1.11.4.1. class 版本</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">1.11.4.2. function 版</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy-with-logits">1.11.5. binary cross entropy with logits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy">1.11.6. cross-entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">1.11.7. 自訂 loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">1.11.8. 對比學習</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoder">1.11.9. autoencoder</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer">1.12. Optimizer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">1.12.1. 建立 optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate">1.12.2. 不同 learning rate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-scheduler">1.12.3. learning rate scheduler</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loops">1.13. Training loops</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">1.13.1. 完整版 (了解概念用)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy">1.13.2. 模組版 (實際做實驗, deploy 時用)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">1.13.3. CNN</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction">1.14. Prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">1.15. Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#save-load-model">1.16. Save/ load model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight">1.16.1. 只存 weight</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-model-structure">1.16.2. 存 weight 和 model structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoints">1.16.3. checkpoints</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#callbacks">1.17. callbacks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">1.17.1. Early stopping</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization">1.18. Visualization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explaianation">1.19. Explaianation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="pytorch-cheatsheet">
<h1><span class="section-number">1. </span>Pytorch Cheatsheet<a class="headerlink" href="#pytorch-cheatsheet" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>settings</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from google.colab import drive
drive.mount(&#39;/content/drive&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mounted at /content/drive
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import os
os.chdir(&quot;/content/drive/MyDrive/0. codepool_python/python_dl/mybook/pytorch&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import math
from collections import OrderedDict

import matplotlib.pyplot as plt

import torch
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import TensorDataset

import torch.nn as nn
import torch.nn.functional as F

import torchvision
from torchvision import transforms

from torchinfo import summary
</pre></div>
</div>
</div>
</div>
<section id="tensors">
<h2><span class="section-number">1.1. </span>Tensors<a class="headerlink" href="#tensors" title="Permalink to this heading">#</a></h2>
<section id="tensor-attribute">
<h3><span class="section-number">1.1.1. </span>tensor 與四個重要 attribute<a class="headerlink" href="#tensor-attribute" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>講到 tensor，就想到：</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor</span> <span class="pre">是可以在指定</span> <span class="pre">device</span> <span class="pre">(cpu</span> <span class="pre">/</span> <span class="pre">gpu)</span> <span class="pre">上，進行</span> <span class="pre">forward</span> <span class="pre">(一般</span> <span class="pre">array</span> <span class="pre">計算)</span> <span class="pre">和</span> <span class="pre">backward</span> <span class="pre">(求gradient)</span> <span class="pre">計算的</span> <span class="pre">numpy</span> <span class="pre">array</span></code></p></li>
</ul>
</li>
<li><p>所以，在建立 pytorch tensor 時，要注意以下四個 component:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">shape</span></code> (例如是 shape = (3,224,224) 的 img )</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dtype</span></code>: array 中每個 element 的 data type 是什麼？ e.g. float32, int32,…</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device</span></code>: 這個 tensor 即將在哪個 device 上做計算? (e.g. cpu? gpu?)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">requires_grad</span></code>: 這個 tensor 是否要記錄將來計算 gradient 會用到的訊息</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>看一下例子：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Default datatype for tensors is float32
float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                               dtype=torch.float32, # defaults to None, which is torch.float32 or whatever datatype is passed
                               device=torch.device(&quot;cpu&quot;), # defaults to None, which uses the default tensor type
                               requires_grad=True) # defaults to False. if True, operations performed on the tensor are recorded 

print(float_32_tensor)
print(float_32_tensor.shape) 
print(float_32_tensor.dtype)
print(float_32_tensor.device)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([3., 6., 9.], requires_grad=True)
torch.Size([3])
torch.float32
cpu
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>新手在 pytorch &amp; deep learning 的 coding 時最容易出現以下四種 error：</p>
<ul>
<li><p>shape 錯誤 (e.g. 兩個 tensor 的 shape 無法相乘 )</p></li>
<li><p>dtype 錯誤 (e.g. 拿 float32 和 int32 的 tensor 做計算)</p></li>
<li><p>device 錯誤 (e.g. 兩個 tensor 所處的 device 不同)</p></li>
<li><p>gradient 沒有適時關閉/開啟，影響到 backward calculation</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>有關 shape 的錯誤，這牽扯矩陣計算的知識，所以這裡沒啥好補充的</p></li>
<li><p>dtype, device, 和 gradient 的設定，是常常會遇到的，以下馬上做整理</p></li>
</ul>
<section id="data-type">
<h4><span class="section-number">1.1.1.1. </span>data type<a class="headerlink" href="#data-type" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>最常見(也是預設的) 就是 torch.float32，這邊會碰到 precision in computing 的概念，只要記得：</p>
<ul>
<li><p>float32: 是 single precision floating point，float32 是指 32 bit floating point (32 bit in memory)，也就是可以存 2^32 -1 個數字.</p></li>
<li><p>float16 是 半精度，犧牲一點 precision，但加快計算速度</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>其他常見的 dtype，包括</p>
<ul>
<li><p>torch.bool</p></li>
<li><p>torch.int8</p></li>
<li><p>torch.uint8</p></li>
<li><p>torch.int16</p></li>
<li><p>torch.int32</p></li>
<li><p>torch.int64</p></li>
<li><p>torch.half</p></li>
<li><p>torch.float32</p></li>
<li><p>torch.float16</p></li>
<li><p>torch.float64</p></li>
<li><p>torch.double</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>建立想要的 dtype 的 3 種方式:</p>
<ul>
<li><p>用資料讓 pytorch 自行判斷 (給整數/小數 …，如果給 3.0，那就會判定為 float32)</p></li>
<li><p>建立 tensor 時，指定 dtype (e.g. <code class="docutils literal notranslate"><span class="pre">torch.tensor(1,</span> <span class="pre">dtype=torch.float32)</span></code>)</p></li>
<li><p>轉換 dtype (e.g <code class="docutils literal notranslate"><span class="pre">int_tensor</span> <span class="pre">=</span> <span class="pre">torch.tensor(87);</span> <span class="pre">float_tensor</span> <span class="pre">=</span> <span class="pre">int_tensor.to(torch.float32)</span></code>)</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 直接給小數點

some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])
print(some_constants)

some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))
print(some_integers)

more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))
print(more_integers)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[3.1416, 2.7183],
        [1.6180, 0.0073]])
tensor([ 2,  3,  5,  7, 11, 13, 17, 19])
tensor([[2, 4, 6],
        [3, 6, 9]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 指定 type
a = torch.ones((2, 3), dtype=torch.int16)
print(a)

b = torch.rand((2, 3), dtype=torch.float64) * 20.
print(b)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 1, 1],
        [1, 1, 1]], dtype=torch.int16)
tensor([[15.1821,  6.0043,  8.3599],
        [ 4.2249,  6.5844, 17.2819]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 轉換 type
c = b.to(torch.int32)
print(c)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[11, 11, 11],
        [18,  0, 18]], dtype=torch.int32)
</pre></div>
</div>
</div>
</div>
</section>
<section id="device">
<h4><span class="section-number">1.1.1.2. </span>device<a class="headerlink" href="#device" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>設定 device</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available else torch.device(&#39;cpu&#39;)
device
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>device(type=&#39;cuda&#39;)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>建立時直接指定 device</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = torch.rand(2, 2, device = device)
x
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0810, 0.1305],
        [0.0693, 0.4869]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>確認 目前變數的 device</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x.device
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>device(type=&#39;cuda&#39;, index=0)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>轉換 device</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y1 = torch.rand(2, 2)
print(y1.device)

y2 = y1.to(device)
print(y2.device)

y3 = y2.to(torch.device(&quot;cpu&quot;))
print(y3.device)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>cpu
cuda:0
cpu
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>tensor 做計算時，必須在同一個 device 上才能算 (都在 GPU or 都在 CPU)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = torch.rand(2, 2)
y = torch.rand(2, 2, device=&#39;gpu&#39;)
z = x + y  # exception will be thrown
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">12</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;gpu&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>  <span class="c1"># exception will be thrown</span>

<span class="ne">RuntimeError</span>: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, ort, mps, xla, lazy, vulkan, meta, hpu, privateuseone device type at start of device string: gpu
</pre></div>
</div>
</div>
</div>
</section>
<section id="gradient">
<h4><span class="section-number">1.1.1.3. </span>關閉 gradient 計算<a class="headerlink" href="#gradient" title="Permalink to this heading">#</a></h4>
<section id="tensor-obj-detach">
<h5><span class="section-number">1.1.1.3.1. </span>tensor_obj.detach()<a class="headerlink" href="#tensor-obj-detach" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>.detach() 的意思，主要就是刪掉 gradient 紀錄</p></li>
<li><p>這主要是用在：</p>
<ul>
<li><p>NN 計算到一半時，你想拿某個中間產物，出去算一些暫時的結果，然後再回來.</p></li>
<li><p>這時，你不希望中間跑出去算的哪些過程，也被記錄下來，導致去做 backpropagation 時，還會更新到那些 gradient，進而影想到真正的 variable 的 gradient</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>a = torch.rand(2, 2, requires_grad=True) # turn on autograd
print(a)

b = a.clone()
print(b)

c = a.detach().clone()
print(c)

print(a)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.7399, 0.9776],
        [0.9391, 0.1434]], requires_grad=True)
tensor([[0.7399, 0.9776],
        [0.9391, 0.1434]], grad_fn=&lt;CloneBackward0&gt;)
tensor([[0.7399, 0.9776],
        [0.9391, 0.1434]])
tensor([[0.7399, 0.9776],
        [0.9391, 0.1434]], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>解釋一下這邊發生的事：</p>
<ul>
<li><p>我們用 <code class="docutils literal notranslate"><span class="pre">requires_grad</span> <span class="pre">=</span> <span class="pre">True</span></code> 建立了 a ，所以去 print(a) 時，他告訴我們： requires_grad = True，表示 autograd 和 computation history tracking 都有被 turn on.</p></li>
<li><p>當我們單純把 a clone 到 b 時，他不僅繼承了 a 的 requires_grad，他也記錄了你的這次 computation history: clone，所以寫成 CloneBackward.</p></li>
<li><p>但如果我們先把 a detach，再把 a clone 給 c，就可以發現 c 乾乾淨淨的沒有任何 gradient 的痕跡。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">detach()</span></code> 會 detaches the tensor from its computation history。他等於在說：不管接下來你要做啥計算，都把 autograd 給關起來。</p></li>
</ul>
</li>
</ul>
</section>
<section id="with-no-grad">
<h5><span class="section-number">1.1.1.3.2. </span>with no_grad<a class="headerlink" href="#with-no-grad" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>在對 model 做 inference/evaluation 時，會關閉 gradient 的紀錄，這時就會用 context management，把 gradient 先關起來</p></li>
</ul>
</section>
</section>
</section>
<section id="tensor">
<h3><span class="section-number">1.1.2. </span>100 種 建立 tensor 的方式<a class="headerlink" href="#tensor" title="Permalink to this heading">#</a></h3>
<section id="torch-empty-torch-zeros-torch-ones-torch-rand">
<h4><span class="section-number">1.1.2.1. </span>torch.empty(), torch.zeros(), torch.ones(), torch.rand()<a class="headerlink" href="#torch-empty-torch-zeros-torch-ones-torch-rand" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 建立出指定 shape 的 placeholder
x = torch.empty(3, 4)
print(x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.5082e+03,  0.0000e+00,  1.5196e+03,  0.0000e+00],
        [-8.3862e+32,  4.5706e-41,  9.1084e-44,  0.0000e+00],
        [ 6.5767e-36,  0.0000e+00, -4.2653e-20,  4.5708e-41]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>這些數字都是假的，實際上只是在 memory 上幫你開好 (3, 4) 這種 shape 的 placeholder</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>torch.zeros(2,3)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0., 0., 0.],
        [0., 0., 0.]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>torch.ones(2, 3)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1., 1., 1.],
        [1., 1., 1.]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>torch.rand(2, 3) # 生出 0~1 的隨機數
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.1293, 0.2467, 0.8110],
        [0.6600, 0.7898, 0.0111]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="torch-manual-seed">
<h4><span class="section-number">1.1.2.2. </span>torch.manual_seed()<a class="headerlink" href="#torch-manual-seed" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>torch.manual_seed(1729)
random1 = torch.rand(2, 3)
print(random1)

random2 = torch.rand(2, 3)
print(random2)

torch.manual_seed(1729)
random3 = torch.rand(2, 3)
print(random3)

random4 = torch.rand(2, 3)
print(random4)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.3126, 0.3791, 0.3087],
        [0.0736, 0.4216, 0.0691]])
tensor([[0.2332, 0.4047, 0.2162],
        [0.9927, 0.4128, 0.5938]])
tensor([[0.3126, 0.3791, 0.3087],
        [0.0736, 0.4216, 0.0691]])
tensor([[0.2332, 0.4047, 0.2162],
        [0.9927, 0.4128, 0.5938]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>設 seed 後，接續的第一、第二…、第 n 次生成，結果會不同，但只要再設一次 seed，那結果就會和之前的第一、第二、…、第 n 次相同</p></li>
</ul>
</section>
<section id="torch-empty-like-torch-zeros-like-torch-ones-like-torch-rand-like">
<h4><span class="section-number">1.1.2.3. </span>torch.empty_like(), torch.zeros_like(), torch.ones_like(), torch.rand_like()<a class="headerlink" href="#torch-empty-like-torch-zeros-like-torch-ones-like-torch-rand-like" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = torch.empty(2, 2, 3)
print(x.shape)
print(x)

empty_like_x = torch.empty_like(x)
print(empty_like_x.shape)
print(empty_like_x)

zeros_like_x = torch.zeros_like(x)
print(zeros_like_x.shape)
print(zeros_like_x)

ones_like_x = torch.ones_like(x)
print(ones_like_x.shape)
print(ones_like_x)

rand_like_x = torch.rand_like(x)
print(rand_like_x.shape)
print(rand_like_x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 2, 3])
tensor([[[6.5738e-36, 0.0000e+00, 1.5085e+03],
         [0.0000e+00, 1.1210e-43, 0.0000e+00]],

        [[8.9683e-44, 0.0000e+00, 0.0000e+00],
         [0.0000e+00, 0.0000e+00, 1.3881e+00]]])
torch.Size([2, 2, 3])
tensor([[[-4.2654e-20,  4.5708e-41, -4.2654e-20],
         [ 4.5708e-41,  4.4842e-44,  0.0000e+00]],

        [[ 1.1210e-43,  0.0000e+00,  1.5110e+03],
         [ 0.0000e+00,  1.4013e-45,  0.0000e+00]]])
torch.Size([2, 2, 3])
tensor([[[0., 0., 0.],
         [0., 0., 0.]],

        [[0., 0., 0.],
         [0., 0., 0.]]])
torch.Size([2, 2, 3])
tensor([[[1., 1., 1.],
         [1., 1., 1.]],

        [[1., 1., 1.],
         [1., 1., 1.]]])
torch.Size([2, 2, 3])
tensor([[[0.6128, 0.1519, 0.0453],
         [0.5035, 0.9978, 0.3884]],

        [[0.6929, 0.1703, 0.1384],
         [0.4759, 0.7481, 0.0361]]])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="tensor-obj-clone">
<h3><span class="section-number">1.1.3. </span>tensor_obj.clone()<a class="headerlink" href="#tensor-obj-clone" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>tensor 是 mutable 的：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>a = torch.ones(2, 2)
b = a

a[0][1] = 561  # we change a...
print(b)       # ...and b is also altered
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[  1., 561.],
        [  1.,   1.]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>所以記得用 tensro_obj.clone() 來做 copy (就是 df.copy() 的類似寫法)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>a = torch.ones(2, 2)
b = a.clone()

assert b is not a      # different objects in memory...
print(torch.eq(a, b))  # ...but still with the same contents!

a[0][1] = 561          # a changes...
print(b)               # ...but b is still all ones
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[True, True],
        [True, True]])
tensor([[1., 1.],
        [1., 1.]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="torch-from-numpy-np-array-and-tensor-obj-clone-numpy">
<h3><span class="section-number">1.1.4. </span><code class="docutils literal notranslate"><span class="pre">torch.from_numpy(np_array)</span></code> and <code class="docutils literal notranslate"><span class="pre">tensor_obj.clone().numpy()</span></code><a class="headerlink" href="#torch-from-numpy-np-array-and-tensor-obj-clone-numpy" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>把 numpy 改成 tensor 的目的：</p>
<ul>
<li><p>可以放到 GPU 上加速</p></li>
<li><p>可以做 autograd.</p></li>
</ul>
</li>
<li><p>把 tensor 改成 numpy 的目的：</p>
<ul>
<li><p>做些中途的計算 &amp; 產出，但不會涉及到 gradient 紀錄.</p></li>
<li><p>特別小心，如果直接用 <code class="docutils literal notranslate"><span class="pre">tensor_obj.numpy()</span></code>，那他們是共享同個記憶體，是 mutable 的，所以改動 numpy 時，會影響到 tensor。所以才要先 clone() 再 numpy() (至於 detach 就不必要了，因為當你轉成 numpy 時，本來就不會有 gradient 紀錄了)</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>numpy_array = np.ones((2, 3))
print(numpy_array)

pytorch_tensor = torch.from_numpy(numpy_array)
print(pytorch_tensor)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1. 1. 1.]
 [1. 1. 1.]]
tensor([[1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>直接做 <code class="docutils literal notranslate"><span class="pre">tensor_obj.numpy()</span></code>，那會是 mutable，改一個，影響另一個：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pytorch_rand = torch.rand(2, 3)
print(pytorch_rand)

numpy_rand = pytorch_rand.numpy()
print(numpy_rand)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.5062, 0.8469, 0.2588],
        [0.2707, 0.4115, 0.6839]])
[[0.5062225  0.84694576 0.25884217]
 [0.2706535  0.41147768 0.6838606 ]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>numpy_array[1, 1] = 23
print(pytorch_tensor)

pytorch_rand[1, 1] = 17
print(numpy_rand)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.,  1.,  1.],
        [ 1., 23.,  1.]], dtype=torch.float64)
[[ 0.5062225   0.84694576  0.25884217]
 [ 0.2706535  17.          0.6838606 ]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>但如果先 clone 再 .numpy，那就是不同記憶體了，彼此不影響：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pytorch_rand = torch.rand(2, 3)
print(pytorch_rand)

numpy_rand = pytorch_rand.clone().numpy()
print(numpy_rand)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0703, 0.5105, 0.9451],
        [0.2359, 0.1979, 0.3327]])
[[0.07025403 0.5105133  0.9450517 ]
 [0.2358576  0.19793254 0.33274257]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>numpy_array[1, 1] = 23
print(pytorch_tensor)

pytorch_rand[1, 1] = 17
print(numpy_rand)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.,  1.,  1.],
        [ 1., 23.,  1.]], dtype=torch.float64)
[[0.07025403 0.5105133  0.9450517 ]
 [0.2358576  0.19793254 0.33274257]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="reshape">
<h3><span class="section-number">1.1.5. </span>reshape<a class="headerlink" href="#reshape" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>介紹四種很常用的 reshape.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.reshape(input,</span> <span class="pre">shape)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.squeeze(input)</span></code> or/and <code class="docutils literal notranslate"><span class="pre">torch.unsqueeze(input,</span> <span class="pre">dim)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.permute(input,</span> <span class="pre">dims)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.stack(tensors,</span> <span class="pre">dim</span> <span class="pre">=</span> <span class="pre">0)</span></code></p></li>
</ul>
</li>
</ul>
<section id="torch-reshape">
<h4><span class="section-number">1.1.5.1. </span>torch.reshape()<a class="headerlink" href="#torch-reshape" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create a tensor
x = torch.arange(1., 9.)
x, x.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([1., 2., 3., 4., 5., 6., 7., 8.]), torch.Size([8]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x.reshape((2,4))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1., 2., 3., 4.],
        [5., 6., 7., 8.]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="unsqueeze-squeeze">
<h4><span class="section-number">1.1.5.2. </span>unsqueeze (增軸) 與 squeeze (減軸)<a class="headerlink" href="#unsqueeze-squeeze" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>我們常常想把單一一張 img 的 shape，增軸成 batch = 1 的一張 img (i.e. 把 shape = (3, 266, 266) 增軸成 (1, 3, 266, 266))</p></li>
<li><p>那 unsqueeze 就是增軸，例如這邊，我想增在 第 0 軸</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>a = torch.rand(3, 226, 226)
print(a.shape)

b = a.unsqueeze(dim = 0)
print(b.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 226, 226])
torch.Size([1, 3, 226, 226])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>相反的，我們有時候拿到帶有 batch 資訊的資料時，我們想把他 un-batch.</p></li>
<li><p>例如，我拿到 shape = (1, 1) 的 output，但最前面的 1 其實是 batch_size，他就等於 1 而已.</p></li>
<li><p>我想把他拔掉，就用 squeeze</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>a = torch.rand(1,1)
print(a)
print(a.shape)

b = a.squeeze(dim = 0)
print(b)
print(b.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.9574]])
torch.Size([1, 1])
tensor([0.9574])
torch.Size([1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>a = torch.rand(1, 20)
print(a.shape)
print(a)

b = a.squeeze(0)
print(b.shape)
print(b)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 20])
tensor([[0.4265, 0.5256, 0.9091, 0.2780, 0.2162, 0.9831, 0.8699, 0.2427, 0.2078,
         0.7600, 0.9164, 0.7021, 0.0459, 0.6895, 0.2177, 0.6973, 0.8097, 0.9646,
         0.1726, 0.5225]])
torch.Size([20])
tensor([0.4265, 0.5256, 0.9091, 0.2780, 0.2162, 0.9831, 0.8699, 0.2427, 0.2078,
        0.7600, 0.9164, 0.7021, 0.0459, 0.6895, 0.2177, 0.6973, 0.8097, 0.9646,
        0.1726, 0.5225])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>pytorch 很聰明的，如果你的原始軸不是 1 ，他不會幫你 squeeze，例如下例就沒改變任何東西：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>c = torch.rand(2, 2)
print(c)
print(c.shape)

d = c.squeeze(0)
print(d)
print(d.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.8604, 0.8837],
        [0.8394, 0.8083]])
torch.Size([2, 2])
tensor([[0.8604, 0.8837],
        [0.8394, 0.8083]])
torch.Size([2, 2])
</pre></div>
</div>
</div>
</div>
</section>
<section id="stack-tensor">
<h4><span class="section-number">1.1.5.3. </span>stack 組合 tensor (會增軸)<a class="headerlink" href="#stack-tensor" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 原始 tensor
x = torch.rand((3,224,224)) # img with shape (channel = 3, width = 224, height = 224)
y = torch.rand((3,224,224))

# 把兩張影像裝成一個 batch
batch = torch.stack([x, y], dim = 0)
print(batch.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 3, 224, 224])
</pre></div>
</div>
</div>
</div>
</section>
<section id="permute">
<h4><span class="section-number">1.1.5.4. </span>permute 換通道<a class="headerlink" href="#permute" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = torch.rand((3,224,224)) # img with shape (channel = 3, width = 224, height = 224)
y = x.permute(1, 2, 0) # 新的第0,1,2軸，分別選原本的第1,2,0軸
y.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([224, 224, 3])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = torch.rand((224,224,3)) # img with shape (width = 224, height = 224, channel = 3)
y = x.permute(2, 0, 1) # 新的第0,1,2軸，分別選原本的第2,0,1軸
y.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 224, 224])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="tensor-element">
<h3><span class="section-number">1.1.6. </span>對 tensor 的每個 element 做運算<a class="headerlink" href="#tensor-element" title="Permalink to this heading">#</a></h3>
<section id="id1">
<h4><span class="section-number">1.1.6.1. </span>加, 減, 乘, 除, 開根號, 次方, …<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>ones = torch.zeros(2, 2) + 1
twos = torch.ones(2, 2) * 2
threes = (torch.ones(2, 2) * 7 - 1) / 2
fours = twos ** 2 # 次方計算
sqrt2s = twos ** 0.5 # 開根號

print(ones)
print(twos)
print(threes)
print(fours)
print(sqrt2s)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1., 1.],
        [1., 1.]])
tensor([[2., 2.],
        [2., 2.]])
tensor([[3., 3.],
        [3., 3.]])
tensor([[4., 4.],
        [4., 4.]])
tensor([[1.4142, 1.4142],
        [1.4142, 1.4142]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="id2">
<h4><span class="section-number">1.1.6.2. </span>取絕對值, 取整數, 截斷, …<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># common functions
a = torch.rand(2, 4) * 2 - 1
print(&#39;Common functions:&#39;)
print(torch.abs(a))
print(torch.ceil(a))
print(torch.floor(a))
print(torch.clamp(a, -0.5, 0.5))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Common functions:
tensor([[0.8065, 0.5906, 0.6122, 0.4887],
        [0.1322, 0.7961, 0.6525, 0.7819]])
tensor([[-0., 1., -0., 1.],
        [-0., 1., 1., -0.]])
tensor([[-1.,  0., -1.,  0.],
        [-1.,  0.,  0., -1.]])
tensor([[-0.5000,  0.5000, -0.5000,  0.4887],
        [-0.1322,  0.5000,  0.5000, -0.5000]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="id3">
<h4><span class="section-number">1.1.6.3. </span>三角函數 與 反函數<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># trigonometric functions and their inverses
angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])
sines = torch.sin(angles)
inverses = torch.asin(sines)
print(&#39;\nSine and arcsine:&#39;)
print(angles)
print(sines)
print(inverses)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sine and arcsine:
tensor([0.0000, 0.7854, 1.5708, 2.3562])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
tensor([0.0000, 0.7854, 1.5708, 0.7854])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># bitwise operations
print(&#39;\nBitwise XOR:&#39;)
b = torch.tensor([1, 5, 11])
c = torch.tensor([2, 7, 10])
print(torch.bitwise_xor(b, c))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Bitwise XOR:
tensor([3, 2, 1])
</pre></div>
</div>
</div>
</div>
</section>
<section id="id4">
<h4><span class="section-number">1.1.6.4. </span>比較兩 tensor 是否相等<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># comparisons:
print(&#39;\nBroadcasted, element-wise equality comparison:&#39;)
d = torch.tensor([[1., 2.], [3., 4.]])
e = torch.ones(1, 2)  # many comparison ops support broadcasting!
print(torch.eq(d, e)) # returns a tensor of type bool
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Broadcasted, element-wise equality comparison:
tensor([[ True, False],
        [False, False]])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="tensor-summarise">
<h3><span class="section-number">1.1.7. </span>對 tensor 做 summarise<a class="headerlink" href="#tensor-summarise" title="Permalink to this heading">#</a></h3>
<section id="id5">
<h4><span class="section-number">1.1.7.1. </span>取最大最小值, 平均, 標準差…<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># reductions:
print(&#39;\nReduction ops:&#39;)
print(torch.max(d))        # returns a single-element tensor
print(torch.max(d).item()) # extracts the value from the returned tensor
print(torch.mean(d))       # average
print(torch.std(d))        # standard deviation
print(torch.prod(d))       # product of all numbers
print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Reduction ops:
tensor(4.)
4.0
tensor(2.5000)
tensor(1.2910)
tensor(24.)
tensor([1, 2])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="matrix-operation">
<h3><span class="section-number">1.1.8. </span>matrix operation<a class="headerlink" href="#matrix-operation" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>element-wise multiplication: <code class="docutils literal notranslate"><span class="pre">tensor1*tensor2</span></code></p></li>
<li><p>matrix multiplication <code class="docutils literal notranslate"><span class="pre">tensor.matmul(tensor1,</span> <span class="pre">tensor2)</span></code></p></li>
</ul>
</section>
<section id="tensor-linear-algebra">
<h3><span class="section-number">1.1.9. </span>對 tensor 做 linear algebra<a class="headerlink" href="#tensor-linear-algebra" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># vector and linear algebra operations
v1 = torch.tensor([1., 0., 0.])         # x unit vector
v2 = torch.tensor([0., 1., 0.])         # y unit vector
m1 = torch.rand(2, 2)                   # random matrix
m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix

print(&#39;\nVectors &amp; Matrices:&#39;)
print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)
print(m1)
m3 = torch.matmul(m1, m2)
print(m3)                  # 3 times m1
print(torch.svd(m3))       # singular value decomposition
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vectors &amp; Matrices:
tensor([ 0.,  0., -1.])
tensor([[0.3837, 0.1149],
        [0.0840, 0.8760]])
tensor([[1.1511, 0.3447],
        [0.2521, 2.6281]])
torch.return_types.svd(
U=tensor([[ 0.2028,  0.9792],
        [ 0.9792, -0.2028]]),
S=tensor([2.6867, 1.0936]),
V=tensor([[ 0.1788,  0.9839],
        [ 0.9839, -0.1788]]))
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="model-loss-fn-optimizer">
<h2><span class="section-number">1.2. </span>自動微分(model, loss_fn 和 optimizer 如何協作)<a class="headerlink" href="#model-loss-fn-optimizer" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>用 linear regression model 來舉例</p>
<ul>
<li><p>假設我們的 NN，超簡單，就是 input 有 5 個 neuron，output 為 1 個 neuron 的結構。(也就是 5 個變數的線性回歸)</p></li>
<li><p>寫成數學式：</p>
<ul>
<li><p>只看一筆資料時： <span class="math notranslate nohighlight">\(\hat{y}_i = x_i^T W + b\)</span>, 其中，<span class="math notranslate nohighlight">\(x_i^T\)</span> 是 1x5 的向量，W 是 5x1 的矩陣, (W 的 shape 是 (input, output), 所以，如果要 output 出 7 個 neuron, W 是 5x7 矩陣)</p></li>
<li><p>看一個 batch 的資料時 (e.g. batch_size = 10): <span class="math notranslate nohighlight">\(\hat{y} = XW + b\)</span>, 其中，y是 10x1 的向量，X是 10x5 的矩陣，b會做 broad casting，所以變 10x1 的向量，但每個 element 的值都一樣</p></li>
<li><p>cost 是 mse，所以 cost function 是 <span class="math notranslate nohighlight">\(cost = \frac{1}{10} \sum_{i=1}^{10} \left( y_i - (x_i^TW + b) \right)^2 = \frac{1}{10} \sum_{i=1}^{10} \left( y_i - (x_{1i}w_1 + ... + x_{5i}w_5 + b) \right)^2\)</span></p></li>
<li><p>所以，做 gradient descent 時，就是要對 <span class="math notranslate nohighlight">\(w_i\)</span> 和 <span class="math notranslate nohighlight">\(b\)</span> 做偏微分，並用 <span class="math notranslate nohighlight">\(w_i^{new} = w_i^{old} - lr\times grad(w_i)\)</span>, <span class="math notranslate nohighlight">\(b^{new} = b^{old} - lr\times grad(b)\)</span>來更新參數。</p></li>
<li><p>可以想像，每一次 iteration，我都要拿到 <span class="math notranslate nohighlight">\(W_{grad}\)</span> 和 <span class="math notranslate nohighlight">\(b_{grad}\)</span> 這兩個東西，且 shape 會和 <span class="math notranslate nohighlight">\(W\)</span> 以及 <span class="math notranslate nohighlight">\(b\)</span> 完全相同</p></li>
</ul>
</li>
</ul>
</li>
<li><p>用 pytorch 來解這個問題，如下：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 真值
X = torch.rand(10,5)
W_true = torch.rand(5,1)
b_true = torch.rand(1)
y_true = torch.matmul(X,W_true)+b_true

print(&#39;W_true: \n&#39;, W_true, &#39;\n&#39;)
print(&#39;b_true: \n&#39;, b_true, &#39;\n&#39;)
print(&#39;y_true: \n&#39;, y_true, &#39;\n&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W_true: 
 tensor([[0.1672],
        [0.7261],
        [0.8677],
        [0.1589],
        [0.6089]]) 

b_true: 
 tensor([0.2286]) 

y_true: 
 tensor([[1.9082],
        [1.3121],
        [1.5016],
        [1.1900],
        [1.6450],
        [1.6336],
        [2.1329],
        [1.1671],
        [1.6328],
        [1.7094]]) 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># model
model = torch.nn.Linear(5, 1)

# optimizer
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)

# loss
mse_loss = torch.nn.MSELoss()

# W 參數估計的起始值
print(&#39;current estmated W: \n&#39;, model.weight.data, &#39;\n&#39;)
print(&#39;current W_grad: \n&#39;, model.weight.grad, &#39;\n&#39;)

# b 參數估計的起始值
print(&#39;current estmated b: \n&#39;, model.bias.data, &#39;\n&#39;)
print(&#39;current b_grad: \n&#39;, model.bias.grad, &#39;\n&#39;)

# 目前的預測值
y_pred = model(X)
print(&#39;current y_pred: \n&#39;, y_pred, &#39;\n&#39;)

# 目前的 loss
loss = mse_loss(y_pred, y_true)
print(&#39;current loss: \n&#39;, loss)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>current estmated W: 
 tensor([[-0.2711,  0.1681,  0.3631,  0.1607,  0.2658]]) 

current W_grad: 
 None 

current estmated b: 
 tensor([-0.1775]) 

current b_grad: 
 None 

current y_pred: 
 tensor([[ 0.3968],
        [-0.0029],
        [ 0.3377],
        [ 0.0565],
        [ 0.1576],
        [ 0.3519],
        [ 0.3798],
        [ 0.0702],
        [ 0.1862],
        [ 0.4231]], grad_fn=&lt;AddmmBackward0&gt;) 

current loss: 
 tensor(1.8532, grad_fn=&lt;MseLossBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>因為資料進來了，所以可以做 backward，取得 W_grad 和 b_grad</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 先清空目前的 gradient
optimizer.zero_grad()

# 算 gradient
loss.backward()

# 看一下算出來的 W_grad 和 b_grad
print(&#39;current W_grad: \n&#39;, model.weight.grad, &#39;\n&#39;)
print(&#39;current b_grad: \n&#39;, model.bias.grad, &#39;\n&#39;)

# 更新 W 和 b
optimizer.step()

# 看一下新的 W 和 b
print(&#39;current estmated W: \n&#39;, model.weight.data, &#39;\n&#39;)
print(&#39;current estmated b: \n&#39;, model.bias.data, &#39;\n&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>current W_grad: 
 tensor([[-0.9529, -1.7177, -1.5181, -1.1836, -1.3753]]) 

current b_grad: 
 tensor([-2.6952]) 

current estmated W: 
 tensor([[-0.2616,  0.1853,  0.3783,  0.1725,  0.2796]]) 

current estmated b: 
 tensor([-0.1506]) 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 第二次的 forward，應該可以看到 loss 變小
y_pred = model(X)
print(&#39;current y_pred: \n&#39;, y_pred, &#39;\n&#39;)
loss = mse_loss(y_pred, y_true)
print(&#39;current loss: \n&#39;, loss)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>current y_pred: 
 tensor([[0.4669],
        [0.0512],
        [0.3960],
        [0.1120],
        [0.2224],
        [0.4154],
        [0.4516],
        [0.1181],
        [0.2523],
        [0.4823]], grad_fn=&lt;AddmmBackward0&gt;) 

current loss: 
 tensor(1.6898, grad_fn=&lt;MseLossBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 第二次的 backward

# 可以看到，原本的 gradient 都還在，若不清空，gradient 會累加 (這不是我們要的)
print(&#39;current W_grad: \n&#39;, model.weight.grad, &#39;\n&#39;)
print(&#39;current b_grad: \n&#39;, model.bias.grad, &#39;\n&#39;)

# 所以才要這樣清空 gradient
optimizer.zero_grad()

# 看一下 gradient 被清空了
print(&#39;current W_grad: \n&#39;, model.weight.grad, &#39;\n&#39;)
print(&#39;current b_grad: \n&#39;, model.bias.grad, &#39;\n&#39;)

# 這時再來做 backward，算 gradient
loss.backward()

# 看一下算出來的 W_grad 和 b_grad
print(&#39;current W_grad: \n&#39;, model.weight.grad, &#39;\n&#39;)
print(&#39;current b_grad: \n&#39;, model.bias.grad, &#39;\n&#39;)

# 更新 W 和 b
optimizer.step()

# 看一下新的 W 和 b
print(&#39;current estmated W: \n&#39;, model.weight.data, &#39;\n&#39;)
print(&#39;current estmated b: \n&#39;, model.bias.data, &#39;\n&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>current W_grad: 
 tensor([[-0.9529, -1.7177, -1.5181, -1.1836, -1.3753]]) 

current b_grad: 
 tensor([-2.6952]) 

current W_grad: 
 tensor([[0., 0., 0., 0., 0.]]) 

current b_grad: 
 tensor([0.]) 

current W_grad: 
 tensor([[-0.9107, -1.6405, -1.4495, -1.1284, -1.3128]]) 

current b_grad: 
 tensor([-2.5729]) 

current estmated W: 
 tensor([[-0.2439,  0.2171,  0.4064,  0.1945,  0.3051]]) 

current estmated b: 
 tensor([-0.1006]) 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 第三次的 forward，應該可以看到 loss 變更小
y_pred = model(X)
print(&#39;current y_pred: \n&#39;, y_pred, &#39;\n&#39;)
loss = mse_loss(y_pred, y_true)
print(&#39;current loss: \n&#39;, loss)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>current y_pred: 
 tensor([[0.5971],
        [0.1515],
        [0.5041],
        [0.2151],
        [0.3425],
        [0.5333],
        [0.5848],
        [0.2070],
        [0.3749],
        [0.5921]], grad_fn=&lt;AddmmBackward0&gt;) 

current loss: 
 tensor(1.4068, grad_fn=&lt;MseLossBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>以上，就是實際在跑 deep learning model 時，如何使用 loss, optimizer，來做到求 gradient 和更新參數～</p></li>
</ul>
</section>
<section id="data-preparation">
<h2><span class="section-number">1.3. </span>Data preparation<a class="headerlink" href="#data-preparation" title="Permalink to this heading">#</a></h2>
<section id="dataset-class">
<h3><span class="section-number">1.3.1. </span>Dataset - 自訂 class<a class="headerlink" href="#dataset-class" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class MyDataset(Dataset):
    def __init__(self, feature_matrix, label_vector):             # 把資料存進 class object
        self.feature_matrix = feature_matrix
        self.label_vector = label_vector
    def __len__(self):
        assert len(self.feature_matrix) == len(self.label_vector) # 確定資料有互相對應
        return len(self.feature_matrix)
    def __getitem__(self, idx):                     # 定義我們需要取得某筆資料的方式
        return self.feature_matrix[idx], self.label_vector[idx]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 測試看看

X = np.random.rand(1000, 100, 100, 1)   # 虛構 1000 張 100 x 100 單色圖片
Y = np.random.randint(0, 7, [1000, 10]) # 虛構 1000 個 labels

my_dataset = MyDataset(X.astype(np.float32), Y.astype(np.float32))
taken_x, taken_y = my_dataset[0] # 取得第一筆資料
taken_x.shape, taken_y.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((100, 100, 1), (10,))
</pre></div>
</div>
</div>
</div>
</section>
<section id="dataset-tensordataset">
<h3><span class="section-number">1.3.2. </span>Dataset - 直接用 <code class="docutils literal notranslate"><span class="pre">TensorDataset</span></code><a class="headerlink" href="#dataset-tensordataset" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 手上有的資料，先轉成 Tensor
X = np.random.rand(1000, 100, 100, 1)   # 虛構 1000 張 100 x 100 單色圖片
Y = np.random.randint(0, 7, [1000, 10]) # 虛構 1000 個 labels
tsrX, tsrY = torch.tensor(X), torch.tensor(Y)

# 餵到 TensorDataset 裡面
tsrdataset = TensorDataset(tsrX, tsrY)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 幾個重要的用法
print(tsrdataset.__len__()) # 幾張圖
taken_x, taken_y = tsrdataset[0] # 取得第一筆資料
print(taken_x.shape, taken_y.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1000
torch.Size([100, 100, 1]) torch.Size([10])
</pre></div>
</div>
</div>
</div>
</section>
<section id="dataloader">
<h3><span class="section-number">1.3.3. </span>DataLoader<a class="headerlink" href="#dataloader" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 將 dataset 包裝成 dataloader
my_dataloader = DataLoader(
    my_dataset, 
    batch_size=4,
    shuffle=True #, 
    # num_workers=4
)

# 跑一個 loop 確認拿到的 batch 是否正確
for batch_x, batch_y in my_dataloader:
    print((batch_x.shape, batch_y.shape))
    break
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([4, 100, 100, 1]), torch.Size([4, 10]))
</pre></div>
</div>
</div>
</div>
</section>
<section id="dataset">
<h3><span class="section-number">1.3.4. </span>內建 dataset<a class="headerlink" href="#dataset" title="Permalink to this heading">#</a></h3>
<section id="id6">
<h4><span class="section-number">1.3.4.1. </span>圖片類<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h4>
<section id="transform">
<h5><span class="section-number">1.3.4.1.1. </span>無 transform<a class="headerlink" href="#transform" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># dataset
mnist_train = torchvision.datasets.FashionMNIST(
    root=&quot;/home/ubuntu/pytorch_dataset&quot;, train=True, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root=&quot;/home/ubuntu/pytorch_dataset&quot;, train=False, download=True)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw/train-images-idx3-ubyte.gz
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100.0%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting /home/ubuntu/pytorch_dataset/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw/train-labels-idx1-ubyte.gz
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100.0%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting /home/ubuntu/pytorch_dataset/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw/t10k-images-idx3-ubyte.gz
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100.0%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting /home/ubuntu/pytorch_dataset/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100.0%
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting /home/ubuntu/pytorch_dataset/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>此時為 dataset 格式</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>mnist_train
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset FashionMNIST
    Number of datapoints: 60000
    Root location: /home/ubuntu/pytorch_dataset
    Split: Train
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>用 index 可以取資料</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x, y = mnist_train[0] # 第 0 筆
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>x 會是 PIL 物件, y 是 lab</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>type(x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PIL.Image.Image
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/adf48a208b1e9f1a3b4756de5ea2d382077db8d5c9393bb6f782fc97580efbc4.png" src="../_images/adf48a208b1e9f1a3b4756de5ea2d382077db8d5c9393bb6f782fc97580efbc4.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.imshow(x);
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/3f6bbe7406c36ad98fcd3671bf0397bdcebdad4380d2474932dd0f89fc74d85a.png" src="../_images/3f6bbe7406c36ad98fcd3671bf0397bdcebdad4380d2474932dd0f89fc74d85a.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>可以把 x 轉成 numpy 看看：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x_array = np.asarray(x)
print(x_array.shape)
print(x_array.dtype)
print(x_array.min())
print(x_array.max())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(28, 28)
uint8
0
255
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>可以看到是 28x28 的圖，且是 uint8 type，介於 0~255 整數值</p></li>
</ul>
</section>
<section id="id7">
<h5><span class="section-number">1.3.4.1.2. </span>有 transform<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>圖片類資料庫，通常都會做以下 transform:</p>
<ul>
<li><p>把圖片改成 float32 浮點數 type.</p></li>
<li><p>把圖片正規化到 0~1 之間</p></li>
<li><p>轉成 tensor (灰階圖，會變成 (1,28,28), RGB圖仍是 (3, 28, 28))</p></li>
</ul>
</li>
<li><p>這其實就是 <code class="docutils literal notranslate"><span class="pre">torchvision.transforms.ToTensor()</span></code> 在做的事</p></li>
<li><p>看一下剛剛的例子</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(type(x))
print(np.asarray(x).dtype)

trans = torchvision.transforms.ToTensor()
x_trans = trans(x)
print(type(x_trans))
print(x_trans.dtype)
print(x_trans.min())
print(x_trans.max())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;PIL.Image.Image&#39;&gt;
uint8
&lt;class &#39;torch.Tensor&#39;&gt;
torch.float32
tensor(0.)
tensor(1.)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>讀檔時，就可以把這個放進去：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>trans = transforms.ToTensor()

mnist_train = torchvision.datasets.FashionMNIST(
    root=&quot;/home/ubuntu/pytorch_dataset&quot;, train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root=&quot;/home/ubuntu/pytorch_dataset&quot;, train=False, transform=trans, download=True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>mnist_train
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset FashionMNIST
    Number of datapoints: 60000
    Root location: /home/ubuntu/pytorch_dataset
    Split: Train
    StandardTransform
Transform: ToTensor()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x, y = mnist_train[0]
print(x.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 28, 28])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x.numpy().shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1, 28, 28)
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
</section>
<section id="transforms">
<h2><span class="section-number">1.4. </span>Transforms<a class="headerlink" href="#transforms" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>transforms 可以大致分為兩種：</p>
<ul>
<li><p>基本的影像前處理 (input 是 PIL 物件，output 也還是 PIL 物件)，例如：</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">transforms.Resize(size)</span></code>:  input 是 PIL 物件，size 如果是 sequence (e.g. (h,w)), 那就是 resize 成 (h,w) 的大小。size 如果是 int (e.g. 224)，那他會幫你把短邊調到 224, 長邊就等比例縮小。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transforms.CenterCrop(size)</span></code>: input 是 PIL 物件，依據給定的 size 沿中心裁減，如果 size 是 int，就 crop 成 (size, size)，如果 size 是 (h, w)，就 crop 成 (h, w)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop(size)</span></code>: input 是 PIL 物件，依據給定的 size，隨機沿一個中心裁減，size 定法同上).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transforms.RandomResizedCrop(size,</span> <span class="pre">scale=(0.08,</span> <span class="pre">1.0),</span> <span class="pre">ratio=(0.75,</span> <span class="pre">1.33),</span> <span class="pre">interpolation=2)</span></code>: 先隨機沿某個中心，將原影像剪裁為原圖 scale 倍的圖片 (以此例來說，就是剪成原圖 0.08 倍 ~ 1 倍),剪裁的長寬比，介於0.75~1.33 之間。剪完後，resize 成 size 大小的圖片，resize 方法預設是雙線性內插法 PIL.Image.BILINEAR</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transforms.RandomHorizontalFlip(p=0.5)</span></code>: 隨機上下翻轉</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transforms.RandomVerticalFlip(p=0.5)</span></code>: 隨機左右翻轉</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transforms.GaussianBlur(kernel_size=(5,</span> <span class="pre">9),</span> <span class="pre">sigma=(0.1,</span> <span class="pre">5))</span></code>: 就 gaussianBlur</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transforms.Grayscale()</span></code>: input 是 PIL 物件，幫你轉成灰階.</p></li>
</ul>
</li>
<li><p>餵進 model 時的數值處理，例如：</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">transforms.ToTensor()</span></code>: input 是 PIL 或 numpy，output是 0 到 1 tensor.float, 且通道在前。(如果 numpy 的 type 是 uint8，那 range 是 0 到 255, 他就會幫你 scaling 到 0~1; 如果 numpy 的 type 就已經是 float，他就不會再幫你 scaling).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transforms.Normalize()</span></code>: input 是 tensor, 假設 k 個通道，那你就要給他 k 維的 mean list 和 sd list，他就幫你對每一個通道做 normalization。如果是用 imagenet pretrained model，他根據資料庫所有影像已幫你算好各通道的平均數和邊準差，分別是： <code class="docutils literal notranslate"><span class="pre">mean=[0.485,</span> <span class="pre">0.456,</span> <span class="pre">0.406]</span></code>, <code class="docutils literal notranslate"><span class="pre">std=[0.229,</span> <span class="pre">0.224,</span> <span class="pre">0.225]</span></code></p></li>
</ul>
</li>
</ul>
</li>
<li><p>官網的範例可以看這：<a class="reference external" href="https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py">https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py</a></p></li>
<li><p>官網的詳細文件看這：<a class="reference external" href="https://pytorch.org/vision/stable/transforms.html">https://pytorch.org/vision/stable/transforms.html</a></p></li>
<li><p>以下列出基本套路，再個別實驗來看看效果</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 用 imagenet pre-trained model 需做的 preprocess
imagenet_transform = transforms.Compose([
    # pre-processing
    transforms.Resize((224, 224)),
    # to tensor &amp; normalization
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# training 時常用的 augmentation
training_transform = transforms.Compose([
    # pre-processing
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.5),
    # to tensor
    transforms.ToTensor()
])

# testing/inference 時用的，就基本的
training_transform = transforms.Compose([
    # pre-processing
    transforms.Resize((224, 224)),
    # to tensor
    transforms.ToTensor()
])
</pre></div>
</div>
</div>
</div>
</section>
<section id="activation-functions">
<h2><span class="section-number">1.5. </span>activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this heading">#</a></h2>
<section id="id8">
<h3><span class="section-number">1.5.1. </span>內建<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h3>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>activation function</p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">torch.nn</span> <span class="pre">as</span> <span class="pre">nn</span></code></p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span> <span class="pre">as</span> <span class="pre">F</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Sigmoid</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.Sigmoid()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.sigmoid</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Softmax</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.Softmax(dim=None)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.softmax</span></code></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>ReLU</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.ReLU()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.relu</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>LeakyReLU</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.LeakyReLU(negative_slope=0.01)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.leaky_relu</span></code></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Tanh</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.Tanh()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.tanh</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>GELU</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.GELU()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.gelu</span></code></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>ReLU6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.ReLU6()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.relu6</span></code></p></td>
</tr>
</tbody>
</table>
<section id="relu">
<h4><span class="section-number">1.5.1.1. </span>ReLU<a class="headerlink" href="#relu" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>主要重點：</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(ReLU(x) = max(x, 0)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{x}{dx} ReLU(x) = 1\)</span> if x &gt; 0; <span class="math notranslate nohighlight">\(\frac{x}{dx} ReLU(x) = 0\)</span> if x &lt;= 0</p></li>
<li><p>relu 的導數，在 x = 0 時，數學上是不存在，但在工程上 “定義” 導數為 0，這樣就能繼續做了</p></li>
<li><p>relu 的優點是求導的結果簡單，不是 0 就是 1，在 backward 更新參數時， <code class="docutils literal notranslate"><span class="pre">weight_new</span> <span class="pre">=</span> <span class="pre">weight_old</span> <span class="pre">-</span> <span class="pre">learning_rate</span> <span class="pre">*</span> <span class="pre">grad</span></code>，那 grad 不是 0 就是 1，減輕了以往NN的梯度消失問題。</p></li>
</ul>
</li>
<li><p>簡單範例：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>activation = nn.ReLU()

x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = activation(x)
print(y)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 4.4409e-16, 1.0000e-01, 2.0000e-01, 3.0000e-01,
        4.0000e-01, 5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01,
        1.0000e+00, 1.1000e+00, 1.2000e+00, 1.3000e+00, 1.4000e+00, 1.5000e+00,
        1.6000e+00, 1.7000e+00, 1.8000e+00, 1.9000e+00, 2.0000e+00, 2.1000e+00,
        2.2000e+00, 2.3000e+00, 2.4000e+00, 2.5000e+00, 2.6000e+00, 2.7000e+00,
        2.8000e+00, 2.9000e+00, 3.0000e+00, 3.1000e+00, 3.2000e+00, 3.3000e+00,
        3.4000e+00, 3.5000e+00, 3.6000e+00, 3.7000e+00, 3.8000e+00, 3.9000e+00,
        4.0000e+00, 4.1000e+00, 4.2000e+00, 4.3000e+00, 4.4000e+00, 4.5000e+00,
        4.6000e+00, 4.7000e+00, 4.8000e+00, 4.9000e+00, 5.0000e+00, 5.1000e+00,
        5.2000e+00, 5.3000e+00, 5.4000e+00, 5.5000e+00, 5.6000e+00, 5.7000e+00,
        5.8000e+00, 5.9000e+00, 6.0000e+00, 6.1000e+00, 6.2000e+00, 6.3000e+00,
        6.4000e+00, 6.5000e+00, 6.6000e+00, 6.7000e+00, 6.8000e+00, 6.9000e+00,
        7.0000e+00, 7.1000e+00, 7.2000e+00, 7.3000e+00, 7.4000e+00, 7.5000e+00,
        7.6000e+00, 7.7000e+00, 7.8000e+00, 7.9000e+00],
       grad_fn=&lt;ReluBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.plot(x.detach(), y.detach());
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b11717f0a994969e429de88b98b871f19e3a926dd381d1f86165901f0e737d95.png" src="../_images/b11717f0a994969e429de88b98b871f19e3a926dd381d1f86165901f0e737d95.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y.backward(torch.ones_like(x), retain_graph=True)
plt.plot(x.detach(), x.grad); # gradient
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/22a466c4b608c1afc3bd3620688de671cfa3a75a6d559aa038f4b5e220284276.png" src="../_images/22a466c4b608c1afc3bd3620688de671cfa3a75a6d559aa038f4b5e220284276.png" />
</div>
</div>
</section>
<section id="sigmoid">
<h4><span class="section-number">1.5.1.2. </span>Sigmoid<a class="headerlink" href="#sigmoid" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>主要重點：</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(sigmoid(x) = \frac{1}{1 + exp(-x)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{x}{dx} sigmoid(x) = sigmoid(x)(1-sigmoid(x))\)</span></p></li>
<li><p>從導數的性質，可以發現，gradient 在 x 靠近 0 時，值較大 (參數更新較快）， x 遠離 0 時， gradient 趨近於 0 (參數停止更新)</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>activation = nn.Sigmoid()

x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = activation(x)

plt.plot(x.detach(), y.detach());
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7b7ed221d14bb843dd2dc8fdb722a9771111a2a4624ab09d2ad29fd40075554c.png" src="../_images/7b7ed221d14bb843dd2dc8fdb722a9771111a2a4624ab09d2ad29fd40075554c.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># gradients
y.backward(torch.ones_like(x),retain_graph=True)
plt.plot(x.detach(), x.grad);
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1fcdc2dc281f4220259261f23638d0a0d088637778017f9aa91fa73ec87b886f.png" src="../_images/1fcdc2dc281f4220259261f23638d0a0d088637778017f9aa91fa73ec87b886f.png" />
</div>
</div>
</section>
</section>
<section id="id9">
<h3><span class="section-number">1.5.2. </span>自訂<a class="headerlink" href="#id9" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>直接定義一個 function</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def relu(x):
    a = torch.zeros_like(x) # shape 會與 x 一樣
    return torch.max(x, a)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="custom-layers-block">
<h2><span class="section-number">1.6. </span>custom layers &amp; block<a class="headerlink" href="#custom-layers-block" title="Permalink to this heading">#</a></h2>
<img src="imgs/layer_block_model.jpg" width=600 alt="layer_block_model"/><ul class="simple">
<li><p>幾個名詞定義一下：</p>
<ul>
<li><p>layer: 只要是 input n 個 neruon, output m 個 neuron 的 function，就被稱為一個 layer。例如 <code class="docutils literal notranslate"><span class="pre">nn.Linear(in_dim,</span> <span class="pre">out_dim)</span></code> 就是個 linear layer.</p></li>
<li><p>block:</p>
<ul>
<li><p>多個 layer 組合在一起，稱為一個 block。例如一個 VGG block，就是由數個 conv, pooling layer 所組成.</p></li>
<li><p>通常用 sequential 來把 layer 組成 block; 或用 sub-class 來把 layer 組成 block</p></li>
</ul>
</li>
<li><p>model:</p>
<ul>
<li><p>由 layers or/and blocks 組起來，只要 input 是 feature/images/sentences..，output 是 回歸/分類…結果，就都可稱為 model。</p></li>
<li><p>例如一個 linear layer 可以是 model (e.g. linear regression)，一個 block 可以是 model (e.g. 多層感知機)，多個 block 組在一起 (e.g. resnet) 也可以是 model</p></li>
<li><p>所以，可以用 <code class="docutils literal notranslate"><span class="pre">layer</span></code> 來做出 model，也可以用 <code class="docutils literal notranslate"><span class="pre">sequential</span></code> 組成 model，也可以用 <code class="docutils literal notranslate"><span class="pre">sub-class</span></code> 組成 model</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<section id="custom-layer">
<h3><span class="section-number">1.6.1. </span>custom layer (不帶參數)<a class="headerlink" href="#custom-layer" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class CenteredLayer(nn.Module):
    &quot;&quot;&quot;用來做中心化(去平均)的layer
    args:
      X: 任何 shape，但通常是 (n, p)，然後我們想把 feature 都 de-mean
    &quot;&quot;&quot;
    def __init__(self, dim = 0):
        super().__init__()
        self.dim = dim

    def forward(self, X):
        return X - X.mean(dim = self.dim, keepdim = True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 做 5 個 sample，每個 sample 都有 2 個 feature 的 X
X = torch.randn(5, 2)
X
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-1.1121,  1.7709],
        [-1.0068, -1.2362],
        [ 0.0348,  1.1227],
        [-0.2837, -1.2738],
        [ 1.2561, -0.0931]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>layer = CenteredLayer()
layer(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.8898,  1.7128],
        [-0.7844, -1.2943],
        [ 0.2571,  1.0646],
        [-0.0613, -1.3318],
        [ 1.4784, -0.1512]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>可以清楚看到，de-mean 後，每個 col 現在相加都是 0</p></li>
</ul>
<ul class="simple">
<li><p>之後，這種 layer 就可以當作前處理，然後這樣用：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model = nn.Sequential(
    CenteredLayer(), # 前處理用，de-mean
    nn.Linear(2, 1) # linear regression
)
model(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.6427],
        [-0.0380],
        [-0.6522],
        [-0.1202],
        [-0.5543]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id10">
<h3><span class="section-number">1.6.2. </span>custom layer (帶參數)<a class="headerlink" href="#id10" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>重點在，weight, bias 要用 <code class="docutils literal notranslate"><span class="pre">nn.Parameter()</span></code> 來造，這樣就可以保有計算 gradient 等功能(預設 requires_grad = True)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>? nn.Parameter
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Red">Init signature:</span>  nn<span class=" -Color -Color-Blue">.</span>Parameter<span class=" -Color -Color-Blue">(</span>data<span class=" -Color -Color-Blue">=</span><span class=" -Color -Color-Green">None</span><span class=" -Color -Color-Blue">,</span> requires_grad<span class=" -Color -Color-Blue">=</span><span class=" -Color -Color-Green">True</span><span class=" -Color -Color-Blue">)</span>
<span class=" -Color -Color-Red">Docstring:</span>     
A kind of Tensor that is to be considered a module parameter.

Parameters are :class:`~torch.Tensor` subclasses, that have a
very special property when used with :class:`Module` s - when they&#39;re
assigned as Module attributes they are automatically added to the list of
its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.
Assigning a Tensor doesn&#39;t have such effect. This is because one might
want to cache some temporary state, like last hidden state of the RNN, in
the model. If there was no such class as :class:`Parameter`, these
temporaries would get registered too.

Args:
    data (Tensor): parameter tensor.
    requires_grad (bool, optional): if the parameter requires gradient. See
        :ref:`locally-disable-grad-doc` for more details. Default: `True`
<span class=" -Color -Color-Red">File:</span>           ~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/parameter.py
<span class=" -Color -Color-Red">Type:</span>           _ParameterMeta
<span class=" -Color -Color-Red">Subclasses:</span>     UninitializedParameter
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class MyLinear(nn.Module):
    &quot;&quot;&quot; 自己寫一個 dense 層 &quot;&quot;&quot;
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_dim, out_dim))
        self.bias = nn.Parameter(torch.randn(out_dim,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return linear
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>看一下實例化後，起始參數：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>linear = MyLinear(5, 3)
linear.weight
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([[ 0.8718,  0.9469,  0.1916],
        [ 1.6973,  1.5636, -0.3113],
        [ 0.7594, -1.1440, -1.7244],
        [-0.7467,  0.9555,  0.9014],
        [-0.5161,  0.7031, -1.6652]], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>用用看：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = torch.randn(10, 5)
linear(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 4.2076,  1.0059, -5.5664],
        [ 2.2081,  3.5985, -4.8784],
        [ 1.4460,  2.0264,  0.4752],
        [-0.4749, -1.9205, -2.2298],
        [ 1.1014, -0.4691, -3.6861],
        [-2.3040,  1.4275, -0.5245],
        [ 1.8266,  1.2462, -3.5570],
        [ 1.2729,  2.0576, -4.6761],
        [-1.4266, -0.1284, -0.0855],
        [ 0.2521, -1.2549,  1.0474]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="sequential-block-nn-sequential-layer1-block2">
<h3><span class="section-number">1.6.3. </span>sequential block (<code class="docutils literal notranslate"><span class="pre">nn.Sequential(layer1,</span> <span class="pre">block2,</span> <span class="pre">...)</span></code>)<a class="headerlink" href="#sequential-block-nn-sequential-layer1-block2" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>net = nn.Sequential(
    nn.Linear(20, 256), 
    nn.ReLU(), 
    nn.Linear(256, 10)
)

X = torch.rand(2, 20)
net(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.1119,  0.0200,  0.2404,  0.2004, -0.0767, -0.1386,  0.1669, -0.1070,
          0.0494, -0.1204],
        [-0.1727,  0.0096,  0.1246,  0.2796, -0.0674, -0.1257,  0.0676,  0.0165,
          0.1145, -0.0782]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="sequential-for-tips">
<h3><span class="section-number">1.6.4. </span>sequential <code class="docutils literal notranslate"><span class="pre">for</span></code> tips<a class="headerlink" href="#sequential-for-tips" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>我們可以建立一個自己的 sequential，就可以看到實際運作狀況：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # args 就是 user 隨意要丟幾個 layer, block 進來，所組成的 list
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>來試試看：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>net = MySequential(
    nn.Linear(20, 256), 
    nn.ReLU(), 
    nn.Linear(256, 10)
)

net(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.1936,  0.0687, -0.0650,  0.2737, -0.0451, -0.0563,  0.0344, -0.1300,
          0.0651, -0.0686],
        [ 0.1334,  0.1245,  0.0674,  0.3944, -0.0648, -0.3130,  0.0865, -0.2485,
          0.0543, -0.1893]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="custom-block">
<h3><span class="section-number">1.6.5. </span>custom block<a class="headerlink" href="#custom-block" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>雖然 sequential block 很方便，但有時我們會需要在 forward 的時候，做一些靈活的控制，例如以下這個刻意做出來的例子：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.out = nn.Linear(256, 10)
        
    def forward(self, X):
        X = self.hidden(X)
        X = F.relu(X)
        X = self.out(X)
        # 這邊開始是 flexible 的設計, 這就是 sequential 辦不到的
        # 我希望控制輸出，當輸出的 tensor 的 L1 norm &gt; 1 時，我就把他除以2，直到輸出的 L1 norm 壓在 1 以內
        while X.abs().sum() &gt; 1:
            X /= 2
        return X.sum()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>net = MLP()
net
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MLP(
  (hidden): Linear(in_features=20, out_features=256, bias=True)
  (out): Linear(in_features=256, out_features=10, bias=True)
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>來試一下：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = torch.rand(2, 20)

net = MLP()
net(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-0.0175, grad_fn=&lt;SumBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="model">
<h3><span class="section-number">1.6.6. </span>經典 model 自己寫系列<a class="headerlink" href="#model" title="Permalink to this heading">#</a></h3>
<section id="vgg11">
<h4><span class="section-number">1.6.6.1. </span>VGG11<a class="headerlink" href="#vgg11" title="Permalink to this heading">#</a></h4>
<section id="vgg-block">
<h5><span class="section-number">1.6.6.1.1. </span>VGG block<a class="headerlink" href="#vgg-block" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>經典的 VGG，就是用了 VGG block 的概念.</p></li>
<li><p>一個 VGG block，是由以下兩個 component 組成：</p>
<ul>
<li><p>k 個 3x3 的 conv2d + ReLU</p>
<ul>
<li><p>k 通常是 1 or 2 而已</p></li>
<li><p>same padding: stride = 1, padding = 1</p></li>
<li><p>輸出通道數是 hyperparameter，一般都從 64 開始，一路翻倍上去 (64-&gt;128-&gt;256-&gt;512).</p></li>
</ul>
</li>
<li><p>1 個 2x2 max_pooling, stride = 2, 做到高寬減半</p></li>
</ul>
</li>
<li><p>來看一個 VGG block 的實作：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def vgg_block(num_convs, in_channels, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels,
                                kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
    final_module = nn.Sequential(*layers)
    return final_module
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>例如第一個 block，我想要 1個 conv2，然後 block 的最終 output channel 是 64</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>vgg_block(num_convs = 1, in_channels = 3, out_channels = 64)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>第二個 block，我想要 2 個 conv2d, 然後 block 的最終 output channel 是 128</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>vgg_block(num_convs = 2, in_channels = 64, out_channels = 128)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id11">
<h5><span class="section-number">1.6.6.1.2. </span>讓 VGG block 疊高高<a class="headerlink" href="#id11" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>VGG 當初的特色，就是當我定義好 VGG block 後，我可以一路疊高高，想疊多少 block 就疊多少 block.</p></li>
<li><p>所以，以下定義一個 function，來把 VGG 疊高高</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def vgg_feature_extraction(conv_arch, input_img_channels = 3):
    &quot;&quot;&quot;
    args:
      - img_channels: number_channel of input image 
      - conv_arch: list of tuples (num_convs, out_channels), e.g. [(1,64),(1,128),...,(2,512)]
    &quot;&quot;&quot;
    conv_blks_dict = OrderedDict()
    in_channels = input_img_channels
    
    for idx, (num_convs, out_channels) in enumerate(conv_arch):
        conv_blks_dict[f&quot;block{idx}&quot;] = vgg_block(num_convs, in_channels, out_channels)
        in_channels = out_channels
    
    feature_module = nn.Sequential(conv_blks_dict)
    return feature_module
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]
feature_module = vgg_feature_extraction(conv_arch = conv_arch, input_img_channels = 3)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>feature_module
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (block0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (block1): Sequential(
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (block2): Sequential(
    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (block3): Sequential(
    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (block4): Sequential(
    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>看一下，如果餵進去 shape = (1, 3,224,224) 的資料 (batch_size = 1, channel = 3, height=width=224)，各 layer 的 shape 會變怎樣：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>summary(feature_module, (1,3,224,224))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Sequential                               [1, 512, 7, 7]            --
├─Sequential: 1-1                        [1, 64, 112, 112]         --
│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792
│    └─ReLU: 2-2                         [1, 64, 224, 224]         --
│    └─MaxPool2d: 2-3                    [1, 64, 112, 112]         --
├─Sequential: 1-2                        [1, 128, 56, 56]          --
│    └─Conv2d: 2-4                       [1, 128, 112, 112]        73,856
│    └─ReLU: 2-5                         [1, 128, 112, 112]        --
│    └─MaxPool2d: 2-6                    [1, 128, 56, 56]          --
├─Sequential: 1-3                        [1, 256, 28, 28]          --
│    └─Conv2d: 2-7                       [1, 256, 56, 56]          295,168
│    └─ReLU: 2-8                         [1, 256, 56, 56]          --
│    └─Conv2d: 2-9                       [1, 256, 56, 56]          590,080
│    └─ReLU: 2-10                        [1, 256, 56, 56]          --
│    └─MaxPool2d: 2-11                   [1, 256, 28, 28]          --
├─Sequential: 1-4                        [1, 512, 14, 14]          --
│    └─Conv2d: 2-12                      [1, 512, 28, 28]          1,180,160
│    └─ReLU: 2-13                        [1, 512, 28, 28]          --
│    └─Conv2d: 2-14                      [1, 512, 28, 28]          2,359,808
│    └─ReLU: 2-15                        [1, 512, 28, 28]          --
│    └─MaxPool2d: 2-16                   [1, 512, 14, 14]          --
├─Sequential: 1-5                        [1, 512, 7, 7]            --
│    └─Conv2d: 2-17                      [1, 512, 14, 14]          2,359,808
│    └─ReLU: 2-18                        [1, 512, 14, 14]          --
│    └─Conv2d: 2-19                      [1, 512, 14, 14]          2,359,808
│    └─ReLU: 2-20                        [1, 512, 14, 14]          --
│    └─MaxPool2d: 2-21                   [1, 512, 7, 7]            --
==========================================================================================
Total params: 9,220,480
Trainable params: 9,220,480
Non-trainable params: 0
Total mult-adds (G): 7.49
==========================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 59.41
Params size (MB): 36.88
Estimated Total Size (MB): 96.89
==========================================================================================
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>可以看到，最後的 feature map 是 (512, 7, 7) 的 shape</p></li>
</ul>
</section>
<section id="flatten-layer">
<h5><span class="section-number">1.6.6.1.3. </span>flatten layer<a class="headerlink" href="#flatten-layer" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>這邊要把最後的 feature map，拉直成向量，好餵入最後的全連階層做預測</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>flatten_module = nn.Flatten()
</pre></div>
</div>
</div>
</div>
</section>
<section id="classifier-layer">
<h5><span class="section-number">1.6.6.1.4. </span>classifier layer<a class="headerlink" href="#classifier-layer" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>classifier 就比較單純了，由一系列的 dense layer + relu + drop out 組成</p></li>
<li><p>最終的 class 數量是 100</p></li>
<li><p>直接寫：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>classifier_module = nn.Sequential(
    nn.Linear(512*7*7, 4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 1000)
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id12">
<h5><span class="section-number">1.6.6.1.5. </span>全部組起來<a class="headerlink" href="#id12" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def custom_vgg(conv_arch, input_img_channels):
    feature_module = vgg_feature_extraction(conv_arch, input_img_channels)
    flatten_module = nn.Flatten()
    classifier_module = nn.Sequential(
        nn.Linear(512*7*7, 4096),
        nn.ReLU(),
        nn.Dropout(p=0.5),
        nn.Linear(4096, 4096),
        nn.ReLU(),
        nn.Dropout(p=0.5),
        nn.Linear(4096, 1000)
    )
    
    final_model_dict = OrderedDict()
    final_model_dict[&quot;feature&quot;] = feature_module
    final_model_dict[&quot;flatten&quot;] = flatten_module
    final_model_dict[&quot;classifier&quot;] = classifier_module
    final_model = nn.Sequential(final_model_dict)
    return final_model
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]
my_vgg = custom_vgg(conv_arch, 3)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>my_vgg
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (feature): Sequential(
    (block0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (block1): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (block2): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (block3): Sequential(
      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (block4): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>對照一下 pretrained 的 vgg，會發現結構一模一樣</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from torchvision.models import vgg11
vgg = vgg11(weights=&quot;IMAGENET1K_V1&quot;)
vgg
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): ReLU(inplace=True)
    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (14): ReLU(inplace=True)
    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): ReLU(inplace=True)
    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): ReLU(inplace=True)
    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>summary(my_vgg, (1, 3, 224, 224))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Sequential                               [1, 1000]                 --
├─Sequential: 1-1                        [1, 512, 7, 7]            --
│    └─Sequential: 2-1                   [1, 64, 112, 112]         --
│    │    └─Conv2d: 3-1                  [1, 64, 224, 224]         1,792
│    │    └─ReLU: 3-2                    [1, 64, 224, 224]         --
│    │    └─MaxPool2d: 3-3               [1, 64, 112, 112]         --
│    └─Sequential: 2-2                   [1, 128, 56, 56]          --
│    │    └─Conv2d: 3-4                  [1, 128, 112, 112]        73,856
│    │    └─ReLU: 3-5                    [1, 128, 112, 112]        --
│    │    └─MaxPool2d: 3-6               [1, 128, 56, 56]          --
│    └─Sequential: 2-3                   [1, 256, 28, 28]          --
│    │    └─Conv2d: 3-7                  [1, 256, 56, 56]          295,168
│    │    └─ReLU: 3-8                    [1, 256, 56, 56]          --
│    │    └─Conv2d: 3-9                  [1, 256, 56, 56]          590,080
│    │    └─ReLU: 3-10                   [1, 256, 56, 56]          --
│    │    └─MaxPool2d: 3-11              [1, 256, 28, 28]          --
│    └─Sequential: 2-4                   [1, 512, 14, 14]          --
│    │    └─Conv2d: 3-12                 [1, 512, 28, 28]          1,180,160
│    │    └─ReLU: 3-13                   [1, 512, 28, 28]          --
│    │    └─Conv2d: 3-14                 [1, 512, 28, 28]          2,359,808
│    │    └─ReLU: 3-15                   [1, 512, 28, 28]          --
│    │    └─MaxPool2d: 3-16              [1, 512, 14, 14]          --
│    └─Sequential: 2-5                   [1, 512, 7, 7]            --
│    │    └─Conv2d: 3-17                 [1, 512, 14, 14]          2,359,808
│    │    └─ReLU: 3-18                   [1, 512, 14, 14]          --
│    │    └─Conv2d: 3-19                 [1, 512, 14, 14]          2,359,808
│    │    └─ReLU: 3-20                   [1, 512, 14, 14]          --
│    │    └─MaxPool2d: 3-21              [1, 512, 7, 7]            --
├─Flatten: 1-2                           [1, 25088]                --
├─Sequential: 1-3                        [1, 1000]                 --
│    └─Linear: 2-6                       [1, 4096]                 102,764,544
│    └─ReLU: 2-7                         [1, 4096]                 --
│    └─Dropout: 2-8                      [1, 4096]                 --
│    └─Linear: 2-9                       [1, 4096]                 16,781,312
│    └─ReLU: 2-10                        [1, 4096]                 --
│    └─Dropout: 2-11                     [1, 4096]                 --
│    └─Linear: 2-12                      [1, 1000]                 4,097,000
==========================================================================================
Total params: 132,863,336
Trainable params: 132,863,336
Non-trainable params: 0
Total mult-adds (G): 7.62
==========================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 59.48
Params size (MB): 531.45
Estimated Total Size (MB): 591.54
==========================================================================================
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="resnet18">
<h4><span class="section-number">1.6.6.2. </span>Resnet18<a class="headerlink" href="#resnet18" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Resnet18 的網路架構如下：</p></li>
</ul>
<p><img alt="" src="../_images/resnet18-90.svg" /></p>
<ul class="simple">
<li><p>從這張架構圖，可以得知：</p>
<ul>
<li><p>img 剛進來時，先經過 7x7 conv -&gt; batch norm -&gt; 3x3 max pooling，得到 feature map.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">2x</span></code> 下面，看起來是一個 block，我先稱它為 <code class="docutils literal notranslate"><span class="pre">simple</span> <span class="pre">resnet</span> <span class="pre">block</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">3x</span></code> 下面，看起來是2個 block 串再一起，前面是多出一個 1x1 conv，我稱它為 <code class="docutils literal notranslate"><span class="pre">1x1</span> <span class="pre">conv</span> <span class="pre">resnet</span> <span class="pre">block</span></code>, 後面就是剛剛看過的 <code class="docutils literal notranslate"><span class="pre">simple</span> <span class="pre">resnet</span> <span class="pre">block</span></code>。所以這邊是 3 個 (1x1 conv resnet block + simple resnet block) 的結構</p></li>
<li><p>最後用 global average pooling，得到這張圖的 embedding (512維而已，好清爽).</p></li>
<li><p>然後接個 fully connected layer, output 到 ImageNet 要預測的 1000 個類別</p></li>
</ul>
</li>
<li><p>現在來細看一下 <code class="docutils literal notranslate"><span class="pre">2x</span></code>, <code class="docutils literal notranslate"><span class="pre">3x</span></code> 下的兩種 resnet block。如下圖：</p></li>
</ul>
<p><img alt="" src="../_images/resnet-block.svg" /></p>
<ul class="simple">
<li><p>右圖為啥要多一個 1x1 conv 呢？ 這是 shape 的考量：</p>
<ul>
<li><p>左圖 (simple resnet block)：</p>
<ul>
<li><p>一路的 conv, 都是通道數不變，高寬也不變(same padding)。</p></li>
<li><p>所以 input x 是 (b, c, h, w), 經過一系列的 conv 後，還是 (b, c, h, w)，就可以和原本的 input x 直接相加</p></li>
</ul>
</li>
<li><p>右圖 (1x1 conv resnet block)：</p>
<ul>
<li><p>一路的 conv, 會刻意將通道數翻倍，高寬減半(stride = 2)。</p></li>
<li><p>所以 input x 是 (b, c, h, w), 經過一系列的 conv 後，變成 (b, 2c, h/2, w/2)，那就和原本的 x 的 shape不同。</p></li>
<li><p>所以把 input 做 1x1 conv (input_channel = c, output_channel = 2c, kernel_size = 1, stride = 2, padding = 0), 就可以把 input x 也變成通道數翻倍, 高寬減半，就可以相加了</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>這樣對 resnet 應該頗瞭解了。先來偷看一下 pytorch 的 pretrained resnet18, 等等就要自己造一個出來</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from torchvision.models import resnet18
resnet = resnet18(weights=&quot;IMAGENET1K_V1&quot;)
resnet
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>nice，跟剛剛介紹的都一樣，唯一不樣的，是 <code class="docutils literal notranslate"><span class="pre">1x1</span> <span class="pre">conv</span> <span class="pre">resnet</span> <span class="pre">block</span></code> 的部分，他除了做 1x1 conv 外，又多做了 batch normalization。但這無傷大雅。</p></li>
<li><p>另外，為啥他要把它命名為 downsample? 因為他用 1x1 conv 把原本的 feature map 高寬減半(stride = 2, padding = 0)，所以是在做高寬的 down sample。</p></li>
<li><p>現在應該信心滿滿了～ 來自己寫一個吧！</p></li>
</ul>
<section id="resnet-block">
<h5><span class="section-number">1.6.6.2.1. </span>resnet block<a class="headerlink" href="#resnet-block" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class Residual(nn.Module):  #@save
    def __init__(self, input_channels, output_channels,
                 use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, output_channels,
                               kernel_size=3, padding=1, stride=strides)
        self.bn1 = nn.BatchNorm2d(output_channels)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(output_channels, output_channels,
                               kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(output_channels)
        if use_1x1conv:
            self.downsample = nn.Sequential(
                nn.Conv2d(input_channels, output_channels,kernel_size=1, stride=strides),
                nn.BatchNorm2d(output_channels)
            )
        else:
            self.downsample = None
        
    def forward(self, X):
        # 內層
        Y = self.conv1(X)
        Y = self.bn1(Y)
        Y = self.relu(Y)
        Y = self.conv2(Y)
        Y = self.bn2(Y)
        
        # 殘差連接層
        if self.downsample:
            X = self.downsample(X)
        Y += X
        
        # output
        Y = self.relu(Y)
        return Y
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># simple resnet block
blk = Residual(input_channels = 64, output_channels = 64, use_1x1conv = False)
blk
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Residual(
  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 1x1 conv resnet block
blk = Residual(input_channels = 64, output_channels = 128, use_1x1conv = True, strides = 2)
blk
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Residual(
  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (downsample): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="resnet-network">
<h5><span class="section-number">1.6.6.2.2. </span>resnet network<a class="headerlink" href="#resnet-network" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>再看一次圖：</p></li>
</ul>
<p><img alt="" src="../_images/resnet18-90.svg" /></p>
<ul class="simple">
<li><p>我想這樣組織我的 code:</p>
<ul>
<li><p>前面這三層，我叫 b1 (block1).</p></li>
<li><p>再來的 <code class="docutils literal notranslate"><span class="pre">2x</span></code>，我叫 b2, 就用 sequential 接2次剛剛寫好的 <code class="docutils literal notranslate"><span class="pre">Residual</span></code> class 就好</p></li>
<li><p>再來的 <code class="docutils literal notranslate"><span class="pre">3x</span></code>, 我想把 <code class="docutils literal notranslate"><span class="pre">conv</span> <span class="pre">resnet</span> <span class="pre">block</span></code>+<code class="docutils literal notranslate"><span class="pre">simple</span> <span class="pre">resnet</span> <span class="pre">block</span></code> 定義為 <code class="docutils literal notranslate"><span class="pre">compose_resnet_block</span></code>，然後分別用 b3, b4, b5 設定三組 block.</p></li>
<li><p>最後就接 global average pooling 和 fully connected layer 就搞定了</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>b1 = nn.Sequential(
    nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)),
    nn.BatchNorm2d(64),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
)

b2 = nn.Sequential(
    Residual(input_channels = 64, output_channels = 64, use_1x1conv = False),
    Residual(input_channels = 64, output_channels = 64, use_1x1conv = False)
)

def compose_resnet_block(input_channels, output_channels):
    out = nn.Sequential(
        Residual(
            input_channels = input_channels, 
            output_channels = output_channels, 
            use_1x1conv = True,
            strides = 2
        ),
        Residual(
            input_channels = output_channels, 
            output_channels = output_channels, 
            use_1x1conv = False,
            strides = 1
        )
    )
    return out

b3 = compose_resnet_block(input_channels = 64, output_channels = 128)
b4 = compose_resnet_block(input_channels = 128, output_channels = 256)
b5 = compose_resnet_block(input_channels = 256, output_channels = 512)

# 組起來吧
model = nn.Sequential(
    b1, 
    b2, 
    b3, b4, b5,
    nn.AdaptiveAvgPool2d((1,1)),
    nn.Flatten(), 
    nn.Linear(512, 1000)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (1): Sequential(
    (0): Residual(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): Residual(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (2): Sequential(
    (0): Residual(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU()
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Residual(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU()
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (3): Sequential(
    (0): Residual(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Residual(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (4): Sequential(
    (0): Residual(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU()
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Residual(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU()
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (5): AdaptiveAvgPool2d(output_size=(1, 1))
  (6): Flatten(start_dim=1, end_dim=-1)
  (7): Linear(in_features=512, out_features=1000, bias=True)
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>讚啦，跑一次看看:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>input_img = torch.rand(10, 3, 224, 224)
out = model(input_img)
out.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([10, 1000])
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="id13">
<h3><span class="section-number">1.6.7. </span>model 手術<a class="headerlink" href="#id13" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>以下整理各種對 model structure 做手術的作法</p></li>
</ul>
<section id="module">
<h4><span class="section-number">1.6.7.1. </span>往後疊加 module<a class="headerlink" href="#module" title="Permalink to this heading">#</a></h4>
<section id="seq-obj-append-module-module">
<h5><span class="section-number">1.6.7.1.1. </span>用 <code class="docutils literal notranslate"><span class="pre">Seq_obj.append(module)</span></code> 來增加 module<a class="headerlink" href="#seq-obj-append-module-module" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>假設我現在已經定義好兩個 module</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>feature_module = nn.Sequential(
    nn.Linear(800, 100),
    nn.ReLU(),
    nn.Linear(100, 50),
    nn.ReLU()
)
classifier_module = nn.Sequential(
    nn.Linear(50, 10),
    nn.ReLU(),
    nn.Linear(10, 1),
    nn.Sigmoid()
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>我可以建立一個空的 sequential 物件，然後把這些 module 給 append 進來</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>net = nn.Sequential()
net.append(feature_module)
net.append(classifier_module)
net
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Sequential(
    (0): Linear(in_features=800, out_features=100, bias=True)
    (1): ReLU()
    (2): Linear(in_features=100, out_features=50, bias=True)
    (3): ReLU()
  )
  (1): Sequential(
    (0): Linear(in_features=50, out_features=10, bias=True)
    (1): ReLU()
    (2): Linear(in_features=10, out_features=1, bias=True)
    (3): Sigmoid()
  )
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>可以發現，這個 <code class="docutils literal notranslate"><span class="pre">.append()</span></code>，就和 keras 的 <code class="docutils literal notranslate"><span class="pre">.add()</span></code> 是一樣的意思</p></li>
</ul>
</section>
<section id="seq-obj-add-module-name-module-module">
<h5><span class="section-number">1.6.7.1.2. </span>用 <code class="docutils literal notranslate"><span class="pre">Seq_obj.add_module(&quot;name&quot;,</span> <span class="pre">module)</span></code> 來增加帶有名稱的module<a class="headerlink" href="#seq-obj-add-module-name-module-module" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>假設我現在已經定義好兩個 module</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>feature_module = nn.Sequential(
    nn.Linear(800, 100),
    nn.ReLU(),
    nn.Linear(100, 50),
    nn.ReLU()
)
classifier_module = nn.Sequential(
    nn.Linear(50, 10),
    nn.ReLU(),
    nn.Linear(10, 1),
    nn.Sigmoid()
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>然後，我的 model，要整合這兩個，那我可以這樣做：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>net = nn.Sequential()
net.add_module(&quot;feature&quot;, feature_module)
net.add_module(&quot;classifier&quot;, classifier_module)
net
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (feature): Sequential(
    (0): Linear(in_features=800, out_features=100, bias=True)
    (1): ReLU()
    (2): Linear(in_features=100, out_features=50, bias=True)
    (3): ReLU()
  )
  (classifier): Sequential(
    (0): Linear(in_features=50, out_features=10, bias=True)
    (1): ReLU()
    (2): Linear(in_features=10, out_features=1, bias=True)
    (3): Sigmoid()
  )
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>net.append(feature_module)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (feature): Sequential(
    (0): Linear(in_features=800, out_features=100, bias=True)
    (1): ReLU()
    (2): Linear(in_features=100, out_features=50, bias=True)
    (3): ReLU()
  )
  (classifier): Sequential(
    (0): Linear(in_features=50, out_features=10, bias=True)
    (1): ReLU()
    (2): Linear(in_features=10, out_features=1, bias=True)
    (3): Sigmoid()
  )
  (2): Sequential(
    (0): Linear(in_features=800, out_features=100, bias=True)
    (1): ReLU()
    (2): Linear(in_features=100, out_features=50, bias=True)
    (3): ReLU()
  )
)
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
</section>
<section id="id14">
<h2><span class="section-number">1.7. </span>model 結構/參數管理<a class="headerlink" href="#id14" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>假設 model 長這樣：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model = nn.Sequential(
    nn.Linear(4, 8), 
    nn.ReLU(), 
    nn.Linear(8, 1)
)

optim = torch.optim.Adam(
    model.parameters(), lr=1e-4)

# loss
criterion = nn.MSELoss()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># device
device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)

# fake data: batch_size = 2
X = torch.rand(size=(2, 4))
y = torch.tensor([[0.7], [0.2]])

# one-epoch training
X = X.to(device)
y = y.to(device)
model = model.to(device)

y_hat = model(X)        # 把 x tensor 移到 GPU 計算
print(y_hat)

loss = criterion(y, y_hat) # 把 y tensor 移到 GPU 計算，
                                      ##  y_hat 因為是從 GPU model input GPU Tensor 出來的
                                      ##  所以不用再次 .to(device) 當然要也是沒差啦 =_=|||
optim.zero_grad() # 把 trainable variable/weights/parameters 的 gradient 給 歸 0
loss.backward() # 利用 loss，計算出每個 trainable variable/weights/parameters 所對應的 gradient
optim.step() # 更新 trainable variable/weights/parameters 的值： parameters_new = parameters_old - learning_rate * gradient
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.1426],
        [0.1753]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<section id="model-structure">
<h3><span class="section-number">1.7.1. </span>看 model structure<a class="headerlink" href="#model-structure" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Linear(in_features=4, out_features=8, bias=True)
  (1): ReLU()
  (2): Linear(in_features=8, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>可以看到，他用 index 來表明每一層的 layer</p></li>
</ul>
<ul class="simple">
<li><p>如果想看各層的 intput/output shape, 以及參數資訊，可以使用 <code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>summary(model, (100, 4)) # (100,4) 是 input shape，我假設一個 batch_size 是 100，所以寫成 (100, 4)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Sequential                               [100, 1]                  --
├─Linear: 1-1                            [100, 8]                  40
├─ReLU: 1-2                              [100, 8]                  --
├─Linear: 1-3                            [100, 1]                  9
==========================================================================================
Total params: 49
Trainable params: 49
Non-trainable params: 0
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.00
Estimated Total Size (MB): 0.01
==========================================================================================
</pre></div>
</div>
</div>
</div>
</section>
<section id="weight-bias-gradient">
<h3><span class="section-number">1.7.2. </span>看單一層的 weight, bias, gradient<a class="headerlink" href="#weight-bias-gradient" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>例如，看第 0 個 index 的 參數：</p></li>
</ul>
<section id="state-dict">
<h4><span class="section-number">1.7.2.1. </span><code class="docutils literal notranslate"><span class="pre">.state_dict()</span></code><a class="headerlink" href="#state-dict" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model[0].state_dict()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;weight&#39;,
              tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],
                      [ 0.4700,  0.4231, -0.2138,  0.2805],
                      [-0.3597, -0.1527, -0.3627, -0.2027],
                      [-0.3738, -0.0498, -0.0484, -0.1716],
                      [-0.0631,  0.4257, -0.2368,  0.3549],
                      [-0.4824, -0.3647, -0.3779,  0.1142],
                      [ 0.3761, -0.4852,  0.2199, -0.3184],
                      [-0.3334, -0.2057,  0.0335, -0.3001]], device=&#39;cuda:0&#39;)),
             (&#39;bias&#39;,
              tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],
                     device=&#39;cuda:0&#39;))])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>所以，要取得 weight 或 bias 的資料可以這樣拿：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model[0].state_dict()[&#39;weight&#39;]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],
        [ 0.4700,  0.4231, -0.2138,  0.2805],
        [-0.3597, -0.1527, -0.3627, -0.2027],
        [-0.3738, -0.0498, -0.0484, -0.1716],
        [-0.0631,  0.4257, -0.2368,  0.3549],
        [-0.4824, -0.3647, -0.3779,  0.1142],
        [ 0.3761, -0.4852,  0.2199, -0.3184],
        [-0.3334, -0.2057,  0.0335, -0.3001]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model[0].state_dict()[&#39;bias&#39;]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],
       device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="weight-weight-data-weight-grad">
<h4><span class="section-number">1.7.2.2. </span><code class="docutils literal notranslate"><span class="pre">.weight</span></code>, <code class="docutils literal notranslate"><span class="pre">.weight.data</span></code>, <code class="docutils literal notranslate"><span class="pre">.weight.grad</span></code><a class="headerlink" href="#weight-weight-data-weight-grad" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>除了這種做法外，也可以用 <code class="docutils literal notranslate"><span class="pre">.weight</span></code> 取得 weight 物件，再往下去取得 data 和 gradient 資訊：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(model[0].weight) # 這是物件
print(type(model[0].weight))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],
        [ 0.4700,  0.4231, -0.2138,  0.2805],
        [-0.3597, -0.1527, -0.3627, -0.2027],
        [-0.3738, -0.0498, -0.0484, -0.1716],
        [-0.0631,  0.4257, -0.2368,  0.3549],
        [-0.4824, -0.3647, -0.3779,  0.1142],
        [ 0.3761, -0.4852,  0.2199, -0.3184],
        [-0.3334, -0.2057,  0.0335, -0.3001]], device=&#39;cuda:0&#39;,
       requires_grad=True)
&lt;class &#39;torch.nn.parameter.Parameter&#39;&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 取這個物件，底下的 data (i.e. value)
model[0].weight.data
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],
        [ 0.4700,  0.4231, -0.2138,  0.2805],
        [-0.3597, -0.1527, -0.3627, -0.2027],
        [-0.3738, -0.0498, -0.0484, -0.1716],
        [-0.0631,  0.4257, -0.2368,  0.3549],
        [-0.4824, -0.3647, -0.3779,  0.1142],
        [ 0.3761, -0.4852,  0.2199, -0.3184],
        [-0.3334, -0.2057,  0.0335, -0.3001]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># gradient
model[0].weight.grad
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0074, -0.0270, -0.0137, -0.0392],
        [ 0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="bias-bias-data-bias-grad">
<h4><span class="section-number">1.7.2.3. </span><code class="docutils literal notranslate"><span class="pre">.bias</span></code>, <code class="docutils literal notranslate"><span class="pre">.bias.data</span></code>, <code class="docutils literal notranslate"><span class="pre">.bias.grad</span></code><a class="headerlink" href="#bias-bias-data-bias-grad" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model[0].bias
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],
       device=&#39;cuda:0&#39;, requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model[0].bias.data
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],
       device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model[0].bias.grad
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.0000, -0.0590,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
       device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="parameters">
<h4><span class="section-number">1.7.2.4. </span><code class="docutils literal notranslate"><span class="pre">.parameters()</span></code><a class="headerlink" href="#parameters" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>也可以用 <code class="docutils literal notranslate"><span class="pre">.parameters</span></code>，出來的會是物件：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model[0].parameters()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;generator object Module.parameters at 0x7f69c250d740&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>list(model[0].parameters())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Parameter containing:
 tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],
         [ 0.4700,  0.4231, -0.2138,  0.2805],
         [-0.3597, -0.1527, -0.3627, -0.2027],
         [-0.3738, -0.0498, -0.0484, -0.1716],
         [-0.0631,  0.4257, -0.2368,  0.3549],
         [-0.4824, -0.3647, -0.3779,  0.1142],
         [ 0.3761, -0.4852,  0.2199, -0.3184],
         [-0.3334, -0.2057,  0.0335, -0.3001]], device=&#39;cuda:0&#39;,
        requires_grad=True),
 Parameter containing:
 tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],
        device=&#39;cuda:0&#39;, requires_grad=True)]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>可以看到，list 裡面的第一個 element，很明顯是 weight, 第二個 element，很明顯是 bias，兩個都是物件，所以真的要取資料時，可以這樣取：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for idx, param in enumerate(model[0].parameters()):
    print(idx)
    print(param.data)
    print(param.grad)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0
tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],
        [ 0.4700,  0.4231, -0.2138,  0.2805],
        [-0.3597, -0.1527, -0.3627, -0.2027],
        [-0.3738, -0.0498, -0.0484, -0.1716],
        [-0.0631,  0.4257, -0.2368,  0.3549],
        [-0.4824, -0.3647, -0.3779,  0.1142],
        [ 0.3761, -0.4852,  0.2199, -0.3184],
        [-0.3334, -0.2057,  0.0335, -0.3001]], device=&#39;cuda:0&#39;)
tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0074, -0.0270, -0.0137, -0.0392],
        [ 0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000]], device=&#39;cuda:0&#39;)
1
tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],
       device=&#39;cuda:0&#39;)
tensor([ 0.0000, -0.0590,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
       device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="named-parameters">
<h4><span class="section-number">1.7.2.5. </span><code class="docutils literal notranslate"><span class="pre">.named_parameters</span></code><a class="headerlink" href="#named-parameters" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>list(model[0].named_parameters())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;weight&#39;,
  Parameter containing:
  tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],
          [ 0.4700,  0.4231, -0.2138,  0.2805],
          [-0.3597, -0.1527, -0.3627, -0.2027],
          [-0.3738, -0.0498, -0.0484, -0.1716],
          [-0.0631,  0.4257, -0.2368,  0.3549],
          [-0.4824, -0.3647, -0.3779,  0.1142],
          [ 0.3761, -0.4852,  0.2199, -0.3184],
          [-0.3334, -0.2057,  0.0335, -0.3001]], device=&#39;cuda:0&#39;,
         requires_grad=True)),
 (&#39;bias&#39;,
  Parameter containing:
  tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],
         device=&#39;cuda:0&#39;, requires_grad=True))]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="parameters-weight-bias-gradient">
<h3><span class="section-number">1.7.3. </span>看所有的 parameters, weight, bias, gradient<a class="headerlink" href="#parameters-weight-bias-gradient" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>有了前面的練習，應該就不難理解一次看全部的結果：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Linear(in_features=4, out_features=8, bias=True)
  (1): ReLU()
  (2): Linear(in_features=8, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>list(model.parameters())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Parameter containing:
 tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],
         [ 0.4700,  0.4231, -0.2138,  0.2805],
         [-0.3597, -0.1527, -0.3627, -0.2027],
         [-0.3738, -0.0498, -0.0484, -0.1716],
         [-0.0631,  0.4257, -0.2368,  0.3549],
         [-0.4824, -0.3647, -0.3779,  0.1142],
         [ 0.3761, -0.4852,  0.2199, -0.3184],
         [-0.3334, -0.2057,  0.0335, -0.3001]], device=&#39;cuda:0&#39;,
        requires_grad=True),
 Parameter containing:
 tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],
        device=&#39;cuda:0&#39;, requires_grad=True),
 Parameter containing:
 tensor([[-0.2124,  0.1014, -0.2084,  0.1536,  0.1722,  0.1967, -0.1082, -0.0799]],
        device=&#39;cuda:0&#39;, requires_grad=True),
 Parameter containing:
 tensor([0.0692], device=&#39;cuda:0&#39;, requires_grad=True)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model.state_dict()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;0.weight&#39;,
              tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],
                      [ 0.4700,  0.4231, -0.2138,  0.2805],
                      [-0.3597, -0.1527, -0.3627, -0.2027],
                      [-0.3738, -0.0498, -0.0484, -0.1716],
                      [-0.0631,  0.4257, -0.2368,  0.3549],
                      [-0.4824, -0.3647, -0.3779,  0.1142],
                      [ 0.3761, -0.4852,  0.2199, -0.3184],
                      [-0.3334, -0.2057,  0.0335, -0.3001]], device=&#39;cuda:0&#39;)),
             (&#39;0.bias&#39;,
              tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],
                     device=&#39;cuda:0&#39;)),
             (&#39;2.weight&#39;,
              tensor([[-0.2124,  0.1014, -0.2084,  0.1536,  0.1722,  0.1967, -0.1082, -0.0799]],
                     device=&#39;cuda:0&#39;)),
             (&#39;2.bias&#39;, tensor([0.0692], device=&#39;cuda:0&#39;))])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>可以看到 weight 和 bias 前面，有加上 index (i.e. 0 和 2)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model.named_parameters()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;generator object Module.named_parameters at 0x7f69c250dac0&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>list(model.named_parameters())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;0.weight&#39;,
  Parameter containing:
  tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],
          [ 0.4700,  0.4231, -0.2138,  0.2805],
          [-0.3597, -0.1527, -0.3627, -0.2027],
          [-0.3738, -0.0498, -0.0484, -0.1716],
          [-0.0631,  0.4257, -0.2368,  0.3549],
          [-0.4824, -0.3647, -0.3779,  0.1142],
          [ 0.3761, -0.4852,  0.2199, -0.3184],
          [-0.3334, -0.2057,  0.0335, -0.3001]], device=&#39;cuda:0&#39;,
         requires_grad=True)),
 (&#39;0.bias&#39;,
  Parameter containing:
  tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],
         device=&#39;cuda:0&#39;, requires_grad=True)),
 (&#39;2.weight&#39;,
  Parameter containing:
  tensor([[-0.2124,  0.1014, -0.2084,  0.1536,  0.1722,  0.1967, -0.1082, -0.0799]],
         device=&#39;cuda:0&#39;, requires_grad=True)),
 (&#39;2.bias&#39;,
  Parameter containing:
  tensor([0.0692], device=&#39;cuda:0&#39;, requires_grad=True))]
</pre></div>
</div>
</div>
</div>
</section>
<section id="block-factory">
<h3><span class="section-number">1.7.4. </span>block factory<a class="headerlink" href="#block-factory" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>如果我們把 block 給 nested 在一起，那要如何做參數管理？</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0930, 0.4586, 0.2298, 0.6663],
        [0.8720, 0.4310, 0.2996, 0.6085]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def block1():
    
    out = nn.Sequential(
        nn.Linear(4, 8), 
        nn.ReLU(),
        nn.Linear(8, 4), 
        nn.ReLU()
    )
    
    return out 

def block2():
    net = nn.Sequential()
    for i in range(4):
        # 在這裡 nested
        net.add_module(f&#39;block {i}&#39;, block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))

X = torch.rand(size=(2, 4))
rgnet(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.2688],
        [-0.2687]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(rgnet)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Sequential(
    (block 0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>那要取資訊時，就是一層一層往下取就好：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>rgnet[0][1][0].bias.data
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.4000, -0.2344, -0.1909, -0.0215,  0.2354,  0.1416,  0.0388,  0.4069])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id15">
<h2><span class="section-number">1.8. </span>參數初始化<a class="headerlink" href="#id15" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>知道如何訪問參數後，現在來講如何初始化參數.</p></li>
<li><p>這要用到 pytorch 的 <code class="docutils literal notranslate"><span class="pre">nn.init</span></code> module 提供的多種方法</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># weight 和 bias 的初始值都設為 N(0, 0.01) 的 init
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)

        
model = nn.Sequential(
    nn.Linear(4, 8), 
    nn.ReLU(), 
    nn.Linear(8, 1)
)

model.apply(init_normal)
model[0].weight.data, model[0].bias.data
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[ 0.0069,  0.0011, -0.0006, -0.0160],
         [ 0.0032, -0.0044, -0.0067,  0.0041],
         [ 0.0032,  0.0114, -0.0113,  0.0095],
         [-0.0025,  0.0135, -0.0093,  0.0144],
         [ 0.0006, -0.0119, -0.0035,  0.0295],
         [-0.0087, -0.0041, -0.0280, -0.0009],
         [-0.0034, -0.0170, -0.0034, -0.0019],
         [-0.0054,  0.0019,  0.0085, -0.0161]]),
 tensor([0., 0., 0., 0., 0., 0., 0., 0.]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># weight 的 初始值都設為 1, bias 都設為 0
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1)
        nn.init.zeros_(m.bias)

model.apply(init_constant)

model[0].weight.data, model[0].bias.data
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]]),
 tensor([0., 0., 0., 0., 0., 0., 0., 0.]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># xavier
def init_xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)

model.apply(init_xavier)

model[0].weight.data, model[0].bias.data
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[ 0.5764,  0.2257,  0.2232, -0.3147],
         [ 0.0899, -0.4019, -0.2921, -0.4732],
         [ 0.4910,  0.1927,  0.1129, -0.7005],
         [-0.2115,  0.6684,  0.2473,  0.4178],
         [-0.3668,  0.1869, -0.1139,  0.0763],
         [ 0.5566, -0.4967,  0.3012,  0.1808],
         [-0.1479, -0.0274,  0.2649,  0.2588],
         [-0.0047,  0.4642,  0.2031, -0.4086]]),
 tensor([0., 0., 0., 0., 0., 0., 0., 0.]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 自訂義初始化
# weight 有 1/4 的可能性，來自 U(5, 10), 1/4 可能性來自 U(-10, -5), 1/2 可能性是 0

def my_init(m):
    if type(m) == nn.Linear:
        print(&quot;Init&quot;, *[(name, param.shape) for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() &gt;= 5

model.apply(my_init)
model[0].weight
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Init weight torch.Size([8, 4])
Init weight torch.Size([1, 8])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([[ 0.0000,  0.0000, -0.0000, -0.0000],
        [-9.4019,  0.0000, -6.3883, -9.9278],
        [ 6.4976,  6.2993, -0.0000, -0.0000],
        [ 0.0000, -0.0000, -0.0000,  0.0000],
        [-0.0000,  0.0000,  5.7446, -0.0000],
        [-7.2997,  0.0000, -6.1244,  0.0000],
        [ 6.6610,  0.0000, -0.0000, -0.0000],
        [-0.0000,  0.0000, -8.7791, -5.9247]], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>我們也可以自己設定參數</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model[0].weight.data[:] += 1
model[0].weight.data[0, 0] = 42
model[0].weight.data[0]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([42.,  1.,  1.,  1.])
</pre></div>
</div>
</div>
</div>
</section>
<section id="transfer-learning">
<h2><span class="section-number">1.9. </span>Transfer learning<a class="headerlink" href="#transfer-learning" title="Permalink to this heading">#</a></h2>
<section id="image-classification">
<h3><span class="section-number">1.9.1. </span>image classification<a class="headerlink" href="#image-classification" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>以下的 pretrained model，都是用 ImageNet 100 萬張圖片作為 training set, 內含 1000 種類別(日常生活中會看到的動物、植物、交通工具…)</p></li>
<li><p>input size 要是 (3, 224, 224)</p></li>
</ul>
<section id="id16">
<h4><span class="section-number">1.9.1.1. </span>vgg11<a class="headerlink" href="#id16" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from torchvision.models import vgg11
# from torchvision.models import resnet50, vgg11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>vgg = vgg11(weights=&quot;IMAGENET1K_V1&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>vgg
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): ReLU(inplace=True)
    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (14): ReLU(inplace=True)
    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): ReLU(inplace=True)
    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): ReLU(inplace=True)
    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
<section id="id17">
<h5><span class="section-number">1.9.1.1.1. </span>直接用<a class="headerlink" href="#id17" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>vgg.features[0].in_features
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AttributeError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">169</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">vgg</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">in_features</span>

<span class="nn">File ~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1207,</span> in <span class="ni">Module.__getattr__</span><span class="nt">(self, name)</span>
<span class="g g-Whitespace">   </span><span class="mi">1205</span>     <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">modules</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1206</span>         <span class="k">return</span> <span class="n">modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
<span class="ne">-&gt; </span><span class="mi">1207</span> <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">{}</span><span class="s2">&#39; object has no attribute &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1208</span>     <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>

<span class="ne">AttributeError</span>: &#39;Conv2d&#39; object has no attribute &#39;in_features&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>input_img = torch.rand((1, 3, 224, 224))
out = vgg(input_img)
print(out.shape)
print(out.argmax())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 1000])
tensor(677)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>預測結果是 1000 類中的第 677 類</p></li>
</ul>
</section>
<section id="featrue-extraction">
<h5><span class="section-number">1.9.1.1.2. </span>僅作 featrue extraction<a class="headerlink" href="#featrue-extraction" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>看一下 vgg 的結構，可以知道，取到 avgpool block，就會完成 feature extraction</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>vgg_feature_extraction = nn.Sequential()
vgg_feature_extraction.add_module(&quot;features&quot;, vgg.features)
vgg_feature_extraction.add_module(&quot;avgpool&quot;, vgg.avgpool)

# 看一下 output size
input_img = torch.rand((1, 3, 224, 224))
out_feature = vgg_feature_extraction(input_img)
out_feature.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 512, 7, 7])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>flatten_layer = nn.Flatten()
out_feature_flatten = flatten_layer(out_feature)
out_feature_flatten.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 25088])
</pre></div>
</div>
</div>
</div>
</section>
<section id="id18">
<h5><span class="section-number">1.9.1.1.3. </span>把最後一層分類層，換掉<a class="headerlink" href="#id18" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>m = torchvision.ops.FeaturePyramidNetwork([10, 20, 30], 5)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>m
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>FeaturePyramidNetwork(
  (inner_blocks): ModuleList(
    (0): Conv2dNormActivation(
      (0): Conv2d(10, 5, kernel_size=(1, 1), stride=(1, 1))
    )
    (1): Conv2dNormActivation(
      (0): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): Conv2dNormActivation(
      (0): Conv2d(30, 5, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (layer_blocks): ModuleList(
    (0): Conv2dNormActivation(
      (0): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): Conv2dNormActivation(
      (0): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): Conv2dNormActivation(
      (0): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = OrderedDict()
x[&#39;feat0&#39;] = torch.rand(1, 10, 64, 64)
x[&#39;feat2&#39;] = torch.rand(1, 20, 16, 16)
x[&#39;feat3&#39;] = torch.rand(1, 30, 8, 8)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>output = m(x)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print([(k, v.shape) for k, v in output.items()])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;feat0&#39;, torch.Size([1, 5, 64, 64])), (&#39;feat2&#39;, torch.Size([1, 5, 16, 16])), (&#39;feat3&#39;, torch.Size([1, 5, 8, 8]))]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>m(x)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;feat0&#39;,
              tensor([[[[ 0.0651,  0.7307,  0.6804,  ...,  0.2749, -0.6173, -0.0506],
                        [ 0.3674,  0.7479,  0.3160,  ...,  0.3150,  0.4875,  0.3863],
                        [ 0.5469,  0.9629,  0.9104,  ...,  0.3155,  0.2938,  0.3751],
                        ...,
                        [-0.2186,  0.0819, -0.1050,  ...,  0.0866,  0.0996,  0.2195],
                        [ 0.0311,  0.2352,  0.7405,  ...,  0.0981, -0.3773,  0.1260],
                        [-0.4862,  0.0621,  0.0980,  ...,  0.7075,  0.0998,  0.4463]],
              
                       [[ 0.7306,  0.8579,  0.7388,  ...,  0.7095,  0.7229,  0.3815],
                        [ 1.1151,  0.4164,  0.6854,  ..., -0.6103,  0.6109,  0.1263],
                        [ 0.8844,  1.1289,  1.4066,  ..., -0.0380,  0.2836,  0.3413],
                        ...,
                        [ 0.8182,  0.6711,  0.7099,  ...,  1.6390,  1.3783,  1.4258],
                        [ 1.2211,  0.7030,  1.3479,  ...,  1.6805,  1.6704,  1.7298],
                        [ 0.3717,  0.2999,  0.4409,  ...,  1.1150,  0.8860,  0.9967]],
              
                       [[-0.7537, -0.1777, -0.2474,  ...,  0.4797, -0.2557,  0.4711],
                        [ 0.2142, -0.0742, -0.0778,  ...,  0.1681,  0.0158,  0.1151],
                        [ 0.4050, -0.3101, -0.1056,  ..., -0.3480,  0.1322,  0.2239],
                        ...,
                        [ 0.0886,  0.4967, -0.1118,  ...,  0.3715,  0.1221,  0.0528],
                        [-0.0165, -0.1399,  0.5543,  ..., -0.0486,  0.0021,  0.4598],
                        [-0.1852,  1.0833,  0.5247,  ...,  0.5244,  0.2833,  0.3631]],
              
                       [[ 0.2271, -0.0982,  0.0698,  ...,  0.0129, -0.0039,  0.3332],
                        [-0.1630, -0.3354, -0.0763,  ..., -0.1846,  0.0825, -0.5498],
                        [-0.2252, -0.6576, -0.5578,  ..., -0.2717,  0.0680,  0.2112],
                        ...,
                        [-0.4049, -0.6838, -0.8301,  ..., -0.3974, -0.5273, -0.0557],
                        [-0.8159, -0.7860, -0.5461,  ..., -0.5485,  0.0715, -0.3864],
                        [ 0.1099, -0.1757, -0.6333,  ..., -0.5165, -0.7815, -0.5082]],
              
                       [[-0.1099,  0.0519,  0.1517,  ..., -0.3964,  0.1471, -0.6940],
                        [-0.3098, -0.1143, -0.4689,  ..., -0.2337, -0.7096, -0.6646],
                        [-0.6796, -0.5282, -0.6652,  ..., -0.1974, -0.7837, -0.5769],
                        ...,
                        [-0.5142, -1.2376, -1.2597,  ..., -0.7708, -0.3910, -1.0117],
                        [-1.4930, -1.5087, -1.9965,  ..., -0.2917, -0.4884, -0.8217],
                        [ 0.0216, -1.1398, -0.8292,  ..., -0.5250, -0.5395, -0.7740]]]],
                     grad_fn=&lt;ConvolutionBackward0&gt;)),
             (&#39;feat2&#39;,
              tensor([[[[-1.0046e+00, -9.6859e-02, -4.3792e-01,  ..., -6.0342e-01,
                         -6.5079e-01, -4.2325e-01],
                        [-1.1751e+00, -6.9843e-01, -8.9345e-02,  ..., -4.9964e-01,
                         -5.3205e-01,  3.3596e-01],
                        [-5.8288e-01, -4.1455e-01, -1.1638e+00,  ..., -2.3590e-01,
                         -8.0533e-01, -1.2404e-02],
                        ...,
                        [-1.1438e+00, -1.1373e+00, -1.1463e+00,  ..., -1.0788e+00,
                         -1.5738e+00, -1.7807e-01],
                        [-1.8233e+00, -1.6144e+00, -1.5602e+00,  ..., -4.5544e-01,
                         -1.1697e+00, -7.6519e-02],
                        [-7.7319e-01, -6.0476e-01, -6.8770e-01,  ..., -2.7921e-01,
                         -3.6239e-01,  8.8864e-02]],
              
                       [[-3.3570e-01, -6.8396e-01,  2.0678e-01,  ...,  1.2873e-01,
                          3.0306e-01,  3.1674e-01],
                        [-1.4044e-01,  2.8896e-01,  1.2419e+00,  ...,  1.3313e+00,
                          1.2536e+00,  6.6258e-01],
                        [-4.7133e-01,  3.4678e-01,  6.3321e-01,  ...,  9.4539e-01,
                          1.1835e+00,  9.3823e-01],
                        ...,
                        [-1.5316e-01,  5.3091e-01,  4.0453e-01,  ...,  4.7463e-01,
                          3.2042e-01, -5.3936e-01],
                        [ 1.8852e-02,  4.7117e-01,  5.1071e-01,  ...,  6.2206e-01,
                         -9.5498e-03,  1.2806e-01],
                        [-1.5201e-01,  4.9281e-01,  4.3185e-01,  ...,  1.0739e+00,
                          2.7759e-01,  1.8371e-01]],
              
                       [[-1.9014e-01, -1.6418e-01, -3.9257e-01,  ..., -7.9676e-02,
                         -4.5493e-01,  3.7891e-01],
                        [-1.0331e-01,  3.9554e-01, -1.2823e-01,  ...,  2.0256e-01,
                         -9.3130e-02,  3.1498e-01],
                        [ 2.4490e-01,  3.1144e-01,  6.3074e-01,  ...,  8.5702e-01,
                          2.8472e-01,  5.1317e-01],
                        ...,
                        [-2.9996e-01,  6.8361e-01,  1.2271e+00,  ...,  3.7819e-01,
                          2.1921e-01,  2.1288e-01],
                        [ 2.6015e-01,  1.3184e+00,  1.2651e+00,  ...,  1.1742e-03,
                          5.5129e-01,  1.0867e+00],
                        [ 6.0323e-01,  2.1325e+00,  2.0986e+00,  ...,  3.7722e-01,
                          8.7845e-01,  1.0769e+00]],
              
                       [[-6.9043e-01, -9.1665e-02,  6.7821e-02,  ..., -3.1753e-01,
                          1.2908e-01,  5.9732e-01],
                        [-6.4826e-01,  1.8252e-01,  5.4596e-01,  ...,  6.1147e-01,
                          9.8260e-01,  1.1496e+00],
                        [-7.9203e-01,  4.0190e-01, -2.8135e-01,  ...,  2.5329e-01,
                          7.5300e-01,  5.0764e-01],
                        ...,
                        [-1.4809e+00, -4.9529e-01, -4.7891e-01,  ..., -2.4788e-01,
                          1.1538e-01,  3.3786e-01],
                        [-1.4629e+00, -1.4426e+00, -4.4455e-01,  ...,  1.8282e-01,
                          1.1021e-02,  1.9584e-01],
                        [-6.7977e-01, -4.5405e-01, -1.0277e-01,  ...,  3.5755e-01,
                          1.1750e-01,  7.0216e-01]],
              
                       [[-4.4328e-03,  5.4331e-01,  5.1264e-01,  ...,  3.2425e-01,
                          2.9698e-01, -2.6950e-01],
                        [-9.7634e-02, -4.8624e-01, -1.6891e-01,  ..., -1.1064e+00,
                         -6.1964e-01, -3.3575e-01],
                        [ 6.3835e-01, -2.3533e-01, -5.9266e-01,  ..., -7.0922e-01,
                         -5.5635e-01, -5.4694e-01],
                        ...,
                        [ 5.0356e-01, -7.5742e-01, -7.6203e-01,  ..., -9.3704e-01,
                         -1.1793e+00, -3.7278e-01],
                        [-4.5237e-01, -1.1845e+00, -1.3505e+00,  ...,  7.0696e-02,
                         -9.2490e-01, -6.8261e-02],
                        [ 8.7253e-01,  2.6779e-01, -1.4501e-01,  ...,  1.1677e+00,
                          8.8643e-01, -1.6407e-01]]]], grad_fn=&lt;ConvolutionBackward0&gt;)),
             (&#39;feat3&#39;,
              tensor([[[[-2.8874e-01, -2.2276e-01, -3.8304e-01, -2.4295e-01, -3.5041e-01,
                         -2.4153e-01, -6.5777e-01,  6.9771e-02],
                        [-5.8159e-01, -9.2228e-01, -1.0400e+00, -8.7284e-01, -6.6970e-01,
                         -4.0442e-01, -5.9494e-01, -3.7480e-01],
                        [-6.8877e-01, -6.7432e-01, -8.9043e-01, -1.9639e-01, -9.9080e-01,
                         -5.9162e-01, -8.8178e-01, -6.0240e-01],
                        [-8.2693e-01, -4.4837e-01, -9.2648e-01, -7.7054e-01, -1.2257e+00,
                         -1.1067e-01, -1.3667e+00, -9.1597e-03],
                        [-9.7302e-01, -6.8912e-01, -1.2097e+00, -4.4555e-01, -7.1897e-01,
                         -1.0317e+00, -4.8158e-01, -6.7882e-01],
                        [-8.0409e-01, -4.5036e-01,  4.8329e-02, -7.0340e-01, -1.0655e+00,
                         -5.1758e-01, -1.1960e+00, -4.8788e-01],
                        [-8.2313e-01, -6.6361e-01, -1.0613e+00, -7.8586e-01, -5.8351e-01,
                         -8.9468e-01, -1.0915e+00, -8.5448e-01],
                        [-6.4070e-01, -4.5867e-01, -4.3224e-01, -5.8518e-01, -6.6837e-01,
                         -7.0784e-01, -9.8576e-01,  1.4557e-02]],
              
                       [[-3.9968e-01, -6.7951e-01, -3.7101e-02, -5.9991e-01, -6.8091e-01,
                         -6.0446e-01, -7.9494e-01, -3.2561e-01],
                        [ 5.0599e-01,  8.7972e-02,  5.2346e-01,  1.1415e-01, -1.0149e-01,
                          1.2449e-01, -1.2265e-01, -7.4945e-02],
                        [ 2.0567e-01, -3.0807e-02, -2.1787e-01, -2.6581e-02, -1.5607e-01,
                          1.1049e-01,  2.7278e-01, -3.7792e-02],
                        [ 6.4684e-01,  1.2713e-01, -2.7874e-01,  4.4691e-01, -3.8186e-03,
                          1.8082e-01, -8.0807e-02,  2.7791e-01],
                        [ 2.7915e-01,  3.1683e-01, -1.5861e-01,  3.3136e-02, -2.2338e-01,
                         -3.1305e-01, -1.2926e-01, -9.7007e-02],
                        [ 3.5112e-01,  2.5498e-01, -1.8532e-01, -5.6975e-01, -3.1606e-01,
                         -2.4860e-01, -1.4100e-01,  3.4120e-01],
                        [ 1.0163e-01, -9.3440e-02, -1.7757e-02,  7.3701e-02,  4.6703e-01,
                          3.7238e-01,  6.9386e-01,  5.3925e-01],
                        [ 2.6343e-01,  4.3834e-01, -6.1088e-02,  1.7633e-01, -1.2902e-01,
                          3.0297e-01,  2.8894e-01,  5.1168e-01]],
              
                       [[-6.8676e-02,  6.0822e-01, -7.7878e-02,  3.9836e-02,  6.3387e-01,
                         -1.4000e-01,  2.6515e-01, -2.1326e-01],
                        [ 3.7375e-01,  2.6949e-01,  9.4511e-02,  8.0981e-02,  1.7669e-01,
                         -1.9057e-01,  9.2003e-01,  3.9352e-01],
                        [-4.3951e-02, -2.7200e-01,  5.0824e-01,  1.3231e-01,  4.8295e-01,
                         -2.5074e-01,  2.7287e-01,  1.8617e-01],
                        [-4.4838e-01,  5.2833e-01,  7.8973e-01,  2.7389e-01,  3.6471e-01,
                          1.5844e-01, -1.5481e-01,  1.4131e-02],
                        [ 2.0741e-01,  3.2294e-01,  2.2939e-01,  6.6524e-02,  5.6415e-01,
                          1.6167e-01,  4.2358e-01,  1.8817e-01],
                        [ 6.3459e-02, -9.8572e-02,  2.6159e-01,  3.9043e-01,  2.5657e-01,
                         -4.2722e-01,  3.8625e-01,  5.6028e-02],
                        [ 5.2971e-02,  1.4209e-01,  8.2037e-01,  1.3158e-01, -3.3500e-01,
                          4.0593e-01,  1.6757e-02,  5.1574e-01],
                        [-1.0005e-01,  6.0240e-02,  5.9779e-01, -6.1668e-02,  1.3741e-01,
                          2.8261e-01, -5.6418e-02,  4.9936e-01]],
              
                       [[ 1.3703e-01, -2.0366e-01, -2.5322e-01, -1.5589e-01, -4.2823e-01,
                         -3.6472e-01,  4.7253e-04, -1.9252e-01],
                        [-2.3838e-01, -1.5146e-01, -6.6108e-01, -8.0034e-01, -7.6699e-02,
                         -4.7165e-01, -6.0114e-01, -4.9989e-01],
                        [-2.7868e-01, -6.9715e-01, -3.1942e-01, -3.8800e-01, -4.3677e-01,
                         -7.3560e-01, -3.9836e-01, -6.3876e-01],
                        [-4.8928e-01, -8.1613e-01, -3.3260e-01, -9.0506e-01, -1.9451e-01,
                         -6.4661e-01, -6.6611e-01, -3.6382e-01],
                        [-4.7075e-01, -7.0676e-01, -3.9286e-01, -1.1942e+00, -3.7660e-01,
                         -6.0977e-01, -3.2889e-01, -4.6632e-01],
                        [-3.5595e-01, -5.6389e-01, -8.8521e-01, -8.5002e-01, -6.1210e-01,
                         -3.0098e-01, -7.2063e-01, -2.3527e-01],
                        [-6.4030e-01, -8.8976e-01, -5.1042e-01, -1.3868e-01, -6.5054e-01,
                         -5.0046e-01, -2.1001e-01, -4.3701e-01],
                        [-4.1798e-01, -7.9194e-01, -6.1039e-01, -3.9954e-01, -8.7290e-02,
                         -4.6569e-02, -1.9503e-01,  9.9226e-02]],
              
                       [[ 3.7162e-01,  5.4013e-01,  6.0619e-01,  2.8003e-01,  3.6680e-01,
                         -8.3289e-02, -6.0576e-02, -4.1560e-01],
                        [ 1.0384e+00,  5.6891e-01,  7.9417e-01,  2.9500e-01,  3.8786e-01,
                          4.4341e-01,  6.6960e-01,  6.6912e-01],
                        [ 1.2105e+00,  3.1149e-01,  5.8467e-01,  6.5764e-01,  1.0214e-01,
                          2.5588e-01,  8.4963e-01,  8.0353e-01],
                        [ 1.0731e+00,  7.8519e-01,  4.5299e-01,  1.1584e+00,  8.1042e-01,
                          6.6112e-01,  5.2571e-01,  6.5583e-01],
                        [ 7.7312e-01,  6.8071e-01,  6.6579e-01,  8.3232e-01,  7.0434e-01,
                          7.6747e-01,  1.0043e+00,  3.7484e-01],
                        [ 1.1412e+00,  7.9162e-01,  8.4955e-01,  7.3521e-01,  1.0017e+00,
                          8.3194e-01,  1.0010e+00,  4.7049e-01],
                        [ 9.8399e-01,  7.7165e-01,  5.7194e-01,  4.3298e-01,  7.6182e-01,
                          9.7778e-01,  4.0487e-01,  5.8800e-01],
                        [ 8.9305e-01,  1.3038e+00,  1.0443e+00,  7.9219e-01,  8.3838e-01,
                          6.6010e-01,  9.0881e-01,  8.6897e-01]]]],
                     grad_fn=&lt;ConvolutionBackward0&gt;))])
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
</section>
<section id="classical-layers">
<h2><span class="section-number">1.10. </span>Classical Layers<a class="headerlink" href="#classical-layers" title="Permalink to this heading">#</a></h2>
<section id="nn">
<h3><span class="section-number">1.10.1. </span>NN<a class="headerlink" href="#nn" title="Permalink to this heading">#</a></h3>
<section id="nn-linear-in-dim-out-dim">
<h4><span class="section-number">1.10.1.1. </span><code class="docutils literal notranslate"><span class="pre">nn.Linear(in_dim,</span> <span class="pre">out_dim)</span></code><a class="headerlink" href="#nn-linear-in-dim-out-dim" title="Permalink to this heading">#</a></h4>
</section>
</section>
<section id="flatten">
<h3><span class="section-number">1.10.2. </span>Flatten<a class="headerlink" href="#flatten" title="Permalink to this heading">#</a></h3>
<section id="torch-nn-flatten-start-dim-1-end-dim-1">
<h4><span class="section-number">1.10.2.1. </span><code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten(start_dim=1,</span> <span class="pre">end_dim=-</span> <span class="pre">1)</span></code><a class="headerlink" href="#torch-nn-flatten-start-dim-1-end-dim-1" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>預設會從 dim = 1 開始，是因為 dim = 0 是 batch 的軸</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = torch.rand(2, 3, 5, 5) # batch_size = 2, 每個 batch，都有 num_channel, Height, Width 的 image
print(X)
print(X.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[0.1776, 0.9160, 0.8642, 0.5876, 0.0466],
          [0.5273, 0.0673, 0.7782, 0.5226, 0.8699],
          [0.7431, 0.8379, 0.7179, 0.9059, 0.1476],
          [0.2583, 0.5090, 0.8046, 0.1763, 0.6814],
          [0.6311, 0.2884, 0.9314, 0.7833, 0.7727]],

         [[0.5804, 0.8342, 0.6768, 0.5435, 0.4607],
          [0.7297, 0.7294, 0.0220, 0.5833, 0.4130],
          [0.5099, 0.3951, 0.7678, 0.2135, 0.0866],
          [0.8996, 0.9625, 0.3403, 0.5967, 0.0765],
          [0.2447, 0.3609, 0.6060, 0.6795, 0.3156]],

         [[0.2016, 0.7268, 0.8850, 0.7330, 0.0563],
          [0.9394, 0.0015, 0.0673, 0.2740, 0.2546],
          [0.0829, 0.2890, 0.8807, 0.0301, 0.4578],
          [0.7551, 0.1646, 0.1234, 0.4990, 0.0452],
          [0.7401, 0.1458, 0.2748, 0.1216, 0.0635]]],


        [[[0.6772, 0.1119, 0.0980, 0.6105, 0.6992],
          [0.8782, 0.8777, 0.9903, 0.8512, 0.5776],
          [0.8834, 0.0357, 0.4094, 0.5686, 0.8500],
          [0.7925, 0.5945, 0.0673, 0.4228, 0.6723],
          [0.1631, 0.3959, 0.3661, 0.7034, 0.0851]],

         [[0.7672, 0.1114, 0.8194, 0.1772, 0.8017],
          [0.0814, 0.3402, 0.6552, 0.7820, 0.1323],
          [0.8027, 0.1131, 0.3673, 0.6294, 0.2033],
          [0.1771, 0.3085, 0.8691, 0.2127, 0.8360],
          [0.9168, 0.4087, 0.4621, 0.3283, 0.7204]],

         [[0.8540, 0.8420, 0.6497, 0.7415, 0.0710],
          [0.1066, 0.8486, 0.9006, 0.3340, 0.1622],
          [0.3209, 0.2985, 0.2924, 0.6111, 0.8170],
          [0.5262, 0.9499, 0.4510, 0.2628, 0.7200],
          [0.0460, 0.8964, 0.7780, 0.7176, 0.9792]]]])
torch.Size([2, 3, 5, 5])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>my_flatten = torch.nn.Flatten()
y = my_flatten(X)
print(y)
print(y.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.2404, 0.7937, 0.4494, 0.3026, 0.4312, 0.8841, 0.5139, 0.7175, 0.4840,
         0.2230, 0.7278, 0.4127, 0.0997, 0.9097, 0.2046, 0.2216, 0.8711, 0.5727,
         0.2523, 0.8165, 0.6370, 0.6532, 0.1003, 0.0487, 0.5645, 0.0552, 0.3277,
         0.0192, 0.1828, 0.5912, 0.2608, 0.2675, 0.2025, 0.0099, 0.5579, 0.3849,
         0.3101, 0.6909, 0.5483, 0.7962, 0.2805, 0.0691, 0.3280, 0.6559, 0.4262,
         0.8914, 0.2734, 0.6694, 0.1777, 0.3344, 0.6636, 0.3904, 0.5210, 0.6946,
         0.7324, 0.8554, 0.1056, 0.6247, 0.9505, 0.5251, 0.3029, 0.5655, 0.6137,
         0.1051, 0.3008, 0.9149, 0.6882, 0.7984, 0.5965, 0.3351, 0.5486, 0.9809,
         0.7653, 0.3887, 0.5888],
        [0.0939, 0.3101, 0.8238, 0.5930, 0.2046, 0.5058, 0.1250, 0.4880, 0.8498,
         0.0114, 0.1792, 0.1898, 0.4208, 0.6207, 0.8486, 0.7073, 0.3133, 0.7857,
         0.8234, 0.3309, 0.7181, 0.0229, 0.5755, 0.5876, 0.4981, 0.2573, 0.9795,
         0.8049, 0.6150, 0.0538, 0.9121, 0.6169, 0.2563, 0.2236, 0.5475, 0.5064,
         0.6025, 0.6887, 0.9408, 0.2192, 0.6901, 0.9923, 0.7835, 0.7694, 0.5130,
         0.4790, 0.9041, 0.3275, 0.6307, 0.2808, 0.9228, 0.6131, 0.1735, 0.6140,
         0.8222, 0.5546, 0.1240, 0.9193, 0.3230, 0.9113, 0.0417, 0.5647, 0.8687,
         0.2223, 0.5476, 0.1715, 0.0361, 0.9582, 0.3656, 0.1617, 0.2577, 0.7713,
         0.0747, 0.8483, 0.3002]])
torch.Size([2, 75])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="dropout">
<h3><span class="section-number">1.10.3. </span>Dropout<a class="headerlink" href="#dropout" title="Permalink to this heading">#</a></h3>
<section id="nn-dropout-p-0-2">
<h4><span class="section-number">1.10.3.1. </span><code class="docutils literal notranslate"><span class="pre">nn.Dropout(p=0.2)</span></code><a class="headerlink" href="#nn-dropout-p-0-2" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>dropout layer 的作用是：</p>
<ul>
<li><p>input: <code class="docutils literal notranslate"><span class="pre">nn.Dropout()</span></code> 通常都是接在全連階層後的 layer，所以他預設是接受到 (batch_size, n_neuron) 這種 shape.</p></li>
<li><p>output:</p>
<ul>
<li><p>對每個 neuron，都有 p 的機率，拔掉這個 neuron &lt;-&gt; 有 p 的機率，讓這個 neuron 的輸出值變成 0</p></li>
<li><p>對於沒有被拔掉的 neuron，他的輸出值會被縮放(原始值/ (1-p))</p></li>
<li><p>也就是說，隨機讓一些 neuron 的影響力變成 0, 但加強剩餘 neuron 的權重</p></li>
</ul>
</li>
</ul>
</li>
<li><p>這樣做的話</p>
<ul>
<li><p>每次 forward，都有 p 比例的輸出值變成 0，那算 gradient 時 (微分後，evaluate在這個輸出值上）就會是 0，就無法更新此 neuron 的參數，就等於停止這個 neuron 的學習。</p></li>
<li><p>由於每次 forawrd，都隨機的讓 p 比例的 neuron 不能學，但整體的 loss 又希望他一直變小。所以可以讓 NN 學到不要依賴某幾個 neuron 來做決策，讓結果可以比較 robust 一點。有點像一個球隊，總是隨機的讓某些球員不能上場，但又希望球隊贏球，所以 optimize 球隊的實力，就不會只依賴在某幾個明星球員上，而是會均分到各個球員。</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X= torch.rand(2, 5) # batch_size = 2, 每個 batch 在上一個 layer 結束，都得到 5 個 ouptut
X
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.3376, 0.8209, 0.5483, 0.5076, 0.9084],
        [0.2590, 0.4310, 0.4476, 0.4791, 0.1276]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>no_dropout = nn.Dropout(p = 0)
all_dropout = nn.Dropout(p = 1)
half_dropout = nn.Dropout(p = 0.5)

y1 = no_dropout(X)
y2 = all_dropout(X)
y3 = half_dropout(X)

print(&quot;input: \n&quot;, X, &quot;\n&quot;)
print(&quot;no_dropout: \n&quot;, y1, &quot;\n&quot;)
print(&quot;all_dropout: \n&quot;, y2, &quot;\n&quot;)
print(&quot;half_dropout: \n&quot;, y3)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>input: 
 tensor([[0.3376, 0.8209, 0.5483, 0.5076, 0.9084],
        [0.2590, 0.4310, 0.4476, 0.4791, 0.1276]]) 

no_dropout: 
 tensor([[0.3376, 0.8209, 0.5483, 0.5076, 0.9084],
        [0.2590, 0.4310, 0.4476, 0.4791, 0.1276]]) 

all_dropout: 
 tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]]) 

half_dropout: 
 tensor([[0.6752, 1.6417, 1.0966, 0.0000, 1.8167],
        [0.0000, 0.0000, 0.8952, 0.0000, 0.0000]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>可以看到，最後有 dropout 掉的結果，剩餘的 output 值都被放大</p></li>
</ul>
<ul class="simple">
<li><p>實際應用時，大概會這樣用：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dropout1, dropout2 = 0.2, 0.5

net = nn.Sequential(
    # 不管進來的是啥 input, 先轉成 (bach_size, n_neuron) 這種 shape
    nn.Flatten(),
    # 一般的 linear 層, activation 層
    nn.Linear(784, 256),
    nn.ReLU(),
    # dropout 來了
    nn.Dropout(dropout1),
    # 同樣的概念再來一次    
    nn.Linear(256, 256),
    nn.ReLU(),
    # 在第二个全连接层之后添加一个dropout层
    nn.Dropout(dropout2),
    nn.Linear(256, 10)
)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>如果要自己寫 dropout layer，會長這樣</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class MyDropout(nn.Module):
    def __init__(self, dropout_rate):
        super().__init__()
        assert 0 &lt;= dropout_rate &lt;= 1
        self.dropout_rate = dropout_rate
    def forward(self, x):
        if self.dropout_rate == 1:
            mask = torch.ones_like(x)
            return mask * x
        elif self.dropout_rate == 0:
            mask = torch.zeros_like(x)
            return mask * x
        else:
            mask = (torch.rand(x.shape) &gt; self.dropout_rate).float()
            return mask * x / (1.0 - self.dropout_rate)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X= torch.rand(2, 5) # batch_size = 2, 每個 batch 在上一個 layer 結束，都得到 5 個 ouptut
print(X)
my_dropout = MyDropout(0.5)
print(my_dropout(X))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.4401, 0.0847, 0.2464, 0.0768, 0.6978],
        [0.6341, 0.5199, 0.0806, 0.3622, 0.9605]])
tensor([[0.8802, 0.0000, 0.0000, 0.0000, 1.3957],
        [1.2681, 0.0000, 0.0000, 0.0000, 1.9211]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="nn-dropout2d-p-0-2">
<h4><span class="section-number">1.10.3.2. </span><code class="docutils literal notranslate"><span class="pre">nn.Dropout2d(p=0.2)</span></code><a class="headerlink" href="#nn-dropout2d-p-0-2" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>input 是 (Batch_size, Channel, Height, Width)</p></li>
<li><p>dropout 過程</p>
<ul>
<li><p>概念：隨機將 某個batch的某個channel下的 feature map 全設為 0</p></li>
<li><p>實際操作：此函數在背後，是先用 Ber(p)，生成 Batch_size x Channel 個 (H,W) 的矩陣 (所以要嘛全 0 ，要嘛全 1)，然後去乘上 input tensor</p></li>
</ul>
</li>
<li><p>output 就還是一樣的 shape: (Batch_size, Channel, Height, Width)</p></li>
</ul>
<ul class="simple">
<li><p>看個例子：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>input = torch.randn(2, 3, 2, 2)
print(input)

m = nn.Dropout2d(p=0.5) # 有 0.5 的機率，將 該batch該channel 下的 feature map 設為 0 &lt;=&gt; 有 50% 的 feature map 被設為 0

out = m(input)
print(out)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[-0.9790, -0.9097],
          [ 1.4508,  1.6278]],

         [[ 0.9969,  0.4001],
          [-0.5196, -0.2183]],

         [[ 0.9921, -1.2808],
          [ 0.7432, -0.5854]]],


        [[[ 0.5112,  1.0735],
          [-0.2309, -0.3365]],

         [[ 1.1461,  2.6715],
          [-1.6048,  1.2848]],

         [[-0.7704, -1.1662],
          [-0.0082, -0.4357]]]])
tensor([[[[-1.9581, -1.8193],
          [ 2.9015,  3.2556]],

         [[ 0.0000,  0.0000],
          [-0.0000, -0.0000]],

         [[ 0.0000, -0.0000],
          [ 0.0000, -0.0000]]],


        [[[ 0.0000,  0.0000],
          [-0.0000, -0.0000]],

         [[ 2.2921,  5.3431],
          [-3.2096,  2.5696]],

         [[-1.5407, -2.3324],
          [-0.0164, -0.8714]]]])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="batch-normalization">
<h3><span class="section-number">1.10.4. </span>Batch Normalization<a class="headerlink" href="#batch-normalization" title="Permalink to this heading">#</a></h3>
<section id="nn-batchnorm2d">
<h4><span class="section-number">1.10.4.1. </span><code class="docutils literal notranslate"><span class="pre">nn.BatchNorm2d()</span></code><a class="headerlink" href="#nn-batchnorm2d" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>input shape: (B, C, H, W).</p></li>
<li><p>batch normalization 的過程：</p>
<ul>
<li><p>by channel 做。例如 C = 3 的話，就把 channel = 1 的所有 batch, H, W 拉成向量，算 mean 和 sd. 然後做標準化.</p></li>
<li><p>同樣的步驟對 channel = 2, channel =3 做。所以會得到 3 個 mean 和 sd.</p></li>
<li><p>output 就會是一樣的 shape.</p></li>
</ul>
</li>
<li><p>output shape: (B, C, H, W)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span>
    <span class="n">num_features</span><span class="p">,</span> <span class="c1"># 在影像中，一個 channel 被當成一個 feature, 所以這邊寫 channel 數</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="c1"># 標準化的時候，分母加上的小數字，避免 sd = 0 時掛掉</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="c1"># 每一個 batch 都在做 EMA, mean_new = momentum * mean_old + (1 - momentum) * this_batch_mean</span>
    <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># 下面解釋</span>
    <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># 會紀錄最新 EMA 結果的 mean 和 std, 稱為 running_mean 和 running_std; 這樣 inference 時就可以用</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>補充一下上面的 affine = True 在講啥</p></li>
<li><p>batch normalization 實際在做時，他的公式長這樣： <span class="math notranslate nohighlight">\(y = \frac{x-E(x)}{\sqrt{var(x)+\epsilon}}\times \gamma + \beta\)</span></p></li>
<li><p>也就是說，標準化完，本來變 <span class="math notranslate nohighlight">\(Normal(0,1)\)</span>，但他可以從資料中學習，需要的話，會變成 <span class="math notranslate nohighlight">\(Normal(\beta, \gamma)\)</span></p></li>
<li><p>而 <code class="docutils literal notranslate"><span class="pre">affine</span> <span class="pre">=</span> <span class="pre">True</span></code> 的意思就是，我會先給 <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 起始值，分別為 1 和 0 (所以目前還沒改變分配)，但 <span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 是可以學習的，他後續就會估這個參數</p></li>
<li><p>以下，開始實際使用吧：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = torch.arange(64, dtype=torch.float32).reshape((2, 2, 4, 4))
X
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]],

         [[16., 17., 18., 19.],
          [20., 21., 22., 23.],
          [24., 25., 26., 27.],
          [28., 29., 30., 31.]]],


        [[[32., 33., 34., 35.],
          [36., 37., 38., 39.],
          [40., 41., 42., 43.],
          [44., 45., 46., 47.]],

         [[48., 49., 50., 51.],
          [52., 53., 54., 55.],
          [56., 57., 58., 59.],
          [60., 61., 62., 63.]]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>my_norm = nn.BatchNorm2d(2)
my_norm(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[-1.4113, -1.3513, -1.2912, -1.2312],
          [-1.1711, -1.1111, -1.0510, -0.9909],
          [-0.9309, -0.8708, -0.8108, -0.7507],
          [-0.6907, -0.6306, -0.5705, -0.5105]],

         [[-1.4113, -1.3513, -1.2912, -1.2312],
          [-1.1711, -1.1111, -1.0510, -0.9909],
          [-0.9309, -0.8708, -0.8108, -0.7507],
          [-0.6907, -0.6306, -0.5705, -0.5105]]],


        [[[ 0.5105,  0.5705,  0.6306,  0.6907],
          [ 0.7507,  0.8108,  0.8708,  0.9309],
          [ 0.9909,  1.0510,  1.1111,  1.1711],
          [ 1.2312,  1.2912,  1.3513,  1.4113]],

         [[ 0.5105,  0.5705,  0.6306,  0.6907],
          [ 0.7507,  0.8108,  0.8708,  0.9309],
          [ 0.9909,  1.0510,  1.1111,  1.1711],
          [ 1.2312,  1.2912,  1.3513,  1.4113]]]],
       grad_fn=&lt;NativeBatchNormBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>my_norm.running_mean
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([2.3500, 3.9500])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(X[:,0,:,:].mean())
print(X[:,1,:,:].mean())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(23.5000)
tensor(39.5000)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>可以看到實際的 mean 和 running_mean 差了 10 倍，是因為 running_mean 已經先把目前的 mean x 0.1, 準備等等下一個 batch 進來時，用這個 running_mean + (1-0.1) x new_batch_mean 來得到 Exponential Moving Average (EMA) 的 mean</p></li>
</ul>
</section>
</section>
<section id="layer-normalization">
<h3><span class="section-number">1.10.5. </span>Layer Normalization<a class="headerlink" href="#layer-normalization" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>batch, sentence_length, embedding_dim = 2, 3, 4
data = torch.randn(batch, sentence_length, embedding_dim)
data
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-1.6527, -1.2206,  0.6258, -0.2745],
         [ 1.5530,  0.2956, -0.7750,  0.2132],
         [-0.2742,  0.4927, -0.8178,  0.5907]],

        [[ 0.5856,  0.7579, -0.3784,  0.1930],
         [-0.1841,  1.4684, -0.8964,  1.9392],
         [ 1.3827, -1.5651,  0.5822, -0.6323]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>layer_norm = nn.LayerNorm(embedding_dim)
layer_norm(data)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-1.1615, -0.6705,  1.4275,  0.4045],
         [ 1.4901, -0.0316, -1.3272, -0.1313],
         [-0.4708,  0.8564, -1.4115,  1.0260]],

        [[ 0.6781,  1.0727, -1.5298, -0.2210],
         [-0.6592,  0.7631, -1.2722,  1.1682],
         [ 1.2777, -1.3363,  0.5678, -0.5092]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>(data[0] - data[0].mean(dim = -1, keepdim = True))/torch.sqrt(torch.var(data[0], dim = -1, keepdim = True, unbiased = False))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-1.1615, -0.6705,  1.4275,  0.4045],
        [ 1.4901, -0.0316, -1.3272, -0.1313],
        [-0.4708,  0.8564, -1.4115,  1.0260]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>(data - data[0,:,:].mean())/torch.sqrt(torch.var(data[0,:,:], unbiased = False))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-1.7847, -1.2869,  0.8404, -0.1969],
         [ 1.9086,  0.4600, -0.7734,  0.3651],
         [-0.1965,  0.6871, -0.8228,  0.8000]],

        [[ 0.7941,  0.9926, -0.3165,  0.3418],
         [-0.0927,  1.8112, -0.9133,  2.3536],
         [ 1.7125, -1.6838,  0.7902, -0.6091]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>N, C, H, W = 2, 2, 3, 3
input = torch.randn(N, C, H, W)
# Normalize over the last three dimensions (i.e. the channel and spatial dimensions)
# as shown in the image below
layer_norm = nn.LayerNorm([C, H, W])
output = layer_norm(input)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>output
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[ 0.2504,  0.5595, -1.3587],
          [ 2.0666, -0.3750,  1.0990],
          [-1.5351,  0.0377, -2.0293]],

         [[ 0.4013,  1.1532,  0.3073],
          [ 0.0422,  0.4210, -0.5679],
          [-0.8564,  0.8200, -0.4358]]],


        [[[ 0.1713, -0.7803, -0.6681],
          [ 0.4865, -0.5627, -1.4878],
          [ 0.6812, -0.9151,  0.4169]],

         [[ 2.7738,  0.2237,  0.3362],
          [-0.4778, -0.7063,  0.9934],
          [ 1.2262, -1.1143, -0.5968]]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="cnn">
<h3><span class="section-number">1.10.6. </span>CNN<a class="headerlink" href="#cnn" title="Permalink to this heading">#</a></h3>
<section id="convolution">
<h4><span class="section-number">1.10.6.1. </span>convolution<a class="headerlink" href="#convolution" title="Permalink to this heading">#</a></h4>
<section id="nn-conv2d">
<h5><span class="section-number">1.10.6.1.1. </span><code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code><a class="headerlink" href="#nn-conv2d" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p>完整語法是： <code class="docutils literal notranslate"><span class="pre">nn.Conv2d(in_channels,</span> <span class="pre">out_channels,</span> <span class="pre">kernel_size,</span> <span class="pre">stride=1,</span> <span class="pre">padding=0)</span></code></p></li>
</ul>
<ul class="simple">
<li><p>最重要的就是學 output shape 的算法</p></li>
<li><p>先講實務運作時的重要結論：</p>
<ul>
<li><p>conv 的 padding 固定設為 (kernel_size - 1)/ 2</p></li>
<li><p>stride 如果要 same padding, 就設為 1, 要高寬減半, 就設為 2</p></li>
</ul>
</li>
<li><p>舉例來說：</p>
<ul>
<li><p>我的 kernel_size = 3, 我想要 same padding，那 padding 設為 (3-1)/2 = 1, stride 設為 1</p></li>
<li><p>我的 kernel_size = 3, 我想要高寬減半，那 padding 設為 (3-1)/2 = 1, stride 設為 2</p></li>
<li><p>1x1 conv 是比較特殊的應用，主要是用調整通道數，所以通常都是要 same padding，那一樣： padding = (1-1)/2 = 0, stride = 1, 合理吧～</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>接下來講 general case，以及解釋為什麼要這樣設.</p></li>
<li><p>符號定義：</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(n_h\)</span>: input 的高</p></li>
<li><p><span class="math notranslate nohighlight">\(k_h\)</span>: kernel 的高</p></li>
<li><p><span class="math notranslate nohighlight">\(p_h\)</span>: 高的方向的 padding 為多少</p></li>
<li><p><span class="math notranslate nohighlight">\(s_h\)</span>: 高的方向的步幅 (stride) 有多少</p></li>
</ul>
</li>
<li><p>那 output shape 為: <span class="math notranslate nohighlight">\(\left \lfloor{\frac{n_h+2p_h-k_h}{s_h}+1}\right \rfloor \times \left \lfloor{\frac{n_w+2p_w-k_w}{s_w}+1}\right \rfloor\)</span></p></li>
<li><p>括號是高斯符號，floor，例如 <span class="math notranslate nohighlight">\(\left \lfloor{3.5}\right \rfloor = 3\)</span>，意思就是 kernel 如果除不盡，最後那一步就不走了。</p></li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>講一下公式的推導。舉例來說， input size = 6x6, kernel size = 3, padding = 1, stride = 1</p>
<ul>
<li><p>只看 width 方向就好。先把兩個 padding 放到 width 旁邊，現在整張圖的 width 變成 <span class="math notranslate nohighlight">\(n + 2p = 8\)</span></p></li>
<li><p>一個 kernel 疊上來，吃掉 k 個寬，所以現在剩下 <span class="math notranslate nohighlight">\(n+2p-k = 6+2-3=5\)</span> 個 widht 可以用</p></li>
<li><p>stride 每次 1 格，表示剩下的寬，每移動1次kernel，被吃掉1格，所以可以移動 <span class="math notranslate nohighlight">\(\frac{n+2p-k}{s} = 5/1 = 5\)</span> 格</p></li>
<li><p>加上最一開始疊上來，就已經算 1 格了，所以最終 output 的 width 為 <span class="math notranslate nohighlight">\(\frac{n+2p-k}{s} + 1 = 5/1 + 1 = 6\)</span>, 所以是 same padding</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>解釋一下實務上這樣設的原因：</p>
<ul>
<li><p>我想要做 same padding:</p>
<ul>
<li><p>padding設為 <span class="math notranslate nohighlight">\(\frac{k-1}{2}\)</span>, stride設為1，那結果就會是 same padding (帶進公式推一下就知道了)</p></li>
<li><p>例如 kernel_size = 3, 那 padding 就設 (3-1)/2 = 1, stride 設為 1，就會是 same padding</p></li>
<li><p>又例如 kernel_size = 7, 那 padding 就設 (7-1)/2 = 3, stride 設為 1，就會是 same padding.</p></li>
</ul>
</li>
<li><p>我想要高寬減半:</p>
<ul>
<li><p>padding設為 <span class="math notranslate nohighlight">\(\frac{k-1}{2}\)</span>, stride設為2，那結果就會是高寬減半。但是是用到 floor.</p></li>
<li><p>帶進公式看一下</p>
<ul>
<li><p>原本的 <span class="math notranslate nohighlight">\(\frac{n_h+2p_h-k_h}{s_h}+1\)</span>，可以把 1 放回分子 <span class="math notranslate nohighlight">\(\frac{n_h+2p_h-k_h+s_h}{s_h}\)</span></p></li>
<li><p>把 <span class="math notranslate nohighlight">\(p_h = \frac{k-1}{2}\)</span> 帶入，變成 <span class="math notranslate nohighlight">\(\frac{n_h+s_h-1}{s_h}\)</span>，再拆成 <span class="math notranslate nohighlight">\(\frac{n_h}{s_h} + \frac{s_h-1}{s_h}\)</span></p></li>
<li><p>前面項，如果 <span class="math notranslate nohighlight">\(s_h = 2\)</span>，那就做到高寬減半了; 後面那項一定 &lt; 1，所以高斯取 floor 後，就被捨棄掉</p></li>
</ul>
</li>
<li><p>來個例子： kernel_size = 3, 那 padding 就設 (3-1)/2 = 1, stride 設為 2，就會是高寬減半</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>來點實際例子吧：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>input_img = torch.rand(10, 3, 244, 244)
print(&quot;original shape:&quot;, input_img.shape)

# same padding, kernel_size = 3
k = 3
p = int((k-1)/2)
s = 1

my_conv = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = k, padding = p, stride = s)
out = my_conv(input_img)
print(&quot;same padding shape:&quot;, out.shape)

# 高寬減半
k = 3
p = int((k-1)/2)
s = 2

my_conv2 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = k, padding = p, stride = s)
out = my_conv2(input_img)
print(&quot;高寬減半:&quot;, out.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>original shape: torch.Size([10, 3, 244, 244])
same padding shape: torch.Size([10, 3, 244, 244])
高寬減半: torch.Size([10, 3, 122, 122])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>來個比較有趣的， 1x1 的 convolution (主要用來改變通道)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 通道數加倍，same padding
input_img = torch.rand(10, 64, 512, 512)
print(&quot;original shape:&quot;, input_img.shape)

# same padding, kernel_size = 1
k = 1
p = int((k-1)/2)
s = 1

my_conv = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = k, padding = p, stride = s)
out = my_conv(input_img)
print(&quot;same padding shape:&quot;, out.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>original shape: torch.Size([10, 64, 512, 512])
same padding shape: torch.Size([10, 128, 512, 512])
</pre></div>
</div>
</div>
</div>
</section>
<section id="d-convolution">
<h5><span class="section-number">1.10.6.1.2. </span>1d convolution<a class="headerlink" href="#d-convolution" title="Permalink to this heading">#</a></h5>
</section>
</section>
<section id="pooling">
<h4><span class="section-number">1.10.6.2. </span>pooling<a class="headerlink" href="#pooling" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>pooling 的重點，也是對 output shape 的掌握。</p></li>
<li><p>原理就和 conv 那邊介紹的一樣，output shape = <span class="math notranslate nohighlight">\(\left \lfloor{\frac{n_h+2p_h-k_h}{s_h}+1}\right \rfloor \times \left \lfloor{\frac{n_w+2p_w-k_w}{s_w}+1}\right \rfloor\)</span></p></li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>但從實務角度來看，pooling 的使用，大概分為兩類：</p>
<ul>
<li><p>高寬降 k 倍的 pooling (e.g. k=2, 就是最常用的高寬減半).</p>
<ul>
<li><p>最常見的，就是 conv 時先做 same padding, 然後在 pooling 時再讓他高寬減半來降維</p></li>
<li><p>那作法就是 kernel_size 設為 2, stride 設為 2, padding 設為 0。</p></li>
<li><p>如果是要降為 k 倍，那就 kernel_size 設為 k, stride 也設為 k, padding 設為 0.</p></li>
<li><p>也因為這種特性，常見的 layer (e.g. <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.AvgPool2d()</span></code>)，你都只要設 kernel_size 就好，stride 他會直接幫你愈設為 kernel_size。</p></li>
</ul>
</li>
<li><p>指定 shape 的 pooling</p>
<ul>
<li><p>例如 Resnet 最後會用到的 global average pooling，就是 by 通道數，將整個 feature map 統整為 1 個值.</p></li>
<li><p>那就等於我指定 output shape 要是 (1,1) 的 pooling.</p></li>
<li><p>這可以用 <code class="docutils literal notranslate"><span class="pre">nn.AdaptiveAvgPool2d(output_size=(1,</span> <span class="pre">1))</span></code> 來搞定</p></li>
<li><p>那在 object detection 的 model 中，有些步驟會需要不管 input feature map 的 shape 是多少，統一幫我做成 (k, k) output 的 pooling 結果 (例如 input feature map 是 (224,224), 我希望做完 average pooling 後，可以得到 (8,8) 的結果。那我其實就想做 kernel_size = 28, stride = 28 的 pooling，但你用 nn.AdaptiveAvgPool2d((8,8))，他就會自動幫你算出 kernel_size = 28, stride = 28, 你就只要收割就好)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>來上例子吧</p></li>
</ul>
<section id="nn-maxpool2d">
<h5><span class="section-number">1.10.6.2.1. </span><code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code><a class="headerlink" href="#nn-maxpool2d" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))
X
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 高寬減半的 pooling
my_pool = nn.MaxPool2d(2) # kernel_size = 2, stride 預設就是 kernel_size, 所以 stride = 2
my_pool(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[ 5.,  7.],
          [13., 15.]]]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="nn-avgpool2d">
<h5><span class="section-number">1.10.6.2.2. </span><code class="docutils literal notranslate"><span class="pre">nn.AvgPool2d()</span></code>.<a class="headerlink" href="#nn-avgpool2d" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))
X
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 高寬減半的 pooling
my_pool = nn.AvgPool2d(2) # kernel_size = 2, stride 預設就是 kernel_size, 所以 stride = 2
my_pool(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[ 2.5000,  4.5000],
          [10.5000, 12.5000]]]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="nn-adaptiveavgpool2d">
<h5><span class="section-number">1.10.6.2.3. </span><code class="docutils literal notranslate"><span class="pre">nn.AdaptiveAvgPool2d()</span></code><a class="headerlink" href="#nn-adaptiveavgpool2d" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))
X
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># global average pooling -&gt; 希望的 output shape 是 (1,1)
my_pool = nn.AdaptiveAvgPool2d((1,1))
my_pool(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[7.5000]]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 指定 output shape 為 (2,2) 的 average pooling
my_pool = nn.AdaptiveAvgPool2d((2,2))
my_pool(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[ 2.5000,  4.5000],
          [10.5000, 12.5000]]]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="nn-adaptivemaxpool2d">
<h5><span class="section-number">1.10.6.2.4. </span><code class="docutils literal notranslate"><span class="pre">nn.AdaptiveMaxPool2d()</span></code><a class="headerlink" href="#nn-adaptivemaxpool2d" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))
X
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 指定 output shape 為 (2,2) 的 max pooling
my_pool = nn.AdaptiveMaxPool2d((2,2))
my_pool(X)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[[ 5.,  7.],
          [13., 15.]]]])
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="rnn">
<h3><span class="section-number">1.10.7. </span>RNN<a class="headerlink" href="#rnn" title="Permalink to this heading">#</a></h3>
</section>
<section id="attention">
<h3><span class="section-number">1.10.8. </span>Attention<a class="headerlink" href="#attention" title="Permalink to this heading">#</a></h3>
</section>
<section id="transformer">
<h3><span class="section-number">1.10.9. </span>Transformer<a class="headerlink" href="#transformer" title="Permalink to this heading">#</a></h3>
</section>
</section>
<section id="loss">
<h2><span class="section-number">1.11. </span>Loss<a class="headerlink" href="#loss" title="Permalink to this heading">#</a></h2>
<section id="overview">
<h3><span class="section-number">1.11.1. </span>overview<a class="headerlink" href="#overview" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>官網連結: <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#loss-functions">https://pytorch.org/docs/stable/nn.html#loss-functions</a></p></li>
<li><p>常用的整理：</p></li>
</ul>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>loss function</p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">torch.nn</span> <span class="pre">as</span> <span class="pre">nn</span></code></p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span> <span class="pre">as</span> <span class="pre">F</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Binary cross entropy</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.BCELoss()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.binary_cross_entropy</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Binary cross entropy with logits</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.BCEWithLogitsLoss()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.binary_cross_entropy_with_logits</span></code></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>categorical cross entropy</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.relu</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>mse</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.MSELoss()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.leaky_relu</span></code></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>mae</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.L1Loss()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">F.tanh</span></code></p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>概念講一下：</p>
<ul>
<li><p>loss 在統計的定義，是對 “單一” sample 算 loss，例如 square error loss = <span class="math notranslate nohighlight">\((y_i - \hat{y_i})^2\)</span></p></li>
<li><p>然後 mse 是 cost，不是 loss，所以 mse cost = <span class="math notranslate nohighlight">\(\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y_i})^2\)</span></p></li>
<li><p>但在 pytorch/tensorflow 中，這兩個已經被混用了，都叫做 loss.</p></li>
<li><p>至於，如何區別這兩者呢？靠 class/function 中的參數定義來決定。</p></li>
<li><p>例如： <code class="docutils literal notranslate"><span class="pre">my_mse</span> <span class="pre">=</span> <span class="pre">nn.MSELoss()</span></code>, 然後 <code class="docutils literal notranslate"><span class="pre">my_mse(y_hat_vector,</span> <span class="pre">y_true_vector)</span></code>，算出來的就是 mse cost.</p></li>
<li><p>但如果 <code class="docutils literal notranslate"><span class="pre">my_mse</span> <span class="pre">=</span> <span class="pre">nn.MSELoss(reduction</span> <span class="pre">=</span> <span class="pre">'none')</span></code>, 然後 <code class="docutils literal notranslate"><span class="pre">my_mse(y_hat_vector,</span> <span class="pre">y_true_vector)</span></code>，算出來的是 n 維的 mse loss</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>y 不一定要是向量，可以是矩陣或陣列：</p>
<ul>
<li><p>在統計上，學 loss 或 cost，都是從回歸的角度去學的，也就 向量 vs 向量， e.g. y = 100 維向量(100個sample)，y_hat 也是 100 維，那就可以算出 1 個 cost 和 100 個 loss.</p></li>
<li><p>但在 deep learning 裡面，y不一定是向量，y可以是矩陣，甚至多維陣列。</p></li>
<li><p>例如做 autoencoder 時</p>
<ul>
<li><p>y就是矩陣，比如 100 張圖片，每張圖片都是 8x8 的矩陣，那 y 可以定義成 (100, 8x8) 的 矩陣，(把圖片拉成 8x8 的向量)。</p></li>
<li><p>y_hat 是這些影像 reconstruct 後的結果，所以也是 100 x 64 的矩陣。</p></li>
<li><p>那我照樣用剛剛定義好的 loss function，我就可以算出 1 個 cost 和 100x8x8 = 6400 個 loss。</p></li>
<li><p>所以關鍵在：他都是 <code class="docutils literal notranslate"><span class="pre">by</span> <span class="pre">element</span></code> 算 loss, 然後紀錄有多少 <code class="docutils literal notranslate"><span class="pre">個數</span></code>, 最後再用 <code class="docutils literal notranslate"><span class="pre">sum</span></code> 或 <code class="docutils literal notranslate"><span class="pre">mean</span></code> 回給你一個 cost。</p></li>
<li><p>這樣，就不需要管 y 的 shape 了。</p></li>
<li><p>例如：我這次不要把 8x8 拉成向量，所以 y 就是 (100, 8, 8) 的 array，那也無所謂，放入我的 loss function，他就可以算出 6400 個 loss，然後依照你的 reduction 的設定 (none or sum or mean)，回給你 6400 個 loss 或是 1 個 cost</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>batch loss 是拿來更新參數用的， epoch loss 是用來檢查有無 overfitting 的</p>
<ul>
<li><p>deep learning 在 training or testing 時，都是一個 batch 一個 batch 做，最後再整合成一個 epoch</p></li>
<li><p>每個 batch 在做的時候，都要算這個 batch 的 cost，他的目的是用來更 gradient 時，要對這個 cost 做偏微分。所以每個 batch 結束，會得到一個 cost</p></li>
<li><p>每個 epoch 結束，要算的 cost 是跨所有樣本的。他的目的，是要去比較 training sample 的 cost 和 validation sample 的 cost，來判斷是否 overfitting 了，要不要做 early stop</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>epoch loss 的算法設計.</p>
<ul>
<li><p>最常見的設計，是直接拿 batch cost 的結果來 summarise，因位省時省力：</p>
<ul>
<li><p>舉例來說，我有 10 個 batch，每個 batch 有 32 個 batch size.</p></li>
<li><p>每個 batch 結束時，其實都拿到該 batch 的 1 個 cost 或 32 個 loss.</p></li>
<li><p>那算 epoch cost 時，我就可以把 10 個 batch cost 取平均，或是 10x32 = 320 個 loss 取平均，就得到 epoch cost。</p></li>
<li><p>pseudo code 就是</p>
<ul>
<li><p>先定義 <code class="docutils literal notranslate"><span class="pre">epoch_cost</span> <span class="pre">=</span> <span class="pre">0</span></code></p></li>
<li><p>然後 for 迴圈去 loop 10 個 batch</p></li>
<li><p>每次 batch 結束，就讓 <code class="docutils literal notranslate"><span class="pre">epoch_cost</span> <span class="pre">+=</span> <span class="pre">batch_cost</span></code>.</p></li>
<li><p>迴圈結束後，用 epoch_cost / 10，得到 mean cost。</p></li>
<li><p>如果要用 loss 的寫法也很簡單。最外面就是定義 <code class="docutils literal notranslate"><span class="pre">loss_list</span> <span class="pre">=</span> <span class="pre">[]</span></code>，然後每個回圈都是算出 batch_size 個 epoch_loss，然後 <code class="docutils literal notranslate"><span class="pre">loss_list.append(epoch_loss)</span></code>，最終再對 loss_list 取平均就好。</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="mse">
<h3><span class="section-number">1.11.2. </span>mse<a class="headerlink" href="#mse" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_hat = torch.randn(5, requires_grad=True)
y_true = torch.randn(5)
print(y_hat)
print(y_true)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-1.6104,  0.4109,  1.3322,  1.2199, -0.6273], requires_grad=True)
tensor([ 0.7016, -1.6683,  1.0668,  0.9080, -1.2761])
</pre></div>
</div>
</div>
</div>
<section id="class">
<h4><span class="section-number">1.11.2.1. </span>class 版本<a class="headerlink" href="#class" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 算 loss
my_loss = nn.MSELoss(reduction = &quot;none&quot;)
loss = my_loss(y_hat, y_true)
print(loss)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([5.3454, 4.3230, 0.0705, 0.0973, 0.4209], grad_fn=&lt;MseLossBackward&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 算 cost (i.e mean loss)
my_loss = nn.MSELoss()
cost = my_loss(y_hat, y_true)
print(cost)
print(loss.mean())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(2.0514, grad_fn=&lt;MseLossBackward&gt;)
tensor(2.0514, grad_fn=&lt;MeanBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 算 cost (i.e sum loss)
my_loss = nn.MSELoss(reduction = &quot;sum&quot;)
sum_cost = my_loss(y_hat, y_true)
print(sum_cost)
print(loss.sum())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(10.2571, grad_fn=&lt;MseLossBackward&gt;)
tensor(10.2571, grad_fn=&lt;SumBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="function">
<h4><span class="section-number">1.11.2.2. </span>function 版本<a class="headerlink" href="#function" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 用 function 做 cost
y_hat = torch.randn(5, requires_grad=True)
y_true = torch.randn(5)

print(y_hat)
print(y_true)
print(F.mse_loss(y_hat, y_true))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.7780, -0.4672, -1.0941, -1.0928, -1.0654], requires_grad=True)
tensor([ 0.5094, -0.6637, -0.5560, -0.5600, -1.6072])
tensor(0.5126, grad_fn=&lt;MseLossBackward&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 用 function 做 loss
y_hat = torch.randn(5, requires_grad=True)
y_true = torch.randn(5)

print(y_hat)
print(y_true)
print(F.mse_loss(y_hat, y_true, reduction=&quot;none&quot;))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.9525,  0.9356, -0.1469, -0.3822,  2.0675], requires_grad=True)
tensor([-0.3021, -0.6323, -1.2846, -0.1762, -0.0629])
tensor([0.4230, 2.4583, 1.2943, 0.0424, 4.5388], grad_fn=&lt;MseLossBackward&gt;)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="mae">
<h3><span class="section-number">1.11.3. </span>mae<a class="headerlink" href="#mae" title="Permalink to this heading">#</a></h3>
</section>
<section id="binary-cross-entropy">
<h3><span class="section-number">1.11.4. </span>binary cross entropy<a class="headerlink" href="#binary-cross-entropy" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_logit = torch.tensor([2.3552, -0.9071,  2.8323])
y_hat = torch.tensor([0.9133, 0.2876, 0.9444]) # 就是 F.sigmoid(y_logit) 後的結果
y = torch.tensor([0.0, 0.0, 0.0])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>y 是 vector/matrix/array 都可 (常見是 vector，element 個數就是樣本數)，值不是 0 就是 1。 e.g. [0, 1, 0] 表示三個樣本的真值。</p></li>
<li><p>y_hat 的 shape 同 y，值介於 0~1 之間。e.g. [0.3, 0.8, 0.1]，表示三個樣本的預測值。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">binary</span> <span class="pre">cross</span> <span class="pre">entropy</span></code>： <span class="math notranslate nohighlight">\(-\frac{1}{n}\sum_{i = 1}^{n}\left[ y_i log(\hat{y_i}) + (1-y_i)(1-log(\hat{y_i})\right]\)</span></p></li>
<li><p>這在這個定義式的中間這項就是 loss： <span class="math notranslate nohighlight">\(y_i log(\hat{y_i}) + (1-y_i)(1-log(\hat{y_i})\)</span> 。可用 <code class="docutils literal notranslate"><span class="pre">reduction</span> <span class="pre">=</span> <span class="pre">&quot;none&quot;</span></code> 來設定，就可拿到 n 個 loss</p></li>
<li><p>那算 cost，可以像定義式那樣，用平均來做，可用 <code class="docutils literal notranslate"><span class="pre">reduction</span> <span class="pre">=</span> <span class="pre">&quot;mean&quot;</span></code> 來設定。不寫也可，預設就是取 mean</p></li>
</ul>
<section id="id19">
<h4><span class="section-number">1.11.4.1. </span>class 版本<a class="headerlink" href="#id19" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># loss 版
my_loss = nn.BCELoss(reduction = &quot;none&quot;)
loss = my_loss(y_hat, y)
print(loss)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([2.4453, 0.3391, 2.8896])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># cost 版
my_loss = nn.BCELoss()
cost = my_loss(y_hat, y)
print(cost)
print(loss.mean())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.8913)
tensor(1.8913)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id20">
<h4><span class="section-number">1.11.4.2. </span>function 版<a class="headerlink" href="#id20" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># loss 版
print(F.binary_cross_entropy(y_hat, y, reduction = &quot;none&quot;))

# cost 版
print(F.binary_cross_entropy(y_hat, y))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([2.4453, 0.3391, 2.8896])
tensor(1.8913)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 自己照定義算
-1*(torch.log(1-y_hat)).mean()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.8913)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>事實上，y和y_hat可以是任何shape，他都會幫你 by element 的去做 <span class="math notranslate nohighlight">\( y_i log(\hat{y_i}) + (1-y_i)(1-log(\hat{y_i})\)</span>，然後最後取總平均</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_hat = torch.rand((3,5))
y = np.random.randint(low = 0, high = 2, size = (3,5))
y = torch.tensor(y, dtype = torch.float32)

print(y_hat)
print(y)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.2420, 0.5219, 0.5408, 0.7095, 0.1231],
        [0.3518, 0.2747, 0.9089, 0.8097, 0.4674],
        [0.2304, 0.0615, 0.1389, 0.2419, 0.7572]])
tensor([[1., 1., 0., 0., 1.],
        [0., 0., 1., 1., 1.],
        [0., 0., 1., 1., 1.]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>F.binary_cross_entropy(y_hat, y)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.7998)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>而且，y 也 不一定要是 0 or 1， y也可以是 0~1 的數，此時 binary entropy 就是在衡量 y 和 y_hat 的 distribution 像不像的一個指標</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_hat = torch.rand((3,5))
y = torch.rand((3,5))

print(y_hat)
print(y)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.1801, 0.4587, 0.9839, 0.5115, 0.7780],
        [0.2146, 0.9854, 0.0592, 0.6360, 0.2658],
        [0.6827, 0.2666, 0.0440, 0.6086, 0.8917]])
tensor([[0.6732, 0.5078, 0.8481, 0.7030, 0.9484],
        [0.7353, 0.8262, 0.2038, 0.2685, 0.1202],
        [0.5659, 0.6011, 0.3651, 0.3918, 0.4598]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>F.binary_cross_entropy(y_hat, y)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.8157)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>這其實就有用在 autoencoder 的 loss 上</p></li>
<li><p>y 是一張正規化後的影像(值都介於 0 到 1)，y_hat 是 reconstruct 後的影像，值也都介於 0 到 1 (因為最後有加 sigmoid)，此時算 loss 就可以用 binary_cross_entropy loss</p></li>
</ul>
</section>
</section>
<section id="binary-cross-entropy-with-logits">
<h3><span class="section-number">1.11.5. </span>binary cross entropy with logits<a class="headerlink" href="#binary-cross-entropy-with-logits" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_logit = torch.tensor([2.3552, -0.9071,  2.8323])
y_hat = torch.tensor([0.9133, 0.2876, 0.9444]) # 就是 F.sigmoid(y_logit) 後的結果
y = torch.tensor([0.0, 0.0, 0.0])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y_i}\)</span> 在轉成機率值前叫 logit (從 deep learning 的角度，就是還沒做 sigmoid 前的值; 從統計角度，sigmoid 在做的就是 logit transform 的逆變換)。</p></li>
<li><p>所以他會幫你把這個 <span class="math notranslate nohighlight">\(\hat{y_i}\)</span> 轉成 <span class="math notranslate nohighlight">\(\frac{1}{1+e^{-\hat{y_i}}}\)</span> (這就是 sigmoid function 在做的事，也就是 logit transform 的逆變換)，再丟進去 binary cross entropy 裡面。</p></li>
<li><p>補充以下以前學過的統計知識：</p>
<ul>
<li><p>logit transform 是把 0 到 1 的機率值，轉成實數域。 e.g. p 介於 0 到 1， <span class="math notranslate nohighlight">\(y = log\left(\frac{p}{1-p}\right)\)</span>，此時 y 介於 -無窮 到 +無窮，此時的 y 被稱為 logits</p></li>
<li><p>logit transform 逆變換，是把時數域壓到 0 到 1 之間。 e.g. y 介於 -無窮 到 +無窮. <span class="math notranslate nohighlight">\(p = \frac{1}{1+e^{-y}}\)</span>，此時 p 介於 0 到 1, 此時的 p 被解釋為機率值</p></li>
</ul>
</li>
</ul>
</section>
<section id="cross-entropy">
<h3><span class="section-number">1.11.6. </span>cross-entropy<a class="headerlink" href="#cross-entropy" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>先寫結論和用法，晚點補詳細的：</p>
<ul>
<li><p>input 的 y_hat 必須是 logit (還沒經過 softmax), y可以是 [0,c) 的 integer，或是 one-hot encoding(必須是 float32，為了通用於 blended-label).</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>y_hat 需要是 logit</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_hat_logit_mat = np.array(
    [[-2.3, 2, 1.5],
     [-1, 2, 3]]
)
y_hat_logit_mat = torch.tensor(y_hat_logit_mat)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_hat_logit_mat
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-2.3000,  2.0000,  1.5000],
        [-1.0000,  2.0000,  3.0000]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>熟悉的 softmax 是這樣：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_hat_mat = torch.nn.functional.softmax(y_hat_logit_mat, dim = 1)
y_hat_mat
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0084, 0.6172, 0.3744],
        [0.0132, 0.2654, 0.7214]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>y 可以是 int 或 one-hot</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_int = torch.tensor([1,1])
y_one_hot = torch.tensor([[0,1,0],[0,1,0]], dtype = torch.float32)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>算 cross-entropy</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>loss = nn.CrossEntropyLoss(reduction=&#39;none&#39;)
print(loss(y_hat_logit_mat, y_int))
print(loss(y_hat_logit_mat, y_one_hot))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.4825, 1.3266], dtype=torch.float64)
tensor([0.4825, 1.3266], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>loss = nn.CrossEntropyLoss(reduction=&#39;mean&#39;)
print(loss(y_hat_logit_mat, y_int))
print(loss(y_hat_logit_mat, y_one_hot))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.9045, dtype=torch.float64)
tensor(0.9045, dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id21">
<h3><span class="section-number">1.11.7. </span>自訂 loss<a class="headerlink" href="#id21" title="Permalink to this heading">#</a></h3>
</section>
<section id="id22">
<h3><span class="section-number">1.11.8. </span>對比學習<a class="headerlink" href="#id22" title="Permalink to this heading">#</a></h3>
</section>
<section id="autoencoder">
<h3><span class="section-number">1.11.9. </span>autoencoder<a class="headerlink" href="#autoencoder" title="Permalink to this heading">#</a></h3>
</section>
</section>
<section id="optimizer">
<h2><span class="section-number">1.12. </span>Optimizer<a class="headerlink" href="#optimizer" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>官網很清楚： <a class="reference external" href="https://pytorch.org/docs/1.8.1/optim.html#how-to-adjust-learning-rate">https://pytorch.org/docs/1.8.1/optim.html#how-to-adjust-learning-rate</a></p></li>
</ul>
<section id="id23">
<h3><span class="section-number">1.12.1. </span>建立 optimizer<a class="headerlink" href="#id23" title="Permalink to this heading">#</a></h3>
</section>
<section id="learning-rate">
<h3><span class="section-number">1.12.2. </span>不同 learning rate<a class="headerlink" href="#learning-rate" title="Permalink to this heading">#</a></h3>
</section>
<section id="learning-rate-scheduler">
<h3><span class="section-number">1.12.3. </span>learning rate scheduler<a class="headerlink" href="#learning-rate-scheduler" title="Permalink to this heading">#</a></h3>
</section>
</section>
<section id="training-loops">
<h2><span class="section-number">1.13. </span>Training loops<a class="headerlink" href="#training-loops" title="Permalink to this heading">#</a></h2>
<section id="id24">
<h3><span class="section-number">1.13.1. </span>完整版 (了解概念用)<a class="headerlink" href="#id24" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Data 準備</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Training
X = np.random.rand(1000, 100, 100, 1)   # 虛構 1000 張 100 x 100 單色圖片
Y = np.random.randint(0, 7, [1000, 10]) # 虛構 1000 個 labels

X, Y = X.astype(np.float32), Y.astype(np.float32)
tsrX, tsrY = torch.tensor(X), torch.tensor(Y)
tsrdataset = TensorDataset(tsrX, tsrY)

tsrdataloader = DataLoader(
    tsrdataset, batch_size=4,
    shuffle=True, num_workers=4)

# Validation
vX = np.random.rand(100, 100, 100, 1)   # 虛構 100 張 100 x 100 單色圖片
vY = np.random.randint(0, 7, [100, 10]) # 虛構 100 個 labels

vX, vY = vX.astype(np.float32), vY.astype(np.float32)
vtsrX, vtsrY = torch.tensor(vX), torch.tensor(vY)
vtsrdataset = TensorDataset(tsrX, tsrY)

vtsrdataloader = DataLoader(
    vtsrdataset, batch_size=4,
    shuffle=False, num_workers=4) # Validation 不需要 shuffle
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>model structure</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(10000, 500),
            nn.ReLU(),
            nn.Linear(500, 10)
        )
    def forward(self, x):
        # 傳入 model 的函數會經過 forward 做 inference
        # x = x.view(x.size(0), -1) # flatten 的意思，原本的 x.size = (batch_size, 100, 100, 1) -&gt; 改成 (batch_size, 100*100*1)
        return self.fc(x)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>確定 device</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
device
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cpu&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># model structure
simpleNN = SimpleNN()
simpleNN.to(device)                           # 把 model 移到 GPU 計算

# optimizer
optim = torch.optim.Adam(
    simpleNN.parameters(), lr=1e-4)

# loss
criterion = nn.MSELoss()
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>tensorboard 設定</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import os

# tensorboard setting
%load_ext tensorboard

logs_base_dir = &quot;runs&quot; # training 的紀錄，放在這個路徑下
os.makedirs(logs_base_dir, exist_ok=True)

from torch.utils.tensorboard import SummaryWriter
tb = SummaryWriter()
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>開始 Training， 本質就是跑一個迴圈，在每一次（叫一個 <strong>epoch</strong>）要做的事有——</p>
<ol class="arabic simple">
<li><p>載入資料</p></li>
<li><p>經過 model 跑一次</p></li>
<li><p>比對資料的正確性，算誤差（loss）</p></li>
<li><p>把梯度清掉，然後根據這次誤差算新的梯度</p></li>
<li><p>根據 optimizer 更新參數</p></li>
<li><p>為了方便觀察，將本次 epoch 訓練的變化顯示出來，包括</p>
<ul>
<li><p>進度條（觀察訓練快慢）</p></li>
<li><p>batch loss （這個有時候會輸出太多東西）</p></li>
<li><p>epoch loss （記得累計並除掉資料數量）</p></li>
<li><p>記錄到其他變數中（方便作圖）</p></li>
<li><p>記錄到 Tensorboard 中（SummaryWriter）</p></li>
</ul>
</li>
</ol>
</li>
<li><p>為了避免 overfit，我們每個 epoch 還會進行一次 validation，事情少一些，變成——</p>
<ol class="arabic simple">
<li><p>載入資料</p></li>
<li><p>經過 model 跑一次</p></li>
<li><p>比對資料的正確性，算誤差（loss）</p></li>
<li><p>為了方便觀察，將本次 epoch validate 的結果顯示出來，包括</p>
<ul>
<li><p>進度條（觀察訓練快慢）</p></li>
<li><p>batch loss （這個有時候會輸出太多東西）</p></li>
<li><p>epoch loss （記得累計並除掉資料數量）</p></li>
<li><p>記錄到其他變數中（方便作圖）</p></li>
<li><p>記錄到 Tensorboard 中（SummaryWriter）</p></li>
</ul>
</li>
</ol>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%tensorboard --logdir {logs_base_dir} # 開啟 tensorboard
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># training loop
EPOCHS = 10
for epoch in range(EPOCHS):
    # training step (training data)
    simpleNN.train() # 切換 simpleNN 為 training 模式，dropout 之類的操作會開啟
    epoch_loss = 0.0
    for x, y in tsrdataloader:
        y_hat = simpleNN(x.to(device))        # 把 x tensor 移到 GPU 計算
        loss = criterion(y.to(device), y_hat) # 把 y tensor 移到 GPU 計算，
                                              ##  y_hat 因為是從 GPU model input GPU Tensor 出來的
                                              ##  所以不用再次 .to(device) 當然要也是沒差啦 =_=|||
        optim.zero_grad() # 把 trainable variable/weights/parameters 的 gradient 給 歸 0
        loss.backward() # 利用 loss，計算出每個 trainable variable/weights/parameters 所對應的 gradient
        optim.step() # 更新 trainable variable/weights/parameters 的值： parameters_new = parameters_old - learning_rate * gradient
        epoch_loss += loss.item()
    average_epoch_loss = epoch_loss / len(tsrdataset)
    print(f&quot;Training   Epoch {epoch + 1:2d}: Loss = {average_epoch_loss:.4f}&quot;)
    tb.add_scalar(&quot;Loss/train&quot;, average_epoch_loss, epoch + 1) # 寫進 tensorboard
    

    # evaluation step (validation data)
    simpleNN.eval() # 將 simpleNN 切換到 evaluation mode， dropout 之類的操作會關閉
    vepoch_loss = 0.0
    for x, y in vtsrdataloader:
        y_hat = simpleNN(x.to(device))
        loss = criterion(y.to(device), y_hat)
        vepoch_loss += loss.item()
    vaverage_epoch_loss = vepoch_loss / len(vtsrdataset)
    print(f&quot;Validation Epoch {epoch + 1:2d}: Loss = {vaverage_epoch_loss:.4f}&quot;)
    tb.add_scalar(&quot;Loss/val&quot;, vaverage_epoch_loss, epoch + 1) # 寫進 tensorboard
tb.close() # 加這個
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training   Epoch  1: Loss = 1.0920
Validation Epoch  1: Loss = 0.9851
Training   Epoch  2: Loss = 0.9812
Validation Epoch  2: Loss = 0.9269
Training   Epoch  3: Loss = 0.9155
Validation Epoch  3: Loss = 0.8980
Training   Epoch  4: Loss = 0.8593
Validation Epoch  4: Loss = 0.7541
Training   Epoch  5: Loss = 0.7777
Validation Epoch  5: Loss = 0.6951
Training   Epoch  6: Loss = 0.7110
Validation Epoch  6: Loss = 0.6363
Training   Epoch  7: Loss = 0.6274
Validation Epoch  7: Loss = 0.5730
Training   Epoch  8: Loss = 0.5564
Validation Epoch  8: Loss = 0.4740
Training   Epoch  9: Loss = 0.4752
Validation Epoch  9: Loss = 0.3994
Training   Epoch 10: Loss = 0.3927
Validation Epoch 10: Loss = 0.3577
</pre></div>
</div>
</div>
</div>
</section>
<section id="deploy">
<h3><span class="section-number">1.13.2. </span>模組版 (實際做實驗, deploy 時用)<a class="headerlink" href="#deploy" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def accuracy(y_hat, y):  #@save
    &quot;&quot;&quot;計算預測正確的數量&quot;&quot;&quot;
    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 計算 metric 時用的
class Accumulator:  #@save
    &quot;&quot;&quot;在n个變量上累加&quot;&quot;&quot;
    def __init__(self, n):
        self.data = [0.0] * n # [0.0, 0.0, ..., 0.0], 共 n 個 0.0

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def evaluate_accuracy(model, data_iter):  #@save
    &quot;&quot;&quot;計算在指定數據集上，模型的準確度&quot;&quot;&quot;
    if isinstance(model, torch.nn.Module):
        model.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 單一 epoch 裡要做的事
def train_epoch(model, train_iter, loss, optimizer):  #@save
    
    model.train()
    # 訓練損失總和、訓練準確度總和、樣本數
    metric = Accumulator(3)
    for X, y in train_iter:
        y_hat = model(X)
        l = loss(y_hat, y)
        optimizer.zero_grad()
        l.mean().backward()
        optimizer.step()
        
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回訓練損失 &amp; 訓練準確度
    return metric[0] / metric[2], metric[1] / metric[2]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class Animator:  #@save
    &quot;&quot;&quot;在動畫中繪製數據&quot;&quot;&quot;
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale=&#39;linear&#39;, yscale=&#39;linear&#39;,
                 fmts=(&#39;-&#39;, &#39;m--&#39;, &#39;g-.&#39;, &#39;r:&#39;), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, &quot;__len__&quot;):
            y = [y]
        n = len(y)
        if not hasattr(x, &quot;__len__&quot;):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def train(model, train_iter, valid_iter, loss, num_epochs, optimizer):  #@save
    #animator = Animator(xlabel=&#39;epoch&#39;, xlim=[1, num_epochs], ylim=[0.3, 0.9],
    #                    legend=[&#39;train loss&#39;, &#39;train acc&#39;, &#39;test acc&#39;])
    for epoch in range(num_epochs):
        train_metrics = train_epoch(model, train_iter, loss, updater)
        valid_acc = evaluate_accuracy(model, valid_iter)
        animator.add(epoch + 1, train_metrics + (valid_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss &lt; 0.5, train_loss
    assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc
    assert valid_acc &lt;= 1 and valid_acc &gt; 0.7, valid_acc
</pre></div>
</div>
</div>
</div>
</section>
<section id="id25">
<h3><span class="section-number">1.13.3. </span>CNN<a class="headerlink" href="#id25" title="Permalink to this heading">#</a></h3>
</section>
</section>
<section id="prediction">
<h2><span class="section-number">1.14. </span>Prediction<a class="headerlink" href="#prediction" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>data 準備 (testing data)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Testing
tX = np.random.rand(100, 100, 100, 1)   # 虛構 100 張 100 x 100 單色圖片
tY = np.random.randint(0, 7, [100, 10]) # 虛構 100 個 labels

tX, tY = tX.astype(np.float32), tY.astype(np.float32)
ttsrX, ttsrY = torch.tensor(tX), torch.tensor(tY)
ttsrdataset = TensorDataset(tsrX, tsrY)

ttsrdataloader = DataLoader(
    ttsrdataset, batch_size=4,
    shuffle=False, num_workers=4) # Testing 不需要 shuffle
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># prediction
simpleNN.eval()
y_hat = [simpleNN(x) for x, y in ttsrdataloader]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def predict(model, test_iter, n=6):  #@save
    for X, y in test_iter:
        break
    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true +&#39;\n&#39; + pred for true, pred in zip(trues, preds)]
    d2l.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])

predict_ch3(net, test_iter)
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluation">
<h2><span class="section-number">1.15. </span>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this heading">#</a></h2>
</section>
<section id="save-load-model">
<h2><span class="section-number">1.16. </span>Save/ load model<a class="headerlink" href="#save-load-model" title="Permalink to this heading">#</a></h2>
<section id="weight">
<h3><span class="section-number">1.16.1. </span>只存 weight<a class="headerlink" href="#weight" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html">https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 假設我們 train 好的 model 是 VGG
import torch
import torchvision.models as models
model = models.vgg16(pretrained=True)

# 只存 weight
torch.save(model.state_dict(), &#39;model_weights.pth&#39;)

# 之後做預測，要先 initialize model
import torch
import torchvision.models as models
model = models.vgg16(pretrained=True)

# load weight
model.load_state_dict(torch.load(&#39;model_weights.pth&#39;))

# 開始做 inference
model.eval() # 關閉 batch normalization layer, dropout layer 等
model(x)
</pre></div>
</div>
</div>
</div>
</section>
<section id="weight-model-structure">
<h3><span class="section-number">1.16.2. </span>存 weight 和 model structure<a class="headerlink" href="#weight-model-structure" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>如果把 model structure 和 weight 一起存起來，就可以 load 後，直接 predict</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import torch
import torchvision.models as models
model = models.vgg16(pretrained=True)

# 存檔時，存整個 model
torch.save(model, &#39;model.pth&#39;)

# 讀檔時，直接讀整個 model
model = torch.load(&#39;model.pth&#39;) # 不需要 initialize 一個 model，再去讀 weight 了
</pre></div>
</div>
</div>
</div>
</section>
<section id="checkpoints">
<h3><span class="section-number">1.16.3. </span>checkpoints<a class="headerlink" href="#checkpoints" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html">https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html</a></p></li>
<li><p>看官網這篇就 ok 了</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Save model
simpleNN.cpu() # 先移回 CPU
torch.save(simpleNN.state_dict(), &quot;randmodel.model&quot;)

# Load model
model2 = SimpleNN()
model2.load_state_dict(torch.load(&quot;randmodel.model&quot;))

# 確認是同一個 model
torch.equal(model2(x), simpleNN(x))
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="callbacks">
<h2><span class="section-number">1.17. </span>callbacks<a class="headerlink" href="#callbacks" title="Permalink to this heading">#</a></h2>
<section id="early-stopping">
<h3><span class="section-number">1.17.1. </span>Early stopping<a class="headerlink" href="#early-stopping" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>修改自：<a class="reference external" href="https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch">https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class EarlyStopper:
    def __init__(self, patience=1, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_validation_loss = np.inf
        self.if_break = False

    def monitor(self, validation_loss):
        if validation_loss &lt; self.min_validation_loss:
            self.min_validation_loss = validation_loss
            self.counter = 0
        elif validation_loss &gt; (self.min_validation_loss + self.min_delta):
            self.counter += 1
        self.if_break = True if self.counter &gt;= self.patience else False
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>然後這樣用：</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>early_stopper = EarlyStopper(patience=3, min_delta=10)
for epoch in np.arange(n_epochs):
    train_loss = train_one_epoch(model, train_loader)
    validation_loss = validate_one_epoch(model, validation_loader)
    
    early_stopper.monitor(validation_loss)
    if early_stopper.counter == 0:
        torch.save(model, &#39;model.pth&#39;)
    if early_stopper.if_break:
        break
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="visualization">
<h2><span class="section-number">1.18. </span>Visualization<a class="headerlink" href="#visualization" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>tensorboard 參考這篇：</p>
<ul>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/tensorboard.html">https://pytorch.org/docs/stable/tensorboard.html</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html">https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="explaianation">
<h2><span class="section-number">1.19. </span>Explaianation<a class="headerlink" href="#explaianation" title="Permalink to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "py39"
        },
        kernelOptions: {
            name: "py39",
            path: "./cheatsheet"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'py39'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Welcome to your Jupyter Book</p>
      </div>
    </a>
    <a class="right-next"
       href="../classical_network/vgg/vgg16.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">2. </span>VGG16</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors">1.1. Tensors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-attribute">1.1.1. tensor 與四個重要 attribute</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-type">1.1.1.1. data type</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#device">1.1.1.2. device</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient">1.1.1.3. 關閉 gradient 計算</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-obj-detach">1.1.1.3.1. tensor_obj.detach()</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#with-no-grad">1.1.1.3.2. with no_grad</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor">1.1.2. 100 種 建立 tensor 的方式</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-empty-torch-zeros-torch-ones-torch-rand">1.1.2.1. torch.empty(), torch.zeros(), torch.ones(), torch.rand()</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-manual-seed">1.1.2.2. torch.manual_seed()</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-empty-like-torch-zeros-like-torch-ones-like-torch-rand-like">1.1.2.3. torch.empty_like(), torch.zeros_like(), torch.ones_like(), torch.rand_like()</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-obj-clone">1.1.3. tensor_obj.clone()</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-from-numpy-np-array-and-tensor-obj-clone-numpy">1.1.4. <code class="docutils literal notranslate"><span class="pre">torch.from_numpy(np_array)</span></code> and <code class="docutils literal notranslate"><span class="pre">tensor_obj.clone().numpy()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reshape">1.1.5. reshape</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-reshape">1.1.5.1. torch.reshape()</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#unsqueeze-squeeze">1.1.5.2. unsqueeze (增軸) 與 squeeze (減軸)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stack-tensor">1.1.5.3. stack 組合 tensor (會增軸)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#permute">1.1.5.4. permute 換通道</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-element">1.1.6. 對 tensor 的每個 element 做運算</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">1.1.6.1. 加, 減, 乘, 除, 開根號, 次方, …</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.1.6.2. 取絕對值, 取整數, 截斷, …</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1.1.6.3. 三角函數 與 反函數</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">1.1.6.4. 比較兩 tensor 是否相等</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-summarise">1.1.7. 對 tensor 做 summarise</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">1.1.7.1. 取最大最小值, 平均, 標準差…</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-operation">1.1.8. matrix operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-linear-algebra">1.1.9. 對 tensor 做 linear algebra</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-loss-fn-optimizer">1.2. 自動微分(model, loss_fn 和 optimizer 如何協作)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">1.3. Data preparation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-class">1.3.1. Dataset - 自訂 class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-tensordataset">1.3.2. Dataset - 直接用 <code class="docutils literal notranslate"><span class="pre">TensorDataset</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataloader">1.3.3. DataLoader</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">1.3.4. 內建 dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">1.3.4.1. 圖片類</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#transform">1.3.4.1.1. 無 transform</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">1.3.4.1.2. 有 transform</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transforms">1.4. Transforms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">1.5. activation functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">1.5.1. 內建</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#relu">1.5.1.1. ReLU</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid">1.5.1.2. Sigmoid</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">1.5.2. 自訂</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-layers-block">1.6. custom layers &amp; block</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-layer">1.6.1. custom layer (不帶參數)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">1.6.2. custom layer (帶參數)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-block-nn-sequential-layer1-block2">1.6.3. sequential block (<code class="docutils literal notranslate"><span class="pre">nn.Sequential(layer1,</span> <span class="pre">block2,</span> <span class="pre">...)</span></code>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-for-tips">1.6.4. sequential <code class="docutils literal notranslate"><span class="pre">for</span></code> tips</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-block">1.6.5. custom block</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model">1.6.6. 經典 model 自己寫系列</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#vgg11">1.6.6.1. VGG11</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#vgg-block">1.6.6.1.1. VGG block</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">1.6.6.1.2. 讓 VGG block 疊高高</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#flatten-layer">1.6.6.1.3. flatten layer</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#classifier-layer">1.6.6.1.4. classifier layer</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">1.6.6.1.5. 全部組起來</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet18">1.6.6.2. Resnet18</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet-block">1.6.6.2.1. resnet block</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet-network">1.6.6.2.2. resnet network</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">1.6.7. model 手術</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#module">1.6.7.1. 往後疊加 module</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#seq-obj-append-module-module">1.6.7.1.1. 用 <code class="docutils literal notranslate"><span class="pre">Seq_obj.append(module)</span></code> 來增加 module</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#seq-obj-add-module-name-module-module">1.6.7.1.2. 用 <code class="docutils literal notranslate"><span class="pre">Seq_obj.add_module("name",</span> <span class="pre">module)</span></code> 來增加帶有名稱的module</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">1.7. model 結構/參數管理</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-structure">1.7.1. 看 model structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-bias-gradient">1.7.2. 看單一層的 weight, bias, gradient</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#state-dict">1.7.2.1. <code class="docutils literal notranslate"><span class="pre">.state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-weight-data-weight-grad">1.7.2.2. <code class="docutils literal notranslate"><span class="pre">.weight</span></code>, <code class="docutils literal notranslate"><span class="pre">.weight.data</span></code>, <code class="docutils literal notranslate"><span class="pre">.weight.grad</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-bias-data-bias-grad">1.7.2.3. <code class="docutils literal notranslate"><span class="pre">.bias</span></code>, <code class="docutils literal notranslate"><span class="pre">.bias.data</span></code>, <code class="docutils literal notranslate"><span class="pre">.bias.grad</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">1.7.2.4. <code class="docutils literal notranslate"><span class="pre">.parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#named-parameters">1.7.2.5. <code class="docutils literal notranslate"><span class="pre">.named_parameters</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-weight-bias-gradient">1.7.3. 看所有的 parameters, weight, bias, gradient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#block-factory">1.7.4. block factory</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">1.8. 參數初始化</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">1.9. Transfer learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-classification">1.9.1. image classification</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">1.9.1.1. vgg11</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">1.9.1.1.1. 直接用</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#featrue-extraction">1.9.1.1.2. 僅作 featrue extraction</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">1.9.1.1.3. 把最後一層分類層，換掉</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classical-layers">1.10. Classical Layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nn">1.10.1. NN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-linear-in-dim-out-dim">1.10.1.1. <code class="docutils literal notranslate"><span class="pre">nn.Linear(in_dim,</span> <span class="pre">out_dim)</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flatten">1.10.2. Flatten</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-nn-flatten-start-dim-1-end-dim-1">1.10.2.1. <code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten(start_dim=1,</span> <span class="pre">end_dim=-</span> <span class="pre">1)</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">1.10.3. Dropout</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-dropout-p-0-2">1.10.3.1. <code class="docutils literal notranslate"><span class="pre">nn.Dropout(p=0.2)</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-dropout2d-p-0-2">1.10.3.2. <code class="docutils literal notranslate"><span class="pre">nn.Dropout2d(p=0.2)</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">1.10.4. Batch Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-batchnorm2d">1.10.4.1. <code class="docutils literal notranslate"><span class="pre">nn.BatchNorm2d()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">1.10.5. Layer Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn">1.10.6. CNN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution">1.10.6.1. convolution</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-conv2d">1.10.6.1.1. <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#d-convolution">1.10.6.1.2. 1d convolution</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling">1.10.6.2. pooling</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-maxpool2d">1.10.6.2.1. <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-avgpool2d">1.10.6.2.2. <code class="docutils literal notranslate"><span class="pre">nn.AvgPool2d()</span></code>.</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-adaptiveavgpool2d">1.10.6.2.3. <code class="docutils literal notranslate"><span class="pre">nn.AdaptiveAvgPool2d()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-adaptivemaxpool2d">1.10.6.2.4. <code class="docutils literal notranslate"><span class="pre">nn.AdaptiveMaxPool2d()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn">1.10.7. RNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">1.10.8. Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">1.10.9. Transformer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss">1.11. Loss</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">1.11.1. overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mse">1.11.2. mse</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#class">1.11.2.1. class 版本</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#function">1.11.2.2. function 版本</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mae">1.11.3. mae</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy">1.11.4. binary cross entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">1.11.4.1. class 版本</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">1.11.4.2. function 版</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy-with-logits">1.11.5. binary cross entropy with logits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy">1.11.6. cross-entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">1.11.7. 自訂 loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">1.11.8. 對比學習</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoder">1.11.9. autoencoder</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer">1.12. Optimizer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">1.12.1. 建立 optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate">1.12.2. 不同 learning rate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-scheduler">1.12.3. learning rate scheduler</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loops">1.13. Training loops</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">1.13.1. 完整版 (了解概念用)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy">1.13.2. 模組版 (實際做實驗, deploy 時用)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">1.13.3. CNN</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction">1.14. Prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">1.15. Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#save-load-model">1.16. Save/ load model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight">1.16.1. 只存 weight</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-model-structure">1.16.2. 存 weight 和 model structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoints">1.16.3. checkpoints</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#callbacks">1.17. callbacks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">1.17.1. Early stopping</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization">1.18. Visualization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explaianation">1.19. Explaianation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>