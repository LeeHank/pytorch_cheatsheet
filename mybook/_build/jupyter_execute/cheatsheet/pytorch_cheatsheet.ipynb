{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkQwGmM64L88"
   },
   "source": [
    "# Pytorch Cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2mIcaFQ4L9A"
   },
   "source": [
    "* settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33333,
     "status": "ok",
     "timestamp": 1671629496799,
     "user": {
      "displayName": "Han-Yueh Lee",
      "userId": "16164657475335713235"
     },
     "user_tz": -480
    },
    "id": "zEROVAar4L9C",
    "outputId": "173ff722-413d-4b3f-eb3f-884984624eb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2021,
     "status": "ok",
     "timestamp": 1671629567419,
     "user": {
      "displayName": "Han-Yueh Lee",
      "userId": "16164657475335713235"
     },
     "user_tz": -480
    },
    "id": "1ND4WTmJ4L9D"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/0. codepool_python/python_dl/mybook/pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3763,
     "status": "ok",
     "timestamp": 1671629573162,
     "user": {
      "displayName": "Han-Yueh Lee",
      "userId": "16164657475335713235"
     },
     "user_tz": -480
    },
    "id": "I2RfHMp54L9E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOB3V12G4L9E"
   },
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### tensor 與四個重要 attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 講到 tensor，就想到：\n",
    "  * `tensor 是可以在指定 device (cpu / gpu) 上，進行 forward (一般 array 計算) 和 backward (求gradient) 計算的 numpy array`\n",
    "* 所以，在建立 pytorch tensor 時，要注意以下四個 component:\n",
    "  * `shape` (例如是 shape = (3,224,224) 的 img )\n",
    "  * `dtype`: array 中每個 element 的 data type 是什麼？ e.g. float32, int32,...\n",
    "  * `device`: 這個 tensor 即將在哪個 device 上做計算? (e.g. cpu? gpu?)\n",
    "  * `requires_grad`: 這個 tensor 是否要記錄將來計算 gradient 會用到的訊息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 看一下例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 6., 9.], requires_grad=True)\n",
      "torch.Size([3])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Default datatype for tensors is float32\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=torch.float32, # defaults to None, which is torch.float32 or whatever datatype is passed\n",
    "                               device=torch.device(\"cpu\"), # defaults to None, which uses the default tensor type\n",
    "                               requires_grad=True) # defaults to False. if True, operations performed on the tensor are recorded \n",
    "\n",
    "print(float_32_tensor)\n",
    "print(float_32_tensor.shape) \n",
    "print(float_32_tensor.dtype)\n",
    "print(float_32_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 新手在 pytorch & deep learning 的 coding 時最容易出現以下四種 error：\n",
    "  * shape 錯誤 (e.g. 兩個 tensor 的 shape 無法相乘 )\n",
    "  * dtype 錯誤 (e.g. 拿 float32 和 int32 的 tensor 做計算)\n",
    "  * device 錯誤 (e.g. 兩個 tensor 所處的 device 不同)\n",
    "  * gradient 沒有適時關閉/開啟，影響到 backward calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 有關 shape 的錯誤，這牽扯矩陣計算的知識，所以這裡沒啥好補充的\n",
    "* dtype, device, 和 gradient 的設定，是常常會遇到的，以下馬上做整理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 最常見(也是預設的) 就是 torch.float32，這邊會碰到 precision in computing 的概念，只要記得：  \n",
    "  * float32: 是 single precision floating point，float32 是指 32 bit floating point (32 bit in memory)，也就是可以存 2^32 -1 個數字. \n",
    "  * float16 是 半精度，犧牲一點 precision，但加快計算速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 其他常見的 dtype，包括\n",
    "  * torch.bool\n",
    "  * torch.int8\n",
    "  * torch.uint8\n",
    "  * torch.int16\n",
    "  * torch.int32\n",
    "  * torch.int64\n",
    "  * torch.half\n",
    "  * torch.float32\n",
    "  * torch.float16\n",
    "  * torch.float64\n",
    "  * torch.double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 建立想要的 dtype 的 3 種方式:  \n",
    "  * 用資料讓 pytorch 自行判斷 (給整數/小數 ...，如果給 3.0，那就會判定為 float32)\n",
    "  * 建立 tensor 時，指定 dtype (e.g. `torch.tensor(1, dtype=torch.float32)`)\n",
    "  * 轉換 dtype (e.g `int_tensor = torch.tensor(87); float_tensor = int_tensor.to(torch.float32)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 2.7183],\n",
      "        [1.6180, 0.0073]])\n",
      "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n",
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "# 直接給小數點\n",
    "\n",
    "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
    "print(some_constants)\n",
    "\n",
    "some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
    "print(some_integers)\n",
    "\n",
    "more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n",
    "print(more_integers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[15.1821,  6.0043,  8.3599],\n",
      "        [ 4.2249,  6.5844, 17.2819]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 指定 type\n",
    "a = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 11, 11],\n",
      "        [18,  0, 18]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 轉換 type\n",
    "c = b.to(torch.int32)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2o2S6Y8E4h9_",
    "tags": []
   },
   "source": [
    "#### device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 設定 device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1671629637438,
     "user": {
      "displayName": "Han-Yueh Lee",
      "userId": "16164657475335713235"
     },
     "user_tz": -480
    },
    "id": "JSSZNWQs4L9E",
    "outputId": "a7e807b9-8a7c-4978-9ba8-cb6b217edf55",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-55p_x0t5Jze"
   },
   "source": [
    "* 建立時直接指定 device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5443,
     "status": "ok",
     "timestamp": 1671629710811,
     "user": {
      "displayName": "Han-Yueh Lee",
      "userId": "16164657475335713235"
     },
     "user_tz": -480
    },
    "id": "iUAk15vv5Hl5",
    "outputId": "b9a3d2e8-3c10-4fd8-fb17-4342f76cef88",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0810, 0.1305],\n",
       "        [0.0693, 0.4869]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 2, device = device)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3o9kLzu5h_n"
   },
   "source": [
    "* 確認 目前變數的 device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1671629795001,
     "user": {
      "displayName": "Han-Yueh Lee",
      "userId": "16164657475335713235"
     },
     "user_tz": -480
    },
    "id": "A4i_sH1A5lfF",
    "outputId": "b93310cc-047a-409c-b7af-3b53eed0c691",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPAcUxkG5R6F"
   },
   "source": [
    "* 轉換 device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1671629860010,
     "user": {
      "displayName": "Han-Yueh Lee",
      "userId": "16164657475335713235"
     },
     "user_tz": -480
    },
    "id": "Kp3C1SRK5Uv6",
    "outputId": "b29bfe91-a286-4f51-b077-2ec53a4cad92",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "y1 = torch.rand(2, 2)\n",
    "print(y1.device)\n",
    "\n",
    "y2 = y1.to(device)\n",
    "print(y2.device)\n",
    "\n",
    "y3 = y2.to(torch.device(\"cpu\"))\n",
    "print(y3.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TN5cx7a6581J"
   },
   "source": [
    "* tensor 做計算時，必須在同一個 device 上才能算 (都在 GPU or 都在 CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "executionInfo": {
     "elapsed": 755,
     "status": "error",
     "timestamp": 1671629928941,
     "user": {
      "displayName": "Han-Yueh Lee",
      "userId": "16164657475335713235"
     },
     "user_tz": -480
    },
    "id": "jdFDlqL56D8k",
    "outputId": "2815e591-4d2b-41d4-f7e4-2bea7b13c657",
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, ort, mps, xla, lazy, vulkan, meta, hpu, privateuseone device type at start of device string: gpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m z \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m y  \u001b[38;5;66;03m# exception will be thrown\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, ort, mps, xla, lazy, vulkan, meta, hpu, privateuseone device type at start of device string: gpu"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2, device='gpu')\n",
    "z = x + y  # exception will be thrown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 關閉 gradient 計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tensor_obj.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* .detach() 的意思，主要就是刪掉 gradient 紀錄  \n",
    "* 這主要是用在：\n",
    "  * NN 計算到一半時，你想拿某個中間產物，出去算一些暫時的結果，然後再回來. \n",
    "  * 這時，你不希望中間跑出去算的哪些過程，也被記錄下來，導致去做 backpropagation 時，還會更新到那些 gradient，進而影想到真正的 variable 的 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7399, 0.9776],\n",
      "        [0.9391, 0.1434]], requires_grad=True)\n",
      "tensor([[0.7399, 0.9776],\n",
      "        [0.9391, 0.1434]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.7399, 0.9776],\n",
      "        [0.9391, 0.1434]])\n",
      "tensor([[0.7399, 0.9776],\n",
      "        [0.9391, 0.1434]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 解釋一下這邊發生的事：  \n",
    "  * 我們用 `requires_grad = True` 建立了 a ，所以去 print(a) 時，他告訴我們： requires_grad = True，表示 autograd 和 computation history tracking 都有被 turn on. \n",
    "  * 當我們單純把 a clone 到 b 時，他不僅繼承了 a 的 requires_grad，他也記錄了你的這次 computation history: clone，所以寫成 CloneBackward. \n",
    "  * 但如果我們先把 a detach，再把 a clone 給 c，就可以發現 c 乾乾淨淨的沒有任何 gradient 的痕跡。  \n",
    "  * `detach()` 會 detaches the tensor from its computation history。他等於在說：不管接下來你要做啥計算，都把 autograd 給關起來。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### with no_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 在對 model 做 inference/evaluation 時，會關閉 gradient 的紀錄，這時就會用 context management，把 gradient 先關起來"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 100 種 建立 tensor 的方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.empty(), torch.zeros(), torch.ones(), torch.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5082e+03,  0.0000e+00,  1.5196e+03,  0.0000e+00],\n",
      "        [-8.3862e+32,  4.5706e-41,  9.1084e-44,  0.0000e+00],\n",
      "        [ 6.5767e-36,  0.0000e+00, -4.2653e-20,  4.5708e-41]])\n"
     ]
    }
   ],
   "source": [
    "# 建立出指定 shape 的 placeholder\n",
    "x = torch.empty(3, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 這些數字都是假的，實際上只是在 memory 上幫你開好 (3, 4) 這種 shape 的 placeholder  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1293, 0.2467, 0.8110],\n",
       "        [0.6600, 0.7898, 0.0111]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 3) # 生出 0~1 的隨機數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.manual_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 設 seed 後，接續的第一、第二...、第 n 次生成，結果會不同，但只要再設一次 seed，那結果就會和之前的第一、第二、...、第 n 次相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.empty_like(), torch.zeros_like(), torch.ones_like(), torch.rand_like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[6.5738e-36, 0.0000e+00, 1.5085e+03],\n",
      "         [0.0000e+00, 1.1210e-43, 0.0000e+00]],\n",
      "\n",
      "        [[8.9683e-44, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3881e+00]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[-4.2654e-20,  4.5708e-41, -4.2654e-20],\n",
      "         [ 4.5708e-41,  4.4842e-44,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.1210e-43,  0.0000e+00,  1.5110e+03],\n",
      "         [ 0.0000e+00,  1.4013e-45,  0.0000e+00]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.6128, 0.1519, 0.0453],\n",
      "         [0.5035, 0.9978, 0.3884]],\n",
      "\n",
      "        [[0.6929, 0.1703, 0.1384],\n",
      "         [0.4759, 0.7481, 0.0361]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2, 2, 3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### tensor_obj.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tensor 是 mutable 的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561  # we change a...\n",
    "print(b)       # ...and b is also altered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 所以記得用 tensro_obj.clone() 來做 copy (就是 df.copy() 的類似寫法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a      # different objects in memory...\n",
    "print(torch.eq(a, b))  # ...but still with the same contents!\n",
    "\n",
    "a[0][1] = 561          # a changes...\n",
    "print(b)               # ...but b is still all ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `torch.from_numpy(np_array)` and `tensor_obj.clone().numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 把 numpy 改成 tensor 的目的：  \n",
    "  * 可以放到 GPU 上加速  \n",
    "  * 可以做 autograd. \n",
    "* 把 tensor 改成 numpy 的目的：  \n",
    "  * 做些中途的計算 & 產出，但不會涉及到 gradient 紀錄. \n",
    "  * 特別小心，如果直接用 `tensor_obj.numpy()`，那他們是共享同個記憶體，是 mutable 的，所以改動 numpy 時，會影響到 tensor。所以才要先 clone() 再 numpy() (至於 detach 就不必要了，因為當你轉成 numpy 時，本來就不會有 gradient 紀錄了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "numpy_array = np.ones((2, 3))\n",
    "print(numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(pytorch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 直接做 `tensor_obj.numpy()`，那會是 mutable，改一個，影響另一個："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5062, 0.8469, 0.2588],\n",
      "        [0.2707, 0.4115, 0.6839]])\n",
      "[[0.5062225  0.84694576 0.25884217]\n",
      " [0.2706535  0.41147768 0.6838606 ]]\n"
     ]
    }
   ],
   "source": [
    "pytorch_rand = torch.rand(2, 3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[ 0.5062225   0.84694576  0.25884217]\n",
      " [ 0.2706535  17.          0.6838606 ]]\n"
     ]
    }
   ],
   "source": [
    "numpy_array[1, 1] = 23\n",
    "print(pytorch_tensor)\n",
    "\n",
    "pytorch_rand[1, 1] = 17\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 但如果先 clone 再 .numpy，那就是不同記憶體了，彼此不影響："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0703, 0.5105, 0.9451],\n",
      "        [0.2359, 0.1979, 0.3327]])\n",
      "[[0.07025403 0.5105133  0.9450517 ]\n",
      " [0.2358576  0.19793254 0.33274257]]\n"
     ]
    }
   ],
   "source": [
    "pytorch_rand = torch.rand(2, 3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.clone().numpy()\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[0.07025403 0.5105133  0.9450517 ]\n",
      " [0.2358576  0.19793254 0.33274257]]\n"
     ]
    }
   ],
   "source": [
    "numpy_array[1, 1] = 23\n",
    "print(pytorch_tensor)\n",
    "\n",
    "pytorch_rand[1, 1] = 17\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 介紹四種很常用的 reshape. \n",
    "  * `torch.reshape(input, shape)`\n",
    "  * `torch.squeeze(input)` or/and `torch.unsqueeze(input, dim)`. \n",
    "  * `torch.permute(input, dims)`\n",
    "  * `torch.stack(tensors, dim = 0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7., 8.]), torch.Size([8]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "x = torch.arange(1., 9.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape((2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### unsqueeze (增軸) 與 squeeze (減軸)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我們常常想把單一一張 img 的 shape，增軸成 batch = 1 的一張 img (i.e. 把 shape = (3, 266, 266) 增軸成 (1, 3, 266, 266))\n",
    "* 那 unsqueeze 就是增軸，例如這邊，我想增在 第 0 軸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 226, 226)\n",
    "print(a.shape)\n",
    "\n",
    "b = a.unsqueeze(dim = 0)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 相反的，我們有時候拿到帶有 batch 資訊的資料時，我們想把他 un-batch. \n",
    "* 例如，我拿到 shape = (1, 1) 的 output，但最前面的 1 其實是 batch_size，他就等於 1 而已. \n",
    "* 我想把他拔掉，就用 squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9574]])\n",
      "torch.Size([1, 1])\n",
      "tensor([0.9574])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1,1)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "\n",
    "b = a.squeeze(dim = 0)\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "tensor([[0.4265, 0.5256, 0.9091, 0.2780, 0.2162, 0.9831, 0.8699, 0.2427, 0.2078,\n",
      "         0.7600, 0.9164, 0.7021, 0.0459, 0.6895, 0.2177, 0.6973, 0.8097, 0.9646,\n",
      "         0.1726, 0.5225]])\n",
      "torch.Size([20])\n",
      "tensor([0.4265, 0.5256, 0.9091, 0.2780, 0.2162, 0.9831, 0.8699, 0.2427, 0.2078,\n",
      "        0.7600, 0.9164, 0.7021, 0.0459, 0.6895, 0.2177, 0.6973, 0.8097, 0.9646,\n",
      "        0.1726, 0.5225])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 20)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "b = a.squeeze(0)\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pytorch 很聰明的，如果你的原始軸不是 1 ，他不會幫你 squeeze，例如下例就沒改變任何東西："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8604, 0.8837],\n",
      "        [0.8394, 0.8083]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[0.8604, 0.8837],\n",
      "        [0.8394, 0.8083]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(2, 2)\n",
    "print(c)\n",
    "print(c.shape)\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print(d)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stack 組合 tensor (會增軸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 原始 tensor\n",
    "x = torch.rand((3,224,224)) # img with shape (channel = 3, width = 224, height = 224)\n",
    "y = torch.rand((3,224,224))\n",
    "\n",
    "# 把兩張影像裝成一個 batch\n",
    "batch = torch.stack([x, y], dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### permute 換通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([224, 224, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((3,224,224)) # img with shape (channel = 3, width = 224, height = 224)\n",
    "y = x.permute(1, 2, 0) # 新的第0,1,2軸，分別選原本的第1,2,0軸\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((224,224,3)) # img with shape (width = 224, height = 224, channel = 3)\n",
    "y = x.permute(2, 0, 1) # 新的第0,1,2軸，分別選原本的第2,0,1軸\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 對 tensor 的每個 element 做運算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 加, 減, 乘, 除, 開根號, 次方, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.zeros(2, 2) + 1\n",
    "twos = torch.ones(2, 2) * 2\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
    "fours = twos ** 2 # 次方計算\n",
    "sqrt2s = twos ** 0.5 # 開根號\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 取絕對值, 取整數, 截斷, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[0.8065, 0.5906, 0.6122, 0.4887],\n",
      "        [0.1322, 0.7961, 0.6525, 0.7819]])\n",
      "tensor([[-0., 1., -0., 1.],\n",
      "        [-0., 1., 1., -0.]])\n",
      "tensor([[-1.,  0., -1.,  0.],\n",
      "        [-1.,  0.,  0., -1.]])\n",
      "tensor([[-0.5000,  0.5000, -0.5000,  0.4887],\n",
      "        [-0.1322,  0.5000,  0.5000, -0.5000]])\n"
     ]
    }
   ],
   "source": [
    "# common functions\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三角函數 與 反函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n"
     ]
    }
   ],
   "source": [
    "# trigonometric functions and their inverses\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# bitwise operations\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比較兩 tensor 是否相等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n"
     ]
    }
   ],
   "source": [
    "# comparisons:\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
    "print(torch.eq(d, e)) # returns a tensor of type bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 對 tensor 做 summarise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 取最大最小值, 平均, 標準差..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# reductions:\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d))        # returns a single-element tensor\n",
    "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
    "print(torch.mean(d))       # average\n",
    "print(torch.std(d))        # standard deviation\n",
    "print(torch.prod(d))       # product of all numbers\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### matrix operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* element-wise multiplication: `tensor1*tensor2`\n",
    "* matrix multiplication `tensor.matmul(tensor1, tensor2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 對 tensor 做 linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.3837, 0.1149],\n",
      "        [0.0840, 0.8760]])\n",
      "tensor([[1.1511, 0.3447],\n",
      "        [0.2521, 2.6281]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[ 0.2028,  0.9792],\n",
      "        [ 0.9792, -0.2028]]),\n",
      "S=tensor([2.6867, 1.0936]),\n",
      "V=tensor([[ 0.1788,  0.9839],\n",
      "        [ 0.9839, -0.1788]]))\n"
     ]
    }
   ],
   "source": [
    "# vector and linear algebra operations\n",
    "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
    "v2 = torch.tensor([0., 1., 0.])         # y unit vector\n",
    "m1 = torch.rand(2, 2)                   # random matrix\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print(m3)                  # 3 times m1\n",
    "print(torch.svd(m3))       # singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIS0ljOL4L9F"
   },
   "source": [
    "## 自動微分(model, loss_fn 和 optimizer 如何協作)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT1crLmW4L9F"
   },
   "source": [
    "* 用 linear regression model 來舉例\n",
    "  * 假設我們的 NN，超簡單，就是 input 有 5 個 neuron，output 為 1 個 neuron 的結構。(也就是 5 個變數的線性回歸)\n",
    "  * 寫成數學式：\n",
    "    * 只看一筆資料時： $\\hat{y}_i = x_i^T W + b$, 其中，$x_i^T$ 是 1x5 的向量，W 是 5x1 的矩陣, (W 的 shape 是 (input, output), 所以，如果要 output 出 7 個 neuron, W 是 5x7 矩陣)\n",
    "    * 看一個 batch 的資料時 (e.g. batch_size = 10): $\\hat{y} = XW + b$, 其中，y是 10x1 的向量，X是 10x5 的矩陣，b會做 broad casting，所以變 10x1 的向量，但每個 element 的值都一樣\n",
    "    * cost 是 mse，所以 cost function 是 $cost = \\frac{1}{10} \\sum_{i=1}^{10} \\left( y_i - (x_i^TW + b) \\right)^2 = \\frac{1}{10} \\sum_{i=1}^{10} \\left( y_i - (x_{1i}w_1 + ... + x_{5i}w_5 + b) \\right)^2$\n",
    "    * 所以，做 gradient descent 時，就是要對 $w_i$ 和 $b$ 做偏微分，並用 $w_i^{new} = w_i^{old} - lr\\times grad(w_i)$, $b^{new} = b^{old} - lr\\times grad(b)$來更新參數。\n",
    "    * 可以想像，每一次 iteration，我都要拿到 $W_{grad}$ 和 $b_{grad}$ 這兩個東西，且 shape 會和 $W$ 以及 $b$ 完全相同\n",
    "* 用 pytorch 來解這個問題，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_true: \n",
      " tensor([[0.1672],\n",
      "        [0.7261],\n",
      "        [0.8677],\n",
      "        [0.1589],\n",
      "        [0.6089]]) \n",
      "\n",
      "b_true: \n",
      " tensor([0.2286]) \n",
      "\n",
      "y_true: \n",
      " tensor([[1.9082],\n",
      "        [1.3121],\n",
      "        [1.5016],\n",
      "        [1.1900],\n",
      "        [1.6450],\n",
      "        [1.6336],\n",
      "        [2.1329],\n",
      "        [1.1671],\n",
      "        [1.6328],\n",
      "        [1.7094]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 真值\n",
    "X = torch.rand(10,5)\n",
    "W_true = torch.rand(5,1)\n",
    "b_true = torch.rand(1)\n",
    "y_true = torch.matmul(X,W_true)+b_true\n",
    "\n",
    "print('W_true: \\n', W_true, '\\n')\n",
    "print('b_true: \\n', b_true, '\\n')\n",
    "print('y_true: \\n', y_true, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current estmated W: \n",
      " tensor([[-0.2711,  0.1681,  0.3631,  0.1607,  0.2658]]) \n",
      "\n",
      "current W_grad: \n",
      " None \n",
      "\n",
      "current estmated b: \n",
      " tensor([-0.1775]) \n",
      "\n",
      "current b_grad: \n",
      " None \n",
      "\n",
      "current y_pred: \n",
      " tensor([[ 0.3968],\n",
      "        [-0.0029],\n",
      "        [ 0.3377],\n",
      "        [ 0.0565],\n",
      "        [ 0.1576],\n",
      "        [ 0.3519],\n",
      "        [ 0.3798],\n",
      "        [ 0.0702],\n",
      "        [ 0.1862],\n",
      "        [ 0.4231]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "current loss: \n",
      " tensor(1.8532, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = torch.nn.Linear(5, 1)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
    "\n",
    "# loss\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# W 參數估計的起始值\n",
    "print('current estmated W: \\n', model.weight.data, '\\n')\n",
    "print('current W_grad: \\n', model.weight.grad, '\\n')\n",
    "\n",
    "# b 參數估計的起始值\n",
    "print('current estmated b: \\n', model.bias.data, '\\n')\n",
    "print('current b_grad: \\n', model.bias.grad, '\\n')\n",
    "\n",
    "# 目前的預測值\n",
    "y_pred = model(X)\n",
    "print('current y_pred: \\n', y_pred, '\\n')\n",
    "\n",
    "# 目前的 loss\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "print('current loss: \\n', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 因為資料進來了，所以可以做 backward，取得 W_grad 和 b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current W_grad: \n",
      " tensor([[-0.9529, -1.7177, -1.5181, -1.1836, -1.3753]]) \n",
      "\n",
      "current b_grad: \n",
      " tensor([-2.6952]) \n",
      "\n",
      "current estmated W: \n",
      " tensor([[-0.2616,  0.1853,  0.3783,  0.1725,  0.2796]]) \n",
      "\n",
      "current estmated b: \n",
      " tensor([-0.1506]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 先清空目前的 gradient\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 算 gradient\n",
    "loss.backward()\n",
    "\n",
    "# 看一下算出來的 W_grad 和 b_grad\n",
    "print('current W_grad: \\n', model.weight.grad, '\\n')\n",
    "print('current b_grad: \\n', model.bias.grad, '\\n')\n",
    "\n",
    "# 更新 W 和 b\n",
    "optimizer.step()\n",
    "\n",
    "# 看一下新的 W 和 b\n",
    "print('current estmated W: \\n', model.weight.data, '\\n')\n",
    "print('current estmated b: \\n', model.bias.data, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current y_pred: \n",
      " tensor([[0.4669],\n",
      "        [0.0512],\n",
      "        [0.3960],\n",
      "        [0.1120],\n",
      "        [0.2224],\n",
      "        [0.4154],\n",
      "        [0.4516],\n",
      "        [0.1181],\n",
      "        [0.2523],\n",
      "        [0.4823]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "current loss: \n",
      " tensor(1.6898, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 第二次的 forward，應該可以看到 loss 變小\n",
    "y_pred = model(X)\n",
    "print('current y_pred: \\n', y_pred, '\\n')\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "print('current loss: \\n', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current W_grad: \n",
      " tensor([[-0.9529, -1.7177, -1.5181, -1.1836, -1.3753]]) \n",
      "\n",
      "current b_grad: \n",
      " tensor([-2.6952]) \n",
      "\n",
      "current W_grad: \n",
      " tensor([[0., 0., 0., 0., 0.]]) \n",
      "\n",
      "current b_grad: \n",
      " tensor([0.]) \n",
      "\n",
      "current W_grad: \n",
      " tensor([[-0.9107, -1.6405, -1.4495, -1.1284, -1.3128]]) \n",
      "\n",
      "current b_grad: \n",
      " tensor([-2.5729]) \n",
      "\n",
      "current estmated W: \n",
      " tensor([[-0.2439,  0.2171,  0.4064,  0.1945,  0.3051]]) \n",
      "\n",
      "current estmated b: \n",
      " tensor([-0.1006]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 第二次的 backward\n",
    "\n",
    "# 可以看到，原本的 gradient 都還在，若不清空，gradient 會累加 (這不是我們要的)\n",
    "print('current W_grad: \\n', model.weight.grad, '\\n')\n",
    "print('current b_grad: \\n', model.bias.grad, '\\n')\n",
    "\n",
    "# 所以才要這樣清空 gradient\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 看一下 gradient 被清空了\n",
    "print('current W_grad: \\n', model.weight.grad, '\\n')\n",
    "print('current b_grad: \\n', model.bias.grad, '\\n')\n",
    "\n",
    "# 這時再來做 backward，算 gradient\n",
    "loss.backward()\n",
    "\n",
    "# 看一下算出來的 W_grad 和 b_grad\n",
    "print('current W_grad: \\n', model.weight.grad, '\\n')\n",
    "print('current b_grad: \\n', model.bias.grad, '\\n')\n",
    "\n",
    "# 更新 W 和 b\n",
    "optimizer.step()\n",
    "\n",
    "# 看一下新的 W 和 b\n",
    "print('current estmated W: \\n', model.weight.data, '\\n')\n",
    "print('current estmated b: \\n', model.bias.data, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current y_pred: \n",
      " tensor([[0.5971],\n",
      "        [0.1515],\n",
      "        [0.5041],\n",
      "        [0.2151],\n",
      "        [0.3425],\n",
      "        [0.5333],\n",
      "        [0.5848],\n",
      "        [0.2070],\n",
      "        [0.3749],\n",
      "        [0.5921]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "current loss: \n",
      " tensor(1.4068, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 第三次的 forward，應該可以看到 loss 變更小\n",
    "y_pred = model(X)\n",
    "print('current y_pred: \\n', y_pred, '\\n')\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "print('current loss: \\n', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 以上，就是實際在跑 deep learning model 時，如何使用 loss, optimizer，來做到求 gradient 和更新參數～"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcf7Osi64L9G"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPb52zVD4L9G"
   },
   "source": [
    "### Dataset - 自訂 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "MnqOFGvr4L9H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, feature_matrix, label_vector):             # 把資料存進 class object\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.label_vector = label_vector\n",
    "    def __len__(self):\n",
    "        assert len(self.feature_matrix) == len(self.label_vector) # 確定資料有互相對應\n",
    "        return len(self.feature_matrix)\n",
    "    def __getitem__(self, idx):                     # 定義我們需要取得某筆資料的方式\n",
    "        return self.feature_matrix[idx], self.label_vector[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "SdXZbeAZ4L9I",
    "outputId": "5ca2722c-855c-4fe6-bf5e-db0536957600",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 100, 1), (10,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 測試看看\n",
    "\n",
    "X = np.random.rand(1000, 100, 100, 1)   # 虛構 1000 張 100 x 100 單色圖片\n",
    "Y = np.random.randint(0, 7, [1000, 10]) # 虛構 1000 個 labels\n",
    "\n",
    "my_dataset = MyDataset(X.astype(np.float32), Y.astype(np.float32))\n",
    "taken_x, taken_y = my_dataset[0] # 取得第一筆資料\n",
    "taken_x.shape, taken_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWDIwPoS4L9J"
   },
   "source": [
    "### Dataset - 直接用 `TensorDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "o06ryKhX4L9K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 手上有的資料，先轉成 Tensor\n",
    "X = np.random.rand(1000, 100, 100, 1)   # 虛構 1000 張 100 x 100 單色圖片\n",
    "Y = np.random.randint(0, 7, [1000, 10]) # 虛構 1000 個 labels\n",
    "tsrX, tsrY = torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "# 餵到 TensorDataset 裡面\n",
    "tsrdataset = TensorDataset(tsrX, tsrY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "ufHwGTIi4L9K",
    "outputId": "e9f57751-904a-46c2-b210-f38d96b5310d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "torch.Size([100, 100, 1]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 幾個重要的用法\n",
    "print(tsrdataset.__len__()) # 幾張圖\n",
    "taken_x, taken_y = tsrdataset[0] # 取得第一筆資料\n",
    "print(taken_x.shape, taken_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5t_vRU-64L9L"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "iccIfheQ4L9L",
    "outputId": "9b8f70f9-7091-4532-85ac-7f45dfa9bef0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([4, 100, 100, 1]), torch.Size([4, 10]))\n"
     ]
    }
   ],
   "source": [
    "# 將 dataset 包裝成 dataloader\n",
    "my_dataloader = DataLoader(\n",
    "    my_dataset, \n",
    "    batch_size=4,\n",
    "    shuffle=True #, \n",
    "    # num_workers=4\n",
    ")\n",
    "\n",
    "# 跑一個 loop 確認拿到的 batch 是否正確\n",
    "for batch_x, batch_y in my_dataloader:\n",
    "    print((batch_x.shape, batch_y.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3S44x0TW4L9M"
   },
   "source": [
    "### 內建 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOq2f30o4L9M"
   },
   "source": [
    "#### 圖片類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U4u4tgo4L9N"
   },
   "source": [
    "##### 無 transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "9UZNPk0a4L9N",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ubuntu/pytorch_dataset/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ubuntu/pytorch_dataset/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ubuntu/pytorch_dataset/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ubuntu/pytorch_dataset/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/ubuntu/pytorch_dataset/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    root=\"/home/ubuntu/pytorch_dataset\", train=True, download=True)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(\n",
    "    root=\"/home/ubuntu/pytorch_dataset\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_NFTcGJ4L9N"
   },
   "source": [
    "* 此時為 dataset 格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "7os5wKxB4L9O",
    "outputId": "0063e6cb-47f9-475d-8259-a3b27bc84bab",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /home/ubuntu/pytorch_dataset\n",
       "    Split: Train"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALktl29f4L9P"
   },
   "source": [
    "* 用 index 可以取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "hqeJNYn64L9P",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y = mnist_train[0] # 第 0 筆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_SlrD8Z4L9P"
   },
   "source": [
    "* x 會是 PIL 物件, y 是 lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "NsSFnSBN4L9P",
    "outputId": "e516b8f1-608f-4278-cf96-7e1f7d8bd890",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "owGAU_LU4L9Q",
    "outputId": "d8f8433a-080a-4728-e1c5-02f27fc55050",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACBUlEQVR4nLXSz0tUURQH8O+597373rx545g6NmNJY6TlJgozEsKFZGCrkDBoVdGmdf9BiwJ3bVr1PwhBUbQfJQcKjWZRTsjo0KDpDPV+eN+997WK0Z2bvtsPfM+Bc4D/HwJycwBZAEAE6zAyfe5RFMQfFYiRAj+CXM/c2HK82VetVMM34RGUmCxz9v7yYnW9dnWyskyHR6azi72Jwep3ScVk9c7LLhKQrpRBSiI2n76puZFT3doUwH4pcmw/zpjrU2zw3dFt4XEWdvbKhpinzTBYt5bDH4qlLYO8cbKBWHOudDHluFtiLrLD0kmM2//6q9VFS+JLLDxv9GzMPV9v3XuzYgHEGSUGCngbRCLd4W6CxPCLHTDwVMkDA0y/qEa/lFFBoF2EEME8CAD6hsZK8+djlmSatuiXXsWfNh27NU6Yelro1bytPElRbaGaO1FGPfc7zPg9HvHlIaVD8AjID9y/+bgZ/6iP9ks7J/QZevh8w/cd2PlGs8CKt92R7MQEk0yA7GtWq9ETN3zRs7fpR7FaWi/3yXaipW1IjFnbaSM70N7dsRzbzbHd8aCx7+wmKskUO5esz0sPmvXYF67gBzoNfxptxb5stxM10iLg1pOTO23NhcUptW1hE6gFYYprC8QMMPNsMM+4pamVbps/HGkSsg+1Cv4d+0Jh//Sm3DjGix4rfwFoJNh2/0cDFgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "-hRV0llv4L9Q",
    "outputId": "512d51ee-159d-4a9e-f6c5-1acbd1212c37",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "P-CYSZo94L9R",
    "outputId": "a873f759-1efc-4ec2-e1e1-2b76e8fee425",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPPirh4p4L9S"
   },
   "source": [
    "* 可以把 x 轉成 numpy 看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "v0AfEt_y4L9S",
    "outputId": "607f16c5-123e-4d0a-f20e-52bbde7e1b07",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "uint8\n",
      "0\n",
      "255\n"
     ]
    }
   ],
   "source": [
    "x_array = np.asarray(x)\n",
    "print(x_array.shape)\n",
    "print(x_array.dtype)\n",
    "print(x_array.min())\n",
    "print(x_array.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHeBkOCm4L9S"
   },
   "source": [
    "* 可以看到是 28x28 的圖，且是 uint8 type，介於 0~255 整數值  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1McesDe4L9T"
   },
   "source": [
    "##### 有 transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj2xBIER4L9T"
   },
   "source": [
    "* 圖片類資料庫，通常都會做以下 transform:  \n",
    "  * 把圖片改成 float32 浮點數 type. \n",
    "  * 把圖片正規化到 0~1 之間\n",
    "  * 轉成 tensor (灰階圖，會變成 (1,28,28), RGB圖仍是 (3, 28, 28)) \n",
    "* 這其實就是 `torchvision.transforms.ToTensor()` 在做的事\n",
    "* 看一下剛剛的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "DzbsQ6xf4L9T",
    "outputId": "c047bbe0-9c6f-4651-9c89-6d752fe4f1b8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n",
      "uint8\n",
      "<class 'torch.Tensor'>\n",
      "torch.float32\n",
      "tensor(0.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(type(x))\n",
    "print(np.asarray(x).dtype)\n",
    "\n",
    "trans = torchvision.transforms.ToTensor()\n",
    "x_trans = trans(x)\n",
    "print(type(x_trans))\n",
    "print(x_trans.dtype)\n",
    "print(x_trans.min())\n",
    "print(x_trans.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqy6qj5S4L9T"
   },
   "source": [
    "* 讀檔時，就可以把這個放進去："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "-6hw3N994L9T",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans = transforms.ToTensor()\n",
    "\n",
    "mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    root=\"/home/ubuntu/pytorch_dataset\", train=True, transform=trans, download=True)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(\n",
    "    root=\"/home/ubuntu/pytorch_dataset\", train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "1aZDD_X_4L9T",
    "outputId": "c58ceda4-0c69-4407-d240-300a9bf41cfc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /home/ubuntu/pytorch_dataset\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "YnCIa0zU4L9U",
    "outputId": "94d3a4f8-f13f-4182-d3b4-8a3dd354fcb7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x, y = mnist_train[0]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "sPrzHHk34L9U",
    "outputId": "867da894-daf9-4de5-b7c5-1a81f3e3280a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LMUdfbC4L9U"
   },
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transforms 可以大致分為兩種：\n",
    "  * 基本的影像前處理 (input 是 PIL 物件，output 也還是 PIL 物件)，例如：\n",
    "    * `transforms.Resize(size)`:  input 是 PIL 物件，size 如果是 sequence (e.g. (h,w)), 那就是 resize 成 (h,w) 的大小。size 如果是 int (e.g. 224)，那他會幫你把短邊調到 224, 長邊就等比例縮小。  \n",
    "    * `transforms.CenterCrop(size)`: input 是 PIL 物件，依據給定的 size 沿中心裁減，如果 size 是 int，就 crop 成 (size, size)，如果 size 是 (h, w)，就 crop 成 (h, w)   \n",
    "    * `transforms.RandomCrop(size)`: input 是 PIL 物件，依據給定的 size，隨機沿一個中心裁減，size 定法同上). \n",
    "    * `transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.33), interpolation=2)`: 先隨機沿某個中心，將原影像剪裁為原圖 scale 倍的圖片 (以此例來說，就是剪成原圖 0.08 倍 ~ 1 倍),剪裁的長寬比，介於0.75~1.33 之間。剪完後，resize 成 size 大小的圖片，resize 方法預設是雙線性內插法 PIL.Image.BILINEAR   \n",
    "    * `transforms.RandomHorizontalFlip(p=0.5)`: 隨機上下翻轉\n",
    "    * `transforms.RandomVerticalFlip(p=0.5)`: 隨機左右翻轉\n",
    "    * `transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))`: 就 gaussianBlur\n",
    "    * `transforms.Grayscale()`: input 是 PIL 物件，幫你轉成灰階. \n",
    "  * 餵進 model 時的數值處理，例如：\n",
    "    * `transforms.ToTensor()`: input 是 PIL 或 numpy，output是 0 到 1 tensor.float, 且通道在前。(如果 numpy 的 type 是 uint8，那 range 是 0 到 255, 他就會幫你 scaling 到 0~1; 如果 numpy 的 type 就已經是 float，他就不會再幫你 scaling). \n",
    "    * `transforms.Normalize()`: input 是 tensor, 假設 k 個通道，那你就要給他 k 維的 mean list 和 sd list，他就幫你對每一個通道做 normalization。如果是用 imagenet pretrained model，他根據資料庫所有影像已幫你算好各通道的平均數和邊準差，分別是： `mean=[0.485, 0.456, 0.406]`, `std=[0.229, 0.224, 0.225]`\n",
    "* 官網的範例可以看這：https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py \n",
    "* 官網的詳細文件看這：https://pytorch.org/vision/stable/transforms.html  \n",
    "* 以下列出基本套路，再個別實驗來看看效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 imagenet pre-trained model 需做的 preprocess\n",
    "imagenet_transform = transforms.Compose([\n",
    "    # pre-processing\n",
    "    transforms.Resize((224, 224)),\n",
    "    # to tensor & normalization\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# training 時常用的 augmentation\n",
    "training_transform = transforms.Compose([\n",
    "    # pre-processing\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    # to tensor\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# testing/inference 時用的，就基本的\n",
    "training_transform = transforms.Compose([\n",
    "    # pre-processing\n",
    "    transforms.Resize((224, 224)),\n",
    "    # to tensor\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5q6s5KM4L9V"
   },
   "source": [
    "## activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-q7xHij4L9V"
   },
   "source": [
    "### 內建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6mXTNG84L9V"
   },
   "source": [
    "| activation function | `torch.nn as nn`                    | `torch.nn.functional as F` |\n",
    "|:-------------------:| ----------------------------------- | -------------------------- |\n",
    "| Sigmoid             | `nn.Sigmoid()`                      | `F.sigmoid`                |\n",
    "| Softmax             | `nn.Softmax(dim=None)`              | `F.softmax`                |\n",
    "| ReLU                | `nn.ReLU()`                         | `F.relu`                   |\n",
    "| LeakyReLU           | `nn.LeakyReLU(negative_slope=0.01)` | `F.leaky_relu`             |\n",
    "| Tanh                | `nn.Tanh()`                         | `F.tanh`                   |\n",
    "| GELU                | `nn.GELU()`                         | `F.gelu`                   |\n",
    "| ReLU6               | `nn.ReLU6()`                        | `F.relu6`                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAf0qwmF4L9V"
   },
   "source": [
    "#### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go7w3Hqa4L9V"
   },
   "source": [
    "* 主要重點：  \n",
    "  * $ReLU(x) = max(x, 0)$  \n",
    "  * $\\frac{x}{dx} ReLU(x) = 1$ if x > 0; $\\frac{x}{dx} ReLU(x) = 0$ if x <= 0\n",
    "  * relu 的導數，在 x = 0 時，數學上是不存在，但在工程上 \"定義\" 導數為 0，這樣就能繼續做了  \n",
    "  * relu 的優點是求導的結果簡單，不是 0 就是 1，在 backward 更新參數時， `weight_new = weight_old - learning_rate * grad`，那 grad 不是 0 就是 1，減輕了以往NN的梯度消失問題。  \n",
    "* 簡單範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "BHm-JKTx4L9V",
    "outputId": "b49d2df1-57f5-4fd0-fbc7-a5184df2b175",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 4.4409e-16, 1.0000e-01, 2.0000e-01, 3.0000e-01,\n",
      "        4.0000e-01, 5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01,\n",
      "        1.0000e+00, 1.1000e+00, 1.2000e+00, 1.3000e+00, 1.4000e+00, 1.5000e+00,\n",
      "        1.6000e+00, 1.7000e+00, 1.8000e+00, 1.9000e+00, 2.0000e+00, 2.1000e+00,\n",
      "        2.2000e+00, 2.3000e+00, 2.4000e+00, 2.5000e+00, 2.6000e+00, 2.7000e+00,\n",
      "        2.8000e+00, 2.9000e+00, 3.0000e+00, 3.1000e+00, 3.2000e+00, 3.3000e+00,\n",
      "        3.4000e+00, 3.5000e+00, 3.6000e+00, 3.7000e+00, 3.8000e+00, 3.9000e+00,\n",
      "        4.0000e+00, 4.1000e+00, 4.2000e+00, 4.3000e+00, 4.4000e+00, 4.5000e+00,\n",
      "        4.6000e+00, 4.7000e+00, 4.8000e+00, 4.9000e+00, 5.0000e+00, 5.1000e+00,\n",
      "        5.2000e+00, 5.3000e+00, 5.4000e+00, 5.5000e+00, 5.6000e+00, 5.7000e+00,\n",
      "        5.8000e+00, 5.9000e+00, 6.0000e+00, 6.1000e+00, 6.2000e+00, 6.3000e+00,\n",
      "        6.4000e+00, 6.5000e+00, 6.6000e+00, 6.7000e+00, 6.8000e+00, 6.9000e+00,\n",
      "        7.0000e+00, 7.1000e+00, 7.2000e+00, 7.3000e+00, 7.4000e+00, 7.5000e+00,\n",
      "        7.6000e+00, 7.7000e+00, 7.8000e+00, 7.9000e+00],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "activation = nn.ReLU()\n",
    "\n",
    "x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n",
    "y = activation(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Wi0ONP8u4L9W",
    "outputId": "169ee231-bbde-42eb-fe9b-6f77cdff6ff0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x.detach(), y.detach());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "yzl_AgHw4L9W",
    "outputId": "64fb72cc-195a-46bb-dd95-43229da37eb1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlLUlEQVR4nO3df1TW9f3/8QcXyoV+E7IIVKKwH8s6FhomkXayRZIzN9fWPOXSMbNTB5vGWoqm5McfVCfNTU3KMtvKI9XKajo7xrKfdDCJPlmpa+Y0DcRVXA43sOv9/v4h14V8RONC8Bm877dzrmO+fb+5nu9jXDx8/Xi+o1zXdQUAAGDEZ10AAADwNsIIAAAwRRgBAACmCCMAAMAUYQQAAJgijAAAAFOEEQAAYIowAgAATHWxLqAlHMfR3r171aNHD0VFRVmXAwAAWsB1XR04cEB9+vSRz3fs8Y8OEUb27t2rlJQU6zIAAEAr7N69W2eeeeYx/7xDhJEePXpIOnwzcXFxxtUAAICWCAQCSklJCf8cP5YOEUZCUzNxcXGEEQAAOpjvWmLBAlYAAGCKMAIAAEwRRgAAgCnCCAAAMEUYAQAApggjAADAFGEEAACYIowAAABThBEAAGAq4jDy5ptvatSoUerTp4+ioqK0Zs2a77xm48aNuvTSS+X3+3Xeeedp5cqVrSgVAAB0RhGHkdraWqWlpWnp0qUtOv/zzz/XyJEjdfXVV6uiokJTpkzRrbfeqldffTXiYgEAQOcT8bNpRowYoREjRrT4/KKiIvXt21cLFiyQJF144YV6++239fDDDys7OzvStwcAAJ1Muz8or7S0VFlZWU2OZWdna8qUKce8pq6uTnV1deHfBwKB9ioPwPfAezv+pVc/rrQuA/C0Xw/pq5TTupu8d7uHkcrKSiUlJTU5lpSUpEAgoP/85z/q1q3bUdcUFhZq9uzZ7V0agO+JqX/+X/3zXwetywA8bVRan84bRlojPz9feXl54d8HAgGlpKQYVgSgPf37v99Kkm4anKLT/l+McTWANyXFxZq9d7uHkV69eqmqqqrJsaqqKsXFxTU7KiJJfr9ffr+/vUsD8D3huK4kacLQvjovsYdxNQBOtnbvM5KZmamSkpImxzZs2KDMzMz2fmsAHYRzOIsoKirKthAAJiIOI//+979VUVGhiooKSYe37lZUVGjXrl2SDk+xjBs3Lnz+7bffrh07duiee+7R1q1b9cgjj+jZZ5/VXXfd1TZ3AKDDC42M+AgjgCdFHEbef/99DRw4UAMHDpQk5eXlaeDAgZo1a5Yk6csvvwwHE0nq27ev1q5dqw0bNigtLU0LFizQ448/zrZeAGENWUQ+sgjgSRGvGRk2bJjc0CdHM5rrrjps2DB98MEHkb4VAI8IOoyMAF7Gs2kAmAtN05BFAG8ijAAw1zhNQxoBvIgwAsBcaGQkmkUjgCcRRgCYY5oG8DbCCABzDtM0gKcRRgCYcpzG3XmEEcCbCCMATDnukWHEsBAAZggjAEwdMTBCO3jAowgjAEwdOTLCbhrAmwgjAEwd2dCZLAJ4E2EEgKmma0ZII4AXEUYAmAoeEUbIIoA3EUYAmHKdxv9mZATwJsIIAFNM0wAgjAAwRZ8RAIQRAKZCfUaiougzAngVYQSAKbdhZIQpGsC7CCMATAXDYcS4EABmCCMATDVO05BGAK8ijAAwFXpqLyMjgHcRRgCYCm2mYc0I4F2EEQCmQlt7owkjgGcRRgCYCoURsgjgXYQRAKZCC1h9LBoBPIswAsCUQ58RwPMIIwBMOfQZATyPMALAlNPw1F76jADeRRgBYIrdNAAIIwBMNfYZsa0DgB3CCABTjVt7SSOAVxFGAJgKPyiPTyPAs/j2B2DKZWsv4HmEEQCmHJ5NA3geYQSAKZ7aC4AwAsAUIyMACCMATLFmBABhBICpIE/tBTyPMALAFNM0AAgjAEw59BkBPI9vfwCmXJ5NA3geYQSAKZ7aC4AwAsBUeJqGLAJ4FmEEgCmHrb2A5xFGAJhiNw0AwggAUw59RgDPI4wAMMXICADCCABT4a29rGAFPIswAsAU0zQACCMATAUb+owwTQN4F2EEgCn6jAAgjAAw5dJnBPA8wggAU6HdNLSDB7yLMALAlBPeTWNcCAAzfPsDMEWfEQCEEQCmWDMCgDACwFTQoc8I4HWEEQCmmKYB0KowsnTpUqWmpio2NlYZGRkqKys77vmLFi3SBRdcoG7duiklJUV33XWX/vvf/7aqYACdi0ufEcDzIg4jxcXFysvLU0FBgcrLy5WWlqbs7Gzt27ev2fNXrVqladOmqaCgQJ9++qmeeOIJFRcXa/r06SdcPICOL9z0jDQCeFbEYWThwoWaOHGicnJydNFFF6moqEjdu3fXihUrmj3/3Xff1ZAhQ3TzzTcrNTVVw4cP10033fSdoykAvIFpGgARhZH6+npt3rxZWVlZjV/A51NWVpZKS0ubveaKK67Q5s2bw+Fjx44dWrdunX70ox8d833q6uoUCASavAB0TrSDB9AlkpP379+vYDCopKSkJseTkpK0devWZq+5+eabtX//fg0dOlSu6+rbb7/V7bffftxpmsLCQs2ePTuS0gB0UI7D1l7A69p9N83GjRs1f/58PfLIIyovL9cLL7ygtWvXas6cOce8Jj8/XzU1NeHX7t2727tMAEZoBw8gopGRhIQERUdHq6qqqsnxqqoq9erVq9lrZs6cqVtuuUW33nqrJOniiy9WbW2tbrvtNs2YMUM+39F5yO/3y+/3R1IagA6KaRoAEY2MxMTEKD09XSUlJeFjjuOopKREmZmZzV5z8ODBowJHdHS0pMYtfQC8iwWsACIaGZGkvLw8jR8/XoMGDdLgwYO1aNEi1dbWKicnR5I0btw4JScnq7CwUJI0atQoLVy4UAMHDlRGRoY+++wzzZw5U6NGjQqHEgDe5YYflEcYAbwq4jAyZswYVVdXa9asWaqsrNSAAQO0fv368KLWXbt2NRkJuffeexUVFaV7771Xe/bs0RlnnKFRo0Zp3rx5bXcXADqs0DQNAyOAd0W5HWCuJBAIKD4+XjU1NYqLi7MuB0Abuv+vW1X0xj80YWhfzbz+IutyALShlv785tk0AEzRDh4AYQSAqcbdNKQRwKsIIwBM0WcEAGEEgCknvJvGuBAAZvj2B2DKpc8I4HmEEQCmgk5oay9hBPAqwggAU7SDB0AYAWCKdvAACCMATNFnBABhBICp8DQNaQTwLMIIAFNM0wAgjAAw5ThM0wBeRxgBYIp28AAIIwBM0Q4eAGEEgCn6jAAgjAAwRTt4AIQRAKbY2guAMALAFNM0AAgjAEwFncO/Mk0DeBdhBIAp2sEDIIwAMBWapmFrL+BdhBEApmgHD4AwAsBUaGQkmk8jwLP49gdgij4jAAgjAEwFHdaMAF5HGAFgij4jAAgjAEwxTQOAMALAFCMjAAgjAEw1hhHSCOBVhBEApugzAoAwAsBU41N7jQsBYIZvfwCmaAcPgDACwJTDU3sBzyOMADDFbhoAhBEApugzAoAwAsAUW3sBEEYAmAoyTQN4HmEEgKnwNA1pBPAswggAUyxgBUAYAWCKPiMACCMATNFnBABhBIApt2FkJJowAngWYQSAqWB4msa4EABmCCMATPHUXgCEEQCmXJ7aC3ge3/4ATDEyAoAwAsAUfUYAEEYAmHIcnk0DeB1hBIApntoLgDACwFSQp/YCnkcYAWDKoc8I4HmEEQCmHJ7aC3geYQSAKZfdNIDnEUYAmKLPCADCCABTDgtYAc8jjAAw47ruEVt7bWsBYKdVYWTp0qVKTU1VbGysMjIyVFZWdtzzv/nmG+Xm5qp3797y+/36wQ9+oHXr1rWqYACdR2iKRmJkBPCyLpFeUFxcrLy8PBUVFSkjI0OLFi1Sdna2tm3bpsTExKPOr6+v17XXXqvExEQ9//zzSk5O1j//+U+deuqpbVE/gA4sNEUjEUYAL4s4jCxcuFATJ05UTk6OJKmoqEhr167VihUrNG3atKPOX7Fihb766iu9++676tq1qyQpNTX1xKoG0CkcGUaimDQGPCuib//6+npt3rxZWVlZjV/A51NWVpZKS0ubvebll19WZmamcnNzlZSUpP79+2v+/PkKBoPHfJ+6ujoFAoEmLwCdj8s0DQBFGEb279+vYDCopKSkJseTkpJUWVnZ7DU7duzQ888/r2AwqHXr1mnmzJlasGCB5s6de8z3KSwsVHx8fPiVkpISSZkAOogjR0aiCSOAZ7X7wKjjOEpMTNRjjz2m9PR0jRkzRjNmzFBRUdExr8nPz1dNTU34tXv37vYuE4CB4BErWMkigHdFtGYkISFB0dHRqqqqanK8qqpKvXr1avaa3r17q2vXroqOjg4fu/DCC1VZWan6+nrFxMQcdY3f75ff74+kNAAdELtpAEgRjozExMQoPT1dJSUl4WOO46ikpESZmZnNXjNkyBB99tlnchwnfGz79u3q3bt3s0EEgHe4TXbTGBYCwFTE0zR5eXlavny5nnrqKX366ae64447VFtbG95dM27cOOXn54fPv+OOO/TVV19p8uTJ2r59u9auXav58+crNze37e4CQIfEyAgAqRVbe8eMGaPq6mrNmjVLlZWVGjBggNavXx9e1Lpr1y75fI0ZJyUlRa+++qruuusuXXLJJUpOTtbkyZM1derUtrsLAB1Sk629ZBHAs6LcI8dJv6cCgYDi4+NVU1OjuLg463IAtJF9B/6rwfNK5IuSdhSOtC4HQBtr6c9v2gwBMBNaSsYUDeBthBEAZnhiLwCJMALAUCiMkEUAbyOMADATWrHGyAjgbYQRAGYap2mMCwFgijACwEyoz4iPNAJ4GmEEgJnQs2mYpgG8jTACwIzLNA0AEUYAGHJYwApAhBEAhhq39hJGAC8jjAAww24aABJhBIAh+owAkAgjAAyFdtNEMzQCeBphBIAZ2sEDkAgjAAyxmwaARBgBYIg+IwAkwggAQ4yMAJAIIwAMsWYEgEQYAWAoFEbYTQN4G2EEgBnHOfwr0zSAtxFGAJihHTwAiTACwBDt4AFIhBEAhmgHD0AijAAwxMgIAIkwAsBQuM8IaQTwNMIIADOhB+UxTQN4G2EEgBnawQOQCCMADIWmadjaC3gbYQSAGRawApAIIwAMNYYR0gjgZYQRAGboMwJAIowAMBTeTcM8DeBphBEAZlgzAkAijAAwxDQNAIkwAsAQIyMAJMIIAEP0GQEgEUYAGGJkBIBEGAFgKBRGokkjgKcRRgCYcRrmaZimAbyNMALAjMNuGgAijAAwxJoRABJhBIAh+owAkAgjAAyFRkbIIoC3EUYAmAmGdtOQRgBPI4wAMMM0DQCJMALAkBN+aq9xIQBM8REAwAzt4AFIhBEAhtjaC0AijAAw5IbDCGkE8DLCCAAzdGAFIBFGABgKMjICQIQRAIZYMwJAIowAMBTuM0IaATyNMALATKjPCLM0gLcRRgCYYQErAIkwAsAQa0YASK0MI0uXLlVqaqpiY2OVkZGhsrKyFl23evVqRUVFafTo0a15WwCdjMOD8gCoFWGkuLhYeXl5KigoUHl5udLS0pSdna19+/Yd97qdO3fq7rvv1pVXXtnqYgF0LqEwQjt4wNsiDiMLFy7UxIkTlZOTo4suukhFRUXq3r27VqxYccxrgsGgxo4dq9mzZ+ucc845oYIBdB6sGQEgRRhG6uvrtXnzZmVlZTV+AZ9PWVlZKi0tPeZ1//M//6PExERNmDChRe9TV1enQCDQ5AWg83FZMwJAEYaR/fv3KxgMKikpqcnxpKQkVVZWNnvN22+/rSeeeELLly9v8fsUFhYqPj4+/EpJSYmkTAAdhOMc/pU+I4C3tetumgMHDuiWW27R8uXLlZCQ0OLr8vPzVVNTE37t3r27HasEYKVxzYhxIQBMdYnk5ISEBEVHR6uqqqrJ8aqqKvXq1euo8//xj39o586dGjVqVPiY0/BPoS5dumjbtm0699xzj7rO7/fL7/dHUhqADijIbhoAinBkJCYmRunp6SopKQkfcxxHJSUlyszMPOr8fv366aOPPlJFRUX49eMf/1hXX321KioqmH4BPM5lASsARTgyIkl5eXkaP368Bg0apMGDB2vRokWqra1VTk6OJGncuHFKTk5WYWGhYmNj1b9//ybXn3rqqZJ01HEA3sM0DQCpFWFkzJgxqq6u1qxZs1RZWakBAwZo/fr14UWtu3btks9HY1cA342tvQCkVoQRSZo0aZImTZrU7J9t3LjxuNeuXLmyNW8JoBOiHTwAiWfTADAU7jNCGgE8jTACwEzQoR08AMIIAEOhNSNs7QW8jTACwAzt4AFIhBEAhthNA0AijAAwRJ8RABJhBIAhRkYASIQRAIYcJ7S117gQAKb4CABgprHpGSMjgJcRRgCYIYwAkAgjAAyxZgSARBgBYIg+IwAkwggAQ6GREdrBA95GGAFghqf2ApAIIwAMhbb2RpNGAE8jjAAwwwJWABJhBIAh2sEDkAgjAAwxMgJAIowAMOTS9AyACCMADLGbBoBEGAFgKOiE1oyQRgAvI4wAMNMwMMLWXsDjCCMAzDBNA0AijAAwRDt4ABJhBIAhRkYASIQRAIZc+owAEGEEgKHQbhrCCOBthBEAZsLTNHwSAZ7GRwAAM7SDByARRgAYoh08AIkwAsAQu2kASIQRAIboMwJAIowAMOQ4jIwAIIwAMBSapuHZNIC3EUYAmGE3DQCJMALAUGhkhCwCeBthBIAZ2sEDkAgjAAw59BkBIMIIAENB+owAEGEEgBHXdRunaUgjgKcRRgCYCAURiWkawOsIIwBMOEekEQZGAG8jjAAw4RwxMkI7eMDbCCMATDAyAiCEMALABGtGAIQQRgCYCDYZGSGMAF5GGAFgosk0DZ9EgKfxEQDAhOs0/jcjI4C3EUYAmHCYpgHQgDACwAS7aQCEEEYAmKDPCIAQwggAEw4PyQPQgDACwEQojESTRgDPI4wAMBGapmGKBgBhBIAJx2GaBsBhhBEAJkKbadjWC6BVYWTp0qVKTU1VbGysMjIyVFZWdsxzly9friuvvFI9e/ZUz549lZWVddzzAXhD4wJWwgjgdRGHkeLiYuXl5amgoEDl5eVKS0tTdna29u3b1+z5Gzdu1E033aTXX39dpaWlSklJ0fDhw7Vnz54TLh5AxxV6Ng1ZBECU6x757MzvlpGRocsuu0xLliyRJDmOo5SUFN15552aNm3ad14fDAbVs2dPLVmyROPGjWvRewYCAcXHx6umpkZxcXGRlAvge+qzfQeUtfBNndq9qypmDbcuB0A7aOnP74hGRurr67V582ZlZWU1fgGfT1lZWSotLW3R1zh48KAOHTqk00477Zjn1NXVKRAINHkB6Fwc1owAaBBRGNm/f7+CwaCSkpKaHE9KSlJlZWWLvsbUqVPVp0+fJoHm/yosLFR8fHz4lZKSEkmZADoAmp4BCDmpu2nuv/9+rV69Wi+++KJiY2OPeV5+fr5qamrCr927d5/EKgGcDE7DU3vpMwKgSyQnJyQkKDo6WlVVVU2OV1VVqVevXse99qGHHtL999+v1157TZdccslxz/X7/fL7/ZGUBqCDYWQEQEhEIyMxMTFKT09XSUlJ+JjjOCopKVFmZuYxr3vwwQc1Z84crV+/XoMGDWp9tQA6Dbb2AgiJaGREkvLy8jR+/HgNGjRIgwcP1qJFi1RbW6ucnBxJ0rhx45ScnKzCwkJJ0gMPPKBZs2Zp1apVSk1NDa8tOeWUU3TKKae04a0A6EhYwAogJOIwMmbMGFVXV2vWrFmqrKzUgAEDtH79+vCi1l27dsnnaxxwWbZsmerr6/Xzn/+8ydcpKCjQfffdd2LVA+iwwiMj9IEGPC/iMCJJkyZN0qRJk5r9s40bNzb5/c6dO1vzFgA6OZdpGgAN+DcJABNM0wAIIYwAMBF6ai9ZBABhBICJINM0ABoQRgCYcMPTNLZ1ALBHGAFggj4jAEIIIwBMsIAVQAhhBIAJ+owACOFjAIAJ+owACCGMADDBU3sBhBBGAJgI8tReAA0IIwBMhKZpohkZATyPMALABLtpAIQQRgCYCO2mIYsAIIwAMMHICIAQwggAEy59RgA04GMAgImgQ58RAIcRRgCYCE3T0GcEAGEEgAknvLXXuBAA5ggjAEzQDh5ACGEEgAmmaQCEEEYAmHBoBw+gAWEEgAmH3TQAGhBGAJgINz3jUwjwPD4GAJhwWMAKoAFhBIAJ2sEDCCGMADDhsoAVQAPCCAATTNMACCGMADARdA7/Sp8RAIQRACboMwIghDACwERozUg0aQTwPMIIABO0gwcQQhgBYIJpGgAhhBEAJugzAiCEMALAROOzaYwLAWCOMALARGiahjUjAAgjAEwwTQMghDACwETj1l7jQgCY42MAgAnawQMIIYwAMEGfEQAhhBEAJugzAiCEMALAROPWXtII4HWEEQAmGnfT2NYBwB5hBICJ8DQNaQTwPMIIABP0GQEQQhgBYMJlASuABoQRACZoBw8ghDACwETQOfwr0zQACCMATDBNAyCEMALAhBN+Ng1pBPA6wggAE7SDBxBCGAFggnbwAEIIIwBMuPQZAdCAMALARNBhZATAYYQRACboMwIghDACwATt4AGEEEYAmHDDW3uNCwFgrlUfA0uXLlVqaqpiY2OVkZGhsrKy457/3HPPqV+/foqNjdXFF1+sdevWtapYAJ0H0zQAQiIOI8XFxcrLy1NBQYHKy8uVlpam7Oxs7du3r9nz3333Xd10002aMGGCPvjgA40ePVqjR4/Wli1bTrh4AB0X0zQAQiIOIwsXLtTEiROVk5Ojiy66SEVFRerevbtWrFjR7Pm///3vdd111+l3v/udLrzwQs2ZM0eXXnqplixZcsLFA+i46DMCIKRLJCfX19dr8+bNys/PDx/z+XzKyspSaWlps9eUlpYqLy+vybHs7GytWbPmmO9TV1enurq68O8DgUAkZbbYE29/ri++PtguXxvA8f1j378lMTICIMIwsn//fgWDQSUlJTU5npSUpK1btzZ7TWVlZbPnV1ZWHvN9CgsLNXv27EhKa5W1/7tX5bu+aff3AXBscd0i+hgC0Al9Lz8F8vPzm4ymBAIBpaSktPn7/Cz9TGWee3qbf10ALZPYI1ZXnn+GdRkAjEUURhISEhQdHa2qqqomx6uqqtSrV69mr+nVq1dE50uS3++X3++PpLRWGZtxdru/BwAAOL6IFrDGxMQoPT1dJSUl4WOO46ikpESZmZnNXpOZmdnkfEnasGHDMc8HAADeEvE0TV5ensaPH69BgwZp8ODBWrRokWpra5WTkyNJGjdunJKTk1VYWChJmjx5sq666iotWLBAI0eO1OrVq/X+++/rsccea9s7AQAAHVLEYWTMmDGqrq7WrFmzVFlZqQEDBmj9+vXhRaq7du2Sz9c44HLFFVdo1apVuvfeezV9+nSdf/75WrNmjfr37992dwEAADqsKDfUk/l7LBAIKD4+XjU1NYqLi7MuBwAAtEBLf37zVAgAAGCKMAIAAEwRRgAAgCnCCAAAMEUYAQAApggjAADAFGEEAACYIowAAABThBEAAGAq4nbwFkJNYgOBgHElAACgpUI/t7+r2XuHCCMHDhyQJKWkpBhXAgAAInXgwAHFx8cf8887xLNpHMfR3r171aNHD0VFRbXZ1w0EAkpJSdHu3bs77TNvuMeOr7Pfn8Q9dhad/R47+/1JbX+PruvqwIED6tOnT5OH6P5fHWJkxOfz6cwzz2y3rx8XF9dp/8cK4R47vs5+fxL32Fl09nvs7Pcnte09Hm9EJIQFrAAAwBRhBAAAmPJ0GPH7/SooKJDf77cupd1wjx1fZ78/iXvsLDr7PXb2+5Ps7rFDLGAFAACdl6dHRgAAgD3CCAAAMEUYAQAApggjAADAFGGkwfbt2/WTn/xECQkJiouL09ChQ/X6669bl9Xm1q5dq4yMDHXr1k09e/bU6NGjrUtqF3V1dRowYICioqJUUVFhXU6b2blzpyZMmKC+ffuqW7duOvfcc1VQUKD6+nrr0k7I0qVLlZqaqtjYWGVkZKisrMy6pDZRWFioyy67TD169FBiYqJGjx6tbdu2WZfVru6//35FRUVpypQp1qW0qT179uiXv/ylTj/9dHXr1k0XX3yx3n//feuy2kwwGNTMmTObfLbMmTPnO58p01YIIw2uv/56ffvtt/rb3/6mzZs3Ky0tTddff70qKyutS2szf/7zn3XLLbcoJydHH374od555x3dfPPN1mW1i3vuuUd9+vSxLqPNbd26VY7j6NFHH9XHH3+shx9+WEVFRZo+fbp1aa1WXFysvLw8FRQUqLy8XGlpacrOzta+ffusSzthb7zxhnJzc/Xee+9pw4YNOnTokIYPH67a2lrr0trFpk2b9Oijj+qSSy6xLqVNff311xoyZIi6du2qv/71r/rkk0+0YMEC9ezZ07q0NvPAAw9o2bJlWrJkiT799FM98MADevDBB7V48eKTU4ALt7q62pXkvvnmm+FjgUDAleRu2LDBsLK2c+jQITc5Odl9/PHHrUtpd+vWrXP79evnfvzxx64k94MPPrAuqV09+OCDbt++fa3LaLXBgwe7ubm54d8Hg0G3T58+bmFhoWFV7WPfvn2uJPeNN96wLqXNHThwwD3//PPdDRs2uFdddZU7efJk65LazNSpU92hQ4dal9GuRo4c6f76179ucuyGG25wx44de1Len5ERSaeffrouuOAC/fGPf1Rtba2+/fZbPfroo0pMTFR6erp1eW2ivLxce/bskc/n08CBA9W7d2+NGDFCW7ZssS6tTVVVVWnixIn605/+pO7du1uXc1LU1NTotNNOsy6jVerr67V582ZlZWWFj/l8PmVlZam0tNSwsvZRU1MjSR327+t4cnNzNXLkyCZ/l53Fyy+/rEGDBunGG29UYmKiBg4cqOXLl1uX1aauuOIKlZSUaPv27ZKkDz/8UG+//bZGjBhxUt6/Qzwor71FRUXptdde0+jRo9WjRw/5fD4lJiZq/fr1nWYYbseOHZKk++67TwsXLlRqaqoWLFigYcOGafv27Z3iw9F1Xf3qV7/S7bffrkGDBmnnzp3WJbW7zz77TIsXL9ZDDz1kXUqr7N+/X8FgUElJSU2OJyUlaevWrUZVtQ/HcTRlyhQNGTJE/fv3ty6nTa1evVrl5eXatGmTdSntYseOHVq2bJny8vI0ffp0bdq0Sb/5zW8UExOj8ePHW5fXJqZNm6ZAIKB+/fopOjpawWBQ8+bN09ixY0/K+3fqkZFp06YpKirquK+tW7fKdV3l5uYqMTFRb731lsrKyjR69GiNGjVKX375pfVtHFdL79FxHEnSjBkz9LOf/Uzp6el68sknFRUVpeeee874Lo6vpfe4ePFiHThwQPn5+dYlR6yl93ikPXv26LrrrtONN96oiRMnGlWOlsrNzdWWLVu0evVq61La1O7duzV58mQ988wzio2NtS6nXTiOo0svvVTz58/XwIEDddttt2nixIkqKiqyLq3NPPvss3rmmWe0atUqlZeX66mnntJDDz2kp5566qS8f6duB19dXa1//etfxz3nnHPO0VtvvaXhw4fr66+/bvLI5PPPP18TJkzQtGnT2rvUVmvpPb7zzjv64Q9/qLfeektDhw4N/1lGRoaysrI0b9689i611Vp6j7/4xS/0yiuvKCoqKnw8GAwqOjpaY8eOPWnfVK3R0nuMiYmRJO3du1fDhg3T5ZdfrpUrV8rn65j/rqivr1f37t31/PPPN9nZNX78eH3zzTd66aWX7IprQ5MmTdJLL72kN998U3379rUup02tWbNGP/3pTxUdHR0+FgwGFRUVJZ/Pp7q6uiZ/1hGdffbZuvbaa/X444+Hjy1btkxz587Vnj17DCtrOykpKZo2bZpyc3PDx+bOnaunn376pIxSduppmjPOOENnnHHGd5538OBBSTrqA93n84VHFL6vWnqP6enp8vv92rZtWziMHDp0SDt37tTZZ5/d3mWekJbe4x/+8AfNnTs3/Pu9e/cqOztbxcXFysjIaM8ST1hL71E6PCJy9dVXh0e3OmoQkaSYmBilp6erpKQkHEYcx1FJSYkmTZpkW1wbcF1Xd955p1588UVt3Lix0wURSbrmmmv00UcfNTmWk5Ojfv36aerUqR0+iEjSkCFDjtqSvX379u/9Z2ckDh48eNRnSXR09Mn7GXhSlsl+z1VXV7unn366e8MNN7gVFRXutm3b3Lvvvtvt2rWrW1FRYV1em5k8ebKbnJzsvvrqq+7WrVvdCRMmuImJie5XX31lXVq7+PzzzzvdbpovvvjCPe+889xrrrnG/eKLL9wvv/wy/OqoVq9e7fr9fnflypXuJ5984t52223uqaee6lZWVlqXdsLuuOMONz4+3t24cWOTv6uDBw9al9auOttumrKyMrdLly7uvHnz3L///e/uM88843bv3t19+umnrUtrM+PHj3eTk5Pdv/zlL+7nn3/uvvDCC25CQoJ7zz33nJT3J4w02LRpkzt8+HD3tNNOc3v06OFefvnl7rp166zLalP19fXub3/7WzcxMdHt0aOHm5WV5W7ZssW6rHbTGcPIk08+6Upq9tWRLV682D3rrLPcmJgYd/Dgwe57771nXVKbONbf1ZNPPmldWrvqbGHEdV33lVdecfv37+/6/X63X79+7mOPPWZdUpsKBALu5MmT3bPOOsuNjY11zznnHHfGjBluXV3dSXn/Tr1mBAAAfP913MlmAADQKRBGAACAKcIIAAAwRRgBAACmCCMAAMAUYQQAAJgijAAAAFOEEQAAYIowAgAATBFGAACAKcIIAAAwRRgBAACm/j9/JotH1f3t5QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.backward(torch.ones_like(x), retain_graph=True)\n",
    "plt.plot(x.detach(), x.grad); # gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjE99U0Z4L9W"
   },
   "source": [
    "#### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0nKQRYp4L9W"
   },
   "source": [
    "* 主要重點：  \n",
    "  * $sigmoid(x) = \\frac{1}{1 + exp(-x)}$  \n",
    "  * $\\frac{x}{dx} sigmoid(x) = sigmoid(x)(1-sigmoid(x))$  \n",
    "  * 從導數的性質，可以發現，gradient 在 x 靠近 0 時，值較大 (參數更新較快）， x 遠離 0 時， gradient 趨近於 0 (參數停止更新)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "AR2EYWFp4L9W",
    "outputId": "b4b069b9-a137-4388-a423-b7ea0df96a0e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation = nn.Sigmoid()\n",
    "\n",
    "x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n",
    "y = activation(x)\n",
    "\n",
    "plt.plot(x.detach(), y.detach());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Y2hmDuMp4L9W",
    "outputId": "ab61bc77-67ac-46da-8fa6-849e01948188",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# gradients\n",
    "y.backward(torch.ones_like(x),retain_graph=True)\n",
    "plt.plot(x.detach(), x.grad);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaIjNfPs4L9X"
   },
   "source": [
    "### 自訂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbqtPqly4L9X"
   },
   "source": [
    "* 直接定義一個 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "QTu1bBmA4L9X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    a = torch.zeros_like(x) # shape 會與 x 一樣\n",
    "    return torch.max(x, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7Tgshco4L9Y",
    "tags": []
   },
   "source": [
    "## custom layers & block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/layer_block_model.jpg\" width=600 alt=\"layer_block_model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1faUIXm14L9Z"
   },
   "source": [
    "* 幾個名詞定義一下：  \n",
    "  * layer: 只要是 input n 個 neruon, output m 個 neuron 的 function，就被稱為一個 layer。例如 `nn.Linear(in_dim, out_dim)` 就是個 linear layer. \n",
    "  * block: \n",
    "    * 多個 layer 組合在一起，稱為一個 block。例如一個 VGG block，就是由數個 conv, pooling layer 所組成. \n",
    "    * 通常用 sequential 來把 layer 組成 block; 或用 sub-class 來把 layer 組成 block\n",
    "  * model: \n",
    "    * 由 layers or/and blocks 組起來，只要 input 是 feature/images/sentences..，output 是 回歸/分類...結果，就都可稱為 model。\n",
    "    * 例如一個 linear layer 可以是 model (e.g. linear regression)，一個 block 可以是 model (e.g. 多層感知機)，多個 block 組在一起 (e.g. resnet) 也可以是 model  \n",
    "    * 所以，可以用 `layer` 來做出 model，也可以用 `sequential` 組成 model，也可以用 `sub-class` 組成 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5_u2lgB4L9Z"
   },
   "source": [
    "### custom layer (不帶參數)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "7uZhCvT14L9Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CenteredLayer(nn.Module):\n",
    "    \"\"\"用來做中心化(去平均)的layer\n",
    "    args:\n",
    "      X: 任何 shape，但通常是 (n, p)，然後我們想把 feature 都 de-mean\n",
    "    \"\"\"\n",
    "    def __init__(self, dim = 0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean(dim = self.dim, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "B1_nvttC4L9Z",
    "outputId": "9a1a91b7-8e9c-454b-bd87-984af7ac0aa6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1121,  1.7709],\n",
       "        [-1.0068, -1.2362],\n",
       "        [ 0.0348,  1.1227],\n",
       "        [-0.2837, -1.2738],\n",
       "        [ 1.2561, -0.0931]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 做 5 個 sample，每個 sample 都有 2 個 feature 的 X\n",
    "X = torch.randn(5, 2)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "0aYFCEdC4L9a",
    "outputId": "2ea46d1f-ddf2-4537-8ffc-eba72f293296",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8898,  1.7128],\n",
       "        [-0.7844, -1.2943],\n",
       "        [ 0.2571,  1.0646],\n",
       "        [-0.0613, -1.3318],\n",
       "        [ 1.4784, -0.1512]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_Hoivp54L9a"
   },
   "source": [
    "* 可以清楚看到，de-mean 後，每個 col 現在相加都是 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twthuUnS4L9a"
   },
   "source": [
    "* 之後，這種 layer 就可以當作前處理，然後這樣用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "9P_Xs_tr4L9a",
    "outputId": "a1968274-aacd-460c-f64c-007230258e59",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6427],\n",
       "        [-0.0380],\n",
       "        [-0.6522],\n",
       "        [-0.1202],\n",
       "        [-0.5543]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    CenteredLayer(), # 前處理用，de-mean\n",
    "    nn.Linear(2, 1) # linear regression\n",
    ")\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmQ574D14L9b"
   },
   "source": [
    "### custom layer (帶參數)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4DdqIQZ4L9b"
   },
   "source": [
    "* 重點在，weight, bias 要用 `nn.Parameter()` 來造，這樣就可以保有計算 gradient 等功能(預設 requires_grad = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "8boW830X4L9b",
    "outputId": "21e84c3d-38b5-41a0-bd21-6df4a9f15194",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m  \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A kind of Tensor that is to be considered a module parameter.\n",
       "\n",
       "Parameters are :class:`~torch.Tensor` subclasses, that have a\n",
       "very special property when used with :class:`Module` s - when they're\n",
       "assigned as Module attributes they are automatically added to the list of\n",
       "its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.\n",
       "Assigning a Tensor doesn't have such effect. This is because one might\n",
       "want to cache some temporary state, like last hidden state of the RNN, in\n",
       "the model. If there was no such class as :class:`Parameter`, these\n",
       "temporaries would get registered too.\n",
       "\n",
       "Args:\n",
       "    data (Tensor): parameter tensor.\n",
       "    requires_grad (bool, optional): if the parameter requires gradient. See\n",
       "        :ref:`locally-disable-grad-doc` for more details. Default: `True`\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/parameter.py\n",
       "\u001b[0;31mType:\u001b[0m           _ParameterMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     UninitializedParameter"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "1Tyert_G4L9b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \"\"\" 自己寫一個 dense 層 \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_dim, out_dim))\n",
    "        self.bias = nn.Parameter(torch.randn(out_dim,))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zauuux3x4L9c"
   },
   "source": [
    "* 看一下實例化後，起始參數："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "zY4Pagsl4L9c",
    "outputId": "d2d9d0aa-ab17-4458-b51f-84ca15df2029",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.8718,  0.9469,  0.1916],\n",
       "        [ 1.6973,  1.5636, -0.3113],\n",
       "        [ 0.7594, -1.1440, -1.7244],\n",
       "        [-0.7467,  0.9555,  0.9014],\n",
       "        [-0.5161,  0.7031, -1.6652]], requires_grad=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVAVAte44L9c"
   },
   "source": [
    "* 用用看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "9vLMgK9R4L9c",
    "outputId": "71b1a31d-6df3-4079-8f34-38852309aacb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.2076,  1.0059, -5.5664],\n",
       "        [ 2.2081,  3.5985, -4.8784],\n",
       "        [ 1.4460,  2.0264,  0.4752],\n",
       "        [-0.4749, -1.9205, -2.2298],\n",
       "        [ 1.1014, -0.4691, -3.6861],\n",
       "        [-2.3040,  1.4275, -0.5245],\n",
       "        [ 1.8266,  1.2462, -3.5570],\n",
       "        [ 1.2729,  2.0576, -4.6761],\n",
       "        [-1.4266, -0.1284, -0.0855],\n",
       "        [ 0.2521, -1.2549,  1.0474]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(10, 5)\n",
    "linear(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hgf_RZL4L9d"
   },
   "source": [
    "### sequential block (`nn.Sequential(layer1, block2, ...)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1119,  0.0200,  0.2404,  0.2004, -0.0767, -0.1386,  0.1669, -0.1070,\n",
       "          0.0494, -0.1204],\n",
       "        [-0.1727,  0.0096,  0.1246,  0.2796, -0.0674, -0.1257,  0.0676,  0.0165,\n",
       "          0.1145, -0.0782]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(20, 256), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequential `for` tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我們可以建立一個自己的 sequential，就可以看到實際運作狀況："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # args 就是 user 隨意要丟幾個 layer, block 進來，所組成的 list\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 來試試看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1936,  0.0687, -0.0650,  0.2737, -0.0451, -0.0563,  0.0344, -0.1300,\n",
       "          0.0651, -0.0686],\n",
       "        [ 0.1334,  0.1245,  0.0674,  0.3944, -0.0648, -0.3130,  0.0865, -0.2485,\n",
       "          0.0543, -0.1893]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(\n",
    "    nn.Linear(20, 256), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir7Npw2n4L9c",
    "tags": []
   },
   "source": [
    "### custom block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 雖然 sequential block 很方便，但有時我們會需要在 forward 的時候，做一些靈活的控制，例如以下這個刻意做出來的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "rPfKY2I44L9d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.hidden(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.out(X)\n",
    "        # 這邊開始是 flexible 的設計, 這就是 sequential 辦不到的\n",
    "        # 我希望控制輸出，當輸出的 tensor 的 L1 norm > 1 時，我就把他除以2，直到輸出的 L1 norm 壓在 1 以內\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 來試一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0175, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 20)\n",
    "\n",
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 經典 model 自己寫系列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 經典的 VGG，就是用了 VGG block 的概念. \n",
    "* 一個 VGG block，是由以下兩個 component 組成：  \n",
    "  * k 個 3x3 的 conv2d + ReLU\n",
    "    * k 通常是 1 or 2 而已\n",
    "    * same padding: stride = 1, padding = 1\n",
    "    * 輸出通道數是 hyperparameter，一般都從 64 開始，一路翻倍上去 (64->128->256->512). \n",
    "  * 1 個 2x2 max_pooling, stride = 2, 做到高寬減半\n",
    "* 來看一個 VGG block 的實作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels,\n",
    "                                kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    final_module = nn.Sequential(*layers)\n",
    "    return final_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 例如第一個 block，我想要 1個 conv2，然後 block 的最終 output channel 是 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_block(num_convs = 1, in_channels = 3, out_channels = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 第二個 block，我想要 2 個 conv2d, 然後 block 的最終 output channel 是 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_block(num_convs = 2, in_channels = 64, out_channels = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 讓 VGG block 疊高高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VGG 當初的特色，就是當我定義好 VGG block 後，我可以一路疊高高，想疊多少 block 就疊多少 block. \n",
    "* 所以，以下定義一個 function，來把 VGG 疊高高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vgg_feature_extraction(conv_arch, input_img_channels = 3):\n",
    "    \"\"\"\n",
    "    args:\n",
    "      - img_channels: number_channel of input image \n",
    "      - conv_arch: list of tuples (num_convs, out_channels), e.g. [(1,64),(1,128),...,(2,512)]\n",
    "    \"\"\"\n",
    "    conv_blks_dict = OrderedDict()\n",
    "    in_channels = input_img_channels\n",
    "    \n",
    "    for idx, (num_convs, out_channels) in enumerate(conv_arch):\n",
    "        conv_blks_dict[f\"block{idx}\"] = vgg_block(num_convs, in_channels, out_channels)\n",
    "        in_channels = out_channels\n",
    "    \n",
    "    feature_module = nn.Sequential(conv_blks_dict)\n",
    "    return feature_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "feature_module = vgg_feature_extraction(conv_arch = conv_arch, input_img_channels = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (block0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 看一下，如果餵進去 shape = (1, 3,224,224) 的資料 (batch_size = 1, channel = 3, height=width=224)，各 layer 的 shape 會變怎樣："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 512, 7, 7]            --\n",
       "├─Sequential: 1-1                        [1, 64, 112, 112]         --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
       "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
       "│    └─MaxPool2d: 2-3                    [1, 64, 112, 112]         --\n",
       "├─Sequential: 1-2                        [1, 128, 56, 56]          --\n",
       "│    └─Conv2d: 2-4                       [1, 128, 112, 112]        73,856\n",
       "│    └─ReLU: 2-5                         [1, 128, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-6                    [1, 128, 56, 56]          --\n",
       "├─Sequential: 1-3                        [1, 256, 28, 28]          --\n",
       "│    └─Conv2d: 2-7                       [1, 256, 56, 56]          295,168\n",
       "│    └─ReLU: 2-8                         [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-9                       [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-10                        [1, 256, 56, 56]          --\n",
       "│    └─MaxPool2d: 2-11                   [1, 256, 28, 28]          --\n",
       "├─Sequential: 1-4                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-12                      [1, 512, 28, 28]          1,180,160\n",
       "│    └─ReLU: 2-13                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-14                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-15                        [1, 512, 28, 28]          --\n",
       "│    └─MaxPool2d: 2-16                   [1, 512, 14, 14]          --\n",
       "├─Sequential: 1-5                        [1, 512, 7, 7]            --\n",
       "│    └─Conv2d: 2-17                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-18                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-19                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-20                        [1, 512, 14, 14]          --\n",
       "│    └─MaxPool2d: 2-21                   [1, 512, 7, 7]            --\n",
       "==========================================================================================\n",
       "Total params: 9,220,480\n",
       "Trainable params: 9,220,480\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 7.49\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 59.41\n",
       "Params size (MB): 36.88\n",
       "Estimated Total Size (MB): 96.89\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(feature_module, (1,3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，最後的 feature map 是 (512, 7, 7) 的 shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### flatten layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 這邊要把最後的 feature map，拉直成向量，好餵入最後的全連階層做預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flatten_module = nn.Flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### classifier layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* classifier 就比較單純了，由一系列的 dense layer + relu + drop out 組成\n",
    "* 最終的 class 數量是 100  \n",
    "* 直接寫："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier_module = nn.Sequential(\n",
    "    nn.Linear(512*7*7, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 全部組起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_vgg(conv_arch, input_img_channels):\n",
    "    feature_module = vgg_feature_extraction(conv_arch, input_img_channels)\n",
    "    flatten_module = nn.Flatten()\n",
    "    classifier_module = nn.Sequential(\n",
    "        nn.Linear(512*7*7, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 1000)\n",
    "    )\n",
    "    \n",
    "    final_model_dict = OrderedDict()\n",
    "    final_model_dict[\"feature\"] = feature_module\n",
    "    final_model_dict[\"flatten\"] = flatten_module\n",
    "    final_model_dict[\"classifier\"] = classifier_module\n",
    "    final_model = nn.Sequential(final_model_dict)\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "my_vgg = custom_vgg(conv_arch, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (feature): Sequential(\n",
       "    (block0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block1): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block3): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block4): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 對照一下 pretrained 的 vgg，會發現結構一模一樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import vgg11\n",
    "vgg = vgg11(weights=\"IMAGENET1K_V1\")\n",
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 1000]                 --\n",
       "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
       "│    └─Sequential: 2-1                   [1, 64, 112, 112]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 224, 224]         1,792\n",
       "│    │    └─ReLU: 3-2                    [1, 64, 224, 224]         --\n",
       "│    │    └─MaxPool2d: 3-3               [1, 64, 112, 112]         --\n",
       "│    └─Sequential: 2-2                   [1, 128, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-4                  [1, 128, 112, 112]        73,856\n",
       "│    │    └─ReLU: 3-5                    [1, 128, 112, 112]        --\n",
       "│    │    └─MaxPool2d: 3-6               [1, 128, 56, 56]          --\n",
       "│    └─Sequential: 2-3                   [1, 256, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-7                  [1, 256, 56, 56]          295,168\n",
       "│    │    └─ReLU: 3-8                    [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-9                  [1, 256, 56, 56]          590,080\n",
       "│    │    └─ReLU: 3-10                   [1, 256, 56, 56]          --\n",
       "│    │    └─MaxPool2d: 3-11              [1, 256, 28, 28]          --\n",
       "│    └─Sequential: 2-4                   [1, 512, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-12                 [1, 512, 28, 28]          1,180,160\n",
       "│    │    └─ReLU: 3-13                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-14                 [1, 512, 28, 28]          2,359,808\n",
       "│    │    └─ReLU: 3-15                   [1, 512, 28, 28]          --\n",
       "│    │    └─MaxPool2d: 3-16              [1, 512, 14, 14]          --\n",
       "│    └─Sequential: 2-5                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-17                 [1, 512, 14, 14]          2,359,808\n",
       "│    │    └─ReLU: 3-18                   [1, 512, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-19                 [1, 512, 14, 14]          2,359,808\n",
       "│    │    └─ReLU: 3-20                   [1, 512, 14, 14]          --\n",
       "│    │    └─MaxPool2d: 3-21              [1, 512, 7, 7]            --\n",
       "├─Flatten: 1-2                           [1, 25088]                --\n",
       "├─Sequential: 1-3                        [1, 1000]                 --\n",
       "│    └─Linear: 2-6                       [1, 4096]                 102,764,544\n",
       "│    └─ReLU: 2-7                         [1, 4096]                 --\n",
       "│    └─Dropout: 2-8                      [1, 4096]                 --\n",
       "│    └─Linear: 2-9                       [1, 4096]                 16,781,312\n",
       "│    └─ReLU: 2-10                        [1, 4096]                 --\n",
       "│    └─Dropout: 2-11                     [1, 4096]                 --\n",
       "│    └─Linear: 2-12                      [1, 1000]                 4,097,000\n",
       "==========================================================================================\n",
       "Total params: 132,863,336\n",
       "Trainable params: 132,863,336\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 7.62\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 59.48\n",
       "Params size (MB): 531.45\n",
       "Estimated Total Size (MB): 591.54\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(my_vgg, (1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Resnet18 的網路架構如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/resnet18-90.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 從這張架構圖，可以得知：  \n",
    "  * img 剛進來時，先經過 7x7 conv -> batch norm -> 3x3 max pooling，得到 feature map. \n",
    "  * `2x` 下面，看起來是一個 block，我先稱它為 `simple resnet block`.  \n",
    "  * `3x` 下面，看起來是2個 block 串再一起，前面是多出一個 1x1 conv，我稱它為 `1x1 conv resnet block`, 後面就是剛剛看過的 `simple resnet block`。所以這邊是 3 個 (1x1 conv resnet block + simple resnet block) 的結構  \n",
    "  * 最後用 global average pooling，得到這張圖的 embedding (512維而已，好清爽). \n",
    "  * 然後接個 fully connected layer, output 到 ImageNet 要預測的 1000 個類別\n",
    "* 現在來細看一下 `2x`, `3x` 下的兩種 resnet block。如下圖："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/resnet-block.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 右圖為啥要多一個 1x1 conv 呢？ 這是 shape 的考量：\n",
    "  * 左圖 (simple resnet block)：\n",
    "    * 一路的 conv, 都是通道數不變，高寬也不變(same padding)。\n",
    "    * 所以 input x 是 (b, c, h, w), 經過一系列的 conv 後，還是 (b, c, h, w)，就可以和原本的 input x 直接相加  \n",
    "  * 右圖 (1x1 conv resnet block)：\n",
    "    * 一路的 conv, 會刻意將通道數翻倍，高寬減半(stride = 2)。\n",
    "    * 所以 input x 是 (b, c, h, w), 經過一系列的 conv 後，變成 (b, 2c, h/2, w/2)，那就和原本的 x 的 shape不同。  \n",
    "    * 所以把 input 做 1x1 conv (input_channel = c, output_channel = 2c, kernel_size = 1, stride = 2, padding = 0), 就可以把 input x 也變成通道數翻倍, 高寬減半，就可以相加了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 這樣對 resnet 應該頗瞭解了。先來偷看一下 pytorch 的 pretrained resnet18, 等等就要自己造一個出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "resnet = resnet18(weights=\"IMAGENET1K_V1\")\n",
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nice，跟剛剛介紹的都一樣，唯一不樣的，是 `1x1 conv resnet block` 的部分，他除了做 1x1 conv 外，又多做了 batch normalization。但這無傷大雅。  \n",
    "* 另外，為啥他要把它命名為 downsample? 因為他用 1x1 conv 把原本的 feature map 高寬減半(stride = 2, padding = 0)，所以是在做高寬的 down sample。  \n",
    "* 現在應該信心滿滿了～ 來自己寫一個吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### resnet block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):  #@save\n",
    "    def __init__(self, input_channels, output_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, output_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.bn1 = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(output_channels, output_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output_channels)\n",
    "        if use_1x1conv:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels,kernel_size=1, stride=strides),\n",
    "                nn.BatchNorm2d(output_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # 內層\n",
    "        Y = self.conv1(X)\n",
    "        Y = self.bn1(Y)\n",
    "        Y = self.relu(Y)\n",
    "        Y = self.conv2(Y)\n",
    "        Y = self.bn2(Y)\n",
    "        \n",
    "        # 殘差連接層\n",
    "        if self.downsample:\n",
    "            X = self.downsample(X)\n",
    "        Y += X\n",
    "        \n",
    "        # output\n",
    "        Y = self.relu(Y)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Residual(\n",
       "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple resnet block\n",
    "blk = Residual(input_channels = 64, output_channels = 64, use_1x1conv = False)\n",
    "blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Residual(\n",
       "  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (downsample): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1x1 conv resnet block\n",
    "blk = Residual(input_channels = 64, output_channels = 128, use_1x1conv = True, strides = 2)\n",
    "blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### resnet network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 再看一次圖："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/resnet18-90.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我想這樣組織我的 code:  \n",
    "  * 前面這三層，我叫 b1 (block1). \n",
    "  * 再來的 `2x`，我叫 b2, 就用 sequential 接2次剛剛寫好的 `Residual` class 就好  \n",
    "  * 再來的 `3x`, 我想把 `conv resnet block`+`simple resnet block` 定義為 `compose_resnet_block`，然後分別用 b3, b4, b5 設定三組 block. \n",
    "  * 最後就接 global average pooling 和 fully connected layer 就搞定了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")\n",
    "\n",
    "b2 = nn.Sequential(\n",
    "    Residual(input_channels = 64, output_channels = 64, use_1x1conv = False),\n",
    "    Residual(input_channels = 64, output_channels = 64, use_1x1conv = False)\n",
    ")\n",
    "\n",
    "def compose_resnet_block(input_channels, output_channels):\n",
    "    out = nn.Sequential(\n",
    "        Residual(\n",
    "            input_channels = input_channels, \n",
    "            output_channels = output_channels, \n",
    "            use_1x1conv = True,\n",
    "            strides = 2\n",
    "        ),\n",
    "        Residual(\n",
    "            input_channels = output_channels, \n",
    "            output_channels = output_channels, \n",
    "            use_1x1conv = False,\n",
    "            strides = 1\n",
    "        )\n",
    "    )\n",
    "    return out\n",
    "\n",
    "b3 = compose_resnet_block(input_channels = 64, output_channels = 128)\n",
    "b4 = compose_resnet_block(input_channels = 128, output_channels = 256)\n",
    "b5 = compose_resnet_block(input_channels = 256, output_channels = 512)\n",
    "\n",
    "# 組起來吧\n",
    "model = nn.Sequential(\n",
    "    b1, \n",
    "    b2, \n",
    "    b3, b4, b5,\n",
    "    nn.AdaptiveAvgPool2d((1,1)),\n",
    "    nn.Flatten(), \n",
    "    nn.Linear(512, 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Residual(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Residual(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Residual(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Residual(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Residual(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Residual(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): Residual(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Residual(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 讚啦，跑一次看看:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = torch.rand(10, 3, 224, 224)\n",
    "out = model(input_img)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 手術"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 以下整理各種對 model structure 做手術的作法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 往後疊加 module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 用 `Seq_obj.append(module)` 來增加 module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 假設我現在已經定義好兩個 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_module = nn.Sequential(\n",
    "    nn.Linear(800, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU()\n",
    ")\n",
    "classifier_module = nn.Sequential(\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我可以建立一個空的 sequential 物件，然後把這些 module 給 append 進來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=800, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.append(feature_module)\n",
    "net.append(classifier_module)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以發現，這個 `.append()`，就和 keras 的 `.add()` 是一樣的意思"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 用 `Seq_obj.add_module(\"name\", module)` 來增加帶有名稱的module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 假設我現在已經定義好兩個 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_module = nn.Sequential(\n",
    "    nn.Linear(800, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU()\n",
    ")\n",
    "classifier_module = nn.Sequential(\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 然後，我的 model，要整合這兩個，那我可以這樣做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (feature): Sequential(\n",
       "    (0): Linear(in_features=800, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add_module(\"feature\", feature_module)\n",
    "net.add_module(\"classifier\", classifier_module)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (feature): Sequential(\n",
       "    (0): Linear(in_features=800, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Linear(in_features=800, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.append(feature_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 結構/參數管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 假設 model 長這樣："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 8), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    model.parameters(), lr=1e-4)\n",
    "\n",
    "# loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1426],\n",
      "        [0.1753]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# fake data: batch_size = 2\n",
    "X = torch.rand(size=(2, 4))\n",
    "y = torch.tensor([[0.7], [0.2]])\n",
    "\n",
    "# one-epoch training\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "y_hat = model(X)        # 把 x tensor 移到 GPU 計算\n",
    "print(y_hat)\n",
    "\n",
    "loss = criterion(y, y_hat) # 把 y tensor 移到 GPU 計算，\n",
    "                                      ##  y_hat 因為是從 GPU model input GPU Tensor 出來的\n",
    "                                      ##  所以不用再次 .to(device) 當然要也是沒差啦 =_=|||\n",
    "optim.zero_grad() # 把 trainable variable/weights/parameters 的 gradient 給 歸 0\n",
    "loss.backward() # 利用 loss，計算出每個 trainable variable/weights/parameters 所對應的 gradient\n",
    "optim.step() # 更新 trainable variable/weights/parameters 的值： parameters_new = parameters_old - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看 model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，他用 index 來表明每一層的 layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如果想看各層的 intput/output shape, 以及參數資訊，可以使用 `torchinfo.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [100, 1]                  --\n",
       "├─Linear: 1-1                            [100, 8]                  40\n",
       "├─ReLU: 1-2                              [100, 8]                  --\n",
       "├─Linear: 1-3                            [100, 1]                  9\n",
       "==========================================================================================\n",
       "Total params: 49\n",
       "Trainable params: 49\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (100, 4)) # (100,4) 是 input shape，我假設一個 batch_size 是 100，所以寫成 (100, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 看單一層的 weight, bias, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 例如，看第 0 個 index 的 參數："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.state_dict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],\n",
       "                      [ 0.4700,  0.4231, -0.2138,  0.2805],\n",
       "                      [-0.3597, -0.1527, -0.3627, -0.2027],\n",
       "                      [-0.3738, -0.0498, -0.0484, -0.1716],\n",
       "                      [-0.0631,  0.4257, -0.2368,  0.3549],\n",
       "                      [-0.4824, -0.3647, -0.3779,  0.1142],\n",
       "                      [ 0.3761, -0.4852,  0.2199, -0.3184],\n",
       "                      [-0.3334, -0.2057,  0.0335, -0.3001]], device='cuda:0')),\n",
       "             ('bias',\n",
       "              tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 所以，要取得 weight 或 bias 的資料可以這樣拿："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],\n",
       "        [ 0.4700,  0.4231, -0.2138,  0.2805],\n",
       "        [-0.3597, -0.1527, -0.3627, -0.2027],\n",
       "        [-0.3738, -0.0498, -0.0484, -0.1716],\n",
       "        [-0.0631,  0.4257, -0.2368,  0.3549],\n",
       "        [-0.4824, -0.3647, -0.3779,  0.1142],\n",
       "        [ 0.3761, -0.4852,  0.2199, -0.3184],\n",
       "        [-0.3334, -0.2057,  0.0335, -0.3001]], device='cuda:0')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].state_dict()['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.weight`, `.weight.data`, `.weight.grad`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 除了這種做法外，也可以用 `.weight` 取得 weight 物件，再往下去取得 data 和 gradient 資訊："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],\n",
      "        [ 0.4700,  0.4231, -0.2138,  0.2805],\n",
      "        [-0.3597, -0.1527, -0.3627, -0.2027],\n",
      "        [-0.3738, -0.0498, -0.0484, -0.1716],\n",
      "        [-0.0631,  0.4257, -0.2368,  0.3549],\n",
      "        [-0.4824, -0.3647, -0.3779,  0.1142],\n",
      "        [ 0.3761, -0.4852,  0.2199, -0.3184],\n",
      "        [-0.3334, -0.2057,  0.0335, -0.3001]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight) # 這是物件\n",
    "print(type(model[0].weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],\n",
       "        [ 0.4700,  0.4231, -0.2138,  0.2805],\n",
       "        [-0.3597, -0.1527, -0.3627, -0.2027],\n",
       "        [-0.3738, -0.0498, -0.0484, -0.1716],\n",
       "        [-0.0631,  0.4257, -0.2368,  0.3549],\n",
       "        [-0.4824, -0.3647, -0.3779,  0.1142],\n",
       "        [ 0.3761, -0.4852,  0.2199, -0.3184],\n",
       "        [-0.3334, -0.2057,  0.0335, -0.3001]], device='cuda:0')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取這個物件，底下的 data (i.e. value)\n",
    "model[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0074, -0.0270, -0.0137, -0.0392],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient\n",
    "model[0].weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.bias`, `.bias.data`, `.bias.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, -0.0590,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.parameters()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 也可以用 `.parameters`，出來的會是物件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f69c250d740>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],\n",
       "         [ 0.4700,  0.4231, -0.2138,  0.2805],\n",
       "         [-0.3597, -0.1527, -0.3627, -0.2027],\n",
       "         [-0.3738, -0.0498, -0.0484, -0.1716],\n",
       "         [-0.0631,  0.4257, -0.2368,  0.3549],\n",
       "         [-0.4824, -0.3647, -0.3779,  0.1142],\n",
       "         [ 0.3761, -0.4852,  0.2199, -0.3184],\n",
       "         [-0.3334, -0.2057,  0.0335, -0.3001]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],\n",
       "        device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model[0].parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，list 裡面的第一個 element，很明顯是 weight, 第二個 element，很明顯是 bias，兩個都是物件，所以真的要取資料時，可以這樣取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],\n",
      "        [ 0.4700,  0.4231, -0.2138,  0.2805],\n",
      "        [-0.3597, -0.1527, -0.3627, -0.2027],\n",
      "        [-0.3738, -0.0498, -0.0484, -0.1716],\n",
      "        [-0.0631,  0.4257, -0.2368,  0.3549],\n",
      "        [-0.4824, -0.3647, -0.3779,  0.1142],\n",
      "        [ 0.3761, -0.4852,  0.2199, -0.3184],\n",
      "        [-0.3334, -0.2057,  0.0335, -0.3001]], device='cuda:0')\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0074, -0.0270, -0.0137, -0.0392],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')\n",
      "1\n",
      "tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.0000, -0.0590,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for idx, param in enumerate(model[0].parameters()):\n",
    "    print(idx)\n",
    "    print(param.data)\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.named_parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],\n",
       "          [ 0.4700,  0.4231, -0.2138,  0.2805],\n",
       "          [-0.3597, -0.1527, -0.3627, -0.2027],\n",
       "          [-0.3738, -0.0498, -0.0484, -0.1716],\n",
       "          [-0.0631,  0.4257, -0.2368,  0.3549],\n",
       "          [-0.4824, -0.3647, -0.3779,  0.1142],\n",
       "          [ 0.3761, -0.4852,  0.2199, -0.3184],\n",
       "          [-0.3334, -0.2057,  0.0335, -0.3001]], device='cuda:0',\n",
       "         requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],\n",
       "         device='cuda:0', requires_grad=True))]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model[0].named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 看所有的 parameters, weight, bias, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 有了前面的練習，應該就不難理解一次看全部的結果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],\n",
       "         [ 0.4700,  0.4231, -0.2138,  0.2805],\n",
       "         [-0.3597, -0.1527, -0.3627, -0.2027],\n",
       "         [-0.3738, -0.0498, -0.0484, -0.1716],\n",
       "         [-0.0631,  0.4257, -0.2368,  0.3549],\n",
       "         [-0.4824, -0.3647, -0.3779,  0.1142],\n",
       "         [ 0.3761, -0.4852,  0.2199, -0.3184],\n",
       "         [-0.3334, -0.2057,  0.0335, -0.3001]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2124,  0.1014, -0.2084,  0.1536,  0.1722,  0.1967, -0.1082, -0.0799]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0692], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],\n",
       "                      [ 0.4700,  0.4231, -0.2138,  0.2805],\n",
       "                      [-0.3597, -0.1527, -0.3627, -0.2027],\n",
       "                      [-0.3738, -0.0498, -0.0484, -0.1716],\n",
       "                      [-0.0631,  0.4257, -0.2368,  0.3549],\n",
       "                      [-0.4824, -0.3647, -0.3779,  0.1142],\n",
       "                      [ 0.3761, -0.4852,  0.2199, -0.3184],\n",
       "                      [-0.3334, -0.2057,  0.0335, -0.3001]], device='cuda:0')),\n",
       "             ('0.bias',\n",
       "              tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],\n",
       "                     device='cuda:0')),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.2124,  0.1014, -0.2084,  0.1536,  0.1722,  0.1967, -0.1082, -0.0799]],\n",
       "                     device='cuda:0')),\n",
       "             ('2.bias', tensor([0.0692], device='cuda:0'))])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到 weight 和 bias 前面，有加上 index (i.e. 0 和 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x7f69c250dac0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.4152,  0.0776, -0.4084, -0.1533],\n",
       "          [ 0.4700,  0.4231, -0.2138,  0.2805],\n",
       "          [-0.3597, -0.1527, -0.3627, -0.2027],\n",
       "          [-0.3738, -0.0498, -0.0484, -0.1716],\n",
       "          [-0.0631,  0.4257, -0.2368,  0.3549],\n",
       "          [-0.4824, -0.3647, -0.3779,  0.1142],\n",
       "          [ 0.3761, -0.4852,  0.2199, -0.3184],\n",
       "          [-0.3334, -0.2057,  0.0335, -0.3001]], device='cuda:0',\n",
       "         requires_grad=True)),\n",
       " ('0.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.4566,  0.3500,  0.2779, -0.0486, -0.4647, -0.0405, -0.1213,  0.2383],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.2124,  0.1014, -0.2084,  0.1536,  0.1722,  0.1967, -0.1082, -0.0799]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.0692], device='cuda:0', requires_grad=True))]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### block factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如果我們把 block 給 nested 在一起，那要如何做參數管理？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0930, 0.4586, 0.2298, 0.6663],\n",
       "        [0.8720, 0.4310, 0.2996, 0.6085]], device='cuda:0')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2688],\n",
       "        [-0.2687]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    \n",
    "    out = nn.Sequential(\n",
    "        nn.Linear(4, 8), \n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, 4), \n",
    "        nn.ReLU()\n",
    "    )\n",
    "    \n",
    "    return out \n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在這裡 nested\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 那要取資訊時，就是一層一層往下取就好："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4000, -0.2344, -0.1909, -0.0215,  0.2354,  0.1416,  0.0388,  0.4069])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參數初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 知道如何訪問參數後，現在來講如何初始化參數. \n",
    "* 這要用到 pytorch 的 `nn.init` module 提供的多種方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0069,  0.0011, -0.0006, -0.0160],\n",
       "         [ 0.0032, -0.0044, -0.0067,  0.0041],\n",
       "         [ 0.0032,  0.0114, -0.0113,  0.0095],\n",
       "         [-0.0025,  0.0135, -0.0093,  0.0144],\n",
       "         [ 0.0006, -0.0119, -0.0035,  0.0295],\n",
       "         [-0.0087, -0.0041, -0.0280, -0.0009],\n",
       "         [-0.0034, -0.0170, -0.0034, -0.0019],\n",
       "         [-0.0054,  0.0019,  0.0085, -0.0161]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight 和 bias 的初始值都設為 N(0, 0.01) 的 init\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "        \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 8), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "model.apply(init_normal)\n",
    "model[0].weight.data, model[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight 的 初始值都設為 1, bias 都設為 0\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(init_constant)\n",
    "\n",
    "model[0].weight.data, model[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.5764,  0.2257,  0.2232, -0.3147],\n",
       "         [ 0.0899, -0.4019, -0.2921, -0.4732],\n",
       "         [ 0.4910,  0.1927,  0.1129, -0.7005],\n",
       "         [-0.2115,  0.6684,  0.2473,  0.4178],\n",
       "         [-0.3668,  0.1869, -0.1139,  0.0763],\n",
       "         [ 0.5566, -0.4967,  0.3012,  0.1808],\n",
       "         [-0.1479, -0.0274,  0.2649,  0.2588],\n",
       "         [-0.0047,  0.4642,  0.2031, -0.4086]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "model.apply(init_xavier)\n",
    "\n",
    "model[0].weight.data, model[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000, -0.0000, -0.0000],\n",
       "        [-9.4019,  0.0000, -6.3883, -9.9278],\n",
       "        [ 6.4976,  6.2993, -0.0000, -0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [-0.0000,  0.0000,  5.7446, -0.0000],\n",
       "        [-7.2997,  0.0000, -6.1244,  0.0000],\n",
       "        [ 6.6610,  0.0000, -0.0000, -0.0000],\n",
       "        [-0.0000,  0.0000, -8.7791, -5.9247]], requires_grad=True)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自訂義初始化\n",
    "# weight 有 1/4 的可能性，來自 U(5, 10), 1/4 可能性來自 U(-10, -5), 1/2 可能性是 0\n",
    "\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape) for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "model.apply(my_init)\n",
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我們也可以自己設定參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight.data[:] += 1\n",
    "model[0].weight.data[0, 0] = 42\n",
    "model[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 以下的 pretrained model，都是用 ImageNet 100 萬張圖片作為 training set, 內含 1000 種類別(日常生活中會看到的動物、植物、交通工具...)\n",
    "* input size 要是 (3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vgg11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vgg11\n",
    "# from torchvision.models import resnet50, vgg11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg = vgg11(weights=\"IMAGENET1K_V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 直接用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Conv2d' object has no attribute 'in_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[169], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_features\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Conv2d' object has no attribute 'in_features'"
     ]
    }
   ],
   "source": [
    "vgg.features[0].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n",
      "tensor(677)\n"
     ]
    }
   ],
   "source": [
    "input_img = torch.rand((1, 3, 224, 224))\n",
    "out = vgg(input_img)\n",
    "print(out.shape)\n",
    "print(out.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 預測結果是 1000 類中的第 677 類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 僅作 featrue extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 看一下 vgg 的結構，可以知道，取到 avgpool block，就會完成 feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_feature_extraction = nn.Sequential()\n",
    "vgg_feature_extraction.add_module(\"features\", vgg.features)\n",
    "vgg_feature_extraction.add_module(\"avgpool\", vgg.avgpool)\n",
    "\n",
    "# 看一下 output size\n",
    "input_img = torch.rand((1, 3, 224, 224))\n",
    "out_feature = vgg_feature_extraction(input_img)\n",
    "out_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25088])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_layer = nn.Flatten()\n",
    "out_feature_flatten = flatten_layer(out_feature)\n",
    "out_feature_flatten.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 把最後一層分類層，換掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torchvision.ops.FeaturePyramidNetwork([10, 20, 30], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturePyramidNetwork(\n",
       "  (inner_blocks): ModuleList(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(10, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): Conv2dNormActivation(\n",
       "      (0): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): Conv2dNormActivation(\n",
       "      (0): Conv2d(30, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (layer_blocks): ModuleList(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): Conv2dNormActivation(\n",
       "      (0): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (2): Conv2dNormActivation(\n",
       "      (0): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = OrderedDict()\n",
    "x['feat0'] = torch.rand(1, 10, 64, 64)\n",
    "x['feat2'] = torch.rand(1, 20, 16, 16)\n",
    "x['feat3'] = torch.rand(1, 30, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('feat0', torch.Size([1, 5, 64, 64])), ('feat2', torch.Size([1, 5, 16, 16])), ('feat3', torch.Size([1, 5, 8, 8]))]\n"
     ]
    }
   ],
   "source": [
    "print([(k, v.shape) for k, v in output.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('feat0',\n",
       "              tensor([[[[ 0.0651,  0.7307,  0.6804,  ...,  0.2749, -0.6173, -0.0506],\n",
       "                        [ 0.3674,  0.7479,  0.3160,  ...,  0.3150,  0.4875,  0.3863],\n",
       "                        [ 0.5469,  0.9629,  0.9104,  ...,  0.3155,  0.2938,  0.3751],\n",
       "                        ...,\n",
       "                        [-0.2186,  0.0819, -0.1050,  ...,  0.0866,  0.0996,  0.2195],\n",
       "                        [ 0.0311,  0.2352,  0.7405,  ...,  0.0981, -0.3773,  0.1260],\n",
       "                        [-0.4862,  0.0621,  0.0980,  ...,  0.7075,  0.0998,  0.4463]],\n",
       "              \n",
       "                       [[ 0.7306,  0.8579,  0.7388,  ...,  0.7095,  0.7229,  0.3815],\n",
       "                        [ 1.1151,  0.4164,  0.6854,  ..., -0.6103,  0.6109,  0.1263],\n",
       "                        [ 0.8844,  1.1289,  1.4066,  ..., -0.0380,  0.2836,  0.3413],\n",
       "                        ...,\n",
       "                        [ 0.8182,  0.6711,  0.7099,  ...,  1.6390,  1.3783,  1.4258],\n",
       "                        [ 1.2211,  0.7030,  1.3479,  ...,  1.6805,  1.6704,  1.7298],\n",
       "                        [ 0.3717,  0.2999,  0.4409,  ...,  1.1150,  0.8860,  0.9967]],\n",
       "              \n",
       "                       [[-0.7537, -0.1777, -0.2474,  ...,  0.4797, -0.2557,  0.4711],\n",
       "                        [ 0.2142, -0.0742, -0.0778,  ...,  0.1681,  0.0158,  0.1151],\n",
       "                        [ 0.4050, -0.3101, -0.1056,  ..., -0.3480,  0.1322,  0.2239],\n",
       "                        ...,\n",
       "                        [ 0.0886,  0.4967, -0.1118,  ...,  0.3715,  0.1221,  0.0528],\n",
       "                        [-0.0165, -0.1399,  0.5543,  ..., -0.0486,  0.0021,  0.4598],\n",
       "                        [-0.1852,  1.0833,  0.5247,  ...,  0.5244,  0.2833,  0.3631]],\n",
       "              \n",
       "                       [[ 0.2271, -0.0982,  0.0698,  ...,  0.0129, -0.0039,  0.3332],\n",
       "                        [-0.1630, -0.3354, -0.0763,  ..., -0.1846,  0.0825, -0.5498],\n",
       "                        [-0.2252, -0.6576, -0.5578,  ..., -0.2717,  0.0680,  0.2112],\n",
       "                        ...,\n",
       "                        [-0.4049, -0.6838, -0.8301,  ..., -0.3974, -0.5273, -0.0557],\n",
       "                        [-0.8159, -0.7860, -0.5461,  ..., -0.5485,  0.0715, -0.3864],\n",
       "                        [ 0.1099, -0.1757, -0.6333,  ..., -0.5165, -0.7815, -0.5082]],\n",
       "              \n",
       "                       [[-0.1099,  0.0519,  0.1517,  ..., -0.3964,  0.1471, -0.6940],\n",
       "                        [-0.3098, -0.1143, -0.4689,  ..., -0.2337, -0.7096, -0.6646],\n",
       "                        [-0.6796, -0.5282, -0.6652,  ..., -0.1974, -0.7837, -0.5769],\n",
       "                        ...,\n",
       "                        [-0.5142, -1.2376, -1.2597,  ..., -0.7708, -0.3910, -1.0117],\n",
       "                        [-1.4930, -1.5087, -1.9965,  ..., -0.2917, -0.4884, -0.8217],\n",
       "                        [ 0.0216, -1.1398, -0.8292,  ..., -0.5250, -0.5395, -0.7740]]]],\n",
       "                     grad_fn=<ConvolutionBackward0>)),\n",
       "             ('feat2',\n",
       "              tensor([[[[-1.0046e+00, -9.6859e-02, -4.3792e-01,  ..., -6.0342e-01,\n",
       "                         -6.5079e-01, -4.2325e-01],\n",
       "                        [-1.1751e+00, -6.9843e-01, -8.9345e-02,  ..., -4.9964e-01,\n",
       "                         -5.3205e-01,  3.3596e-01],\n",
       "                        [-5.8288e-01, -4.1455e-01, -1.1638e+00,  ..., -2.3590e-01,\n",
       "                         -8.0533e-01, -1.2404e-02],\n",
       "                        ...,\n",
       "                        [-1.1438e+00, -1.1373e+00, -1.1463e+00,  ..., -1.0788e+00,\n",
       "                         -1.5738e+00, -1.7807e-01],\n",
       "                        [-1.8233e+00, -1.6144e+00, -1.5602e+00,  ..., -4.5544e-01,\n",
       "                         -1.1697e+00, -7.6519e-02],\n",
       "                        [-7.7319e-01, -6.0476e-01, -6.8770e-01,  ..., -2.7921e-01,\n",
       "                         -3.6239e-01,  8.8864e-02]],\n",
       "              \n",
       "                       [[-3.3570e-01, -6.8396e-01,  2.0678e-01,  ...,  1.2873e-01,\n",
       "                          3.0306e-01,  3.1674e-01],\n",
       "                        [-1.4044e-01,  2.8896e-01,  1.2419e+00,  ...,  1.3313e+00,\n",
       "                          1.2536e+00,  6.6258e-01],\n",
       "                        [-4.7133e-01,  3.4678e-01,  6.3321e-01,  ...,  9.4539e-01,\n",
       "                          1.1835e+00,  9.3823e-01],\n",
       "                        ...,\n",
       "                        [-1.5316e-01,  5.3091e-01,  4.0453e-01,  ...,  4.7463e-01,\n",
       "                          3.2042e-01, -5.3936e-01],\n",
       "                        [ 1.8852e-02,  4.7117e-01,  5.1071e-01,  ...,  6.2206e-01,\n",
       "                         -9.5498e-03,  1.2806e-01],\n",
       "                        [-1.5201e-01,  4.9281e-01,  4.3185e-01,  ...,  1.0739e+00,\n",
       "                          2.7759e-01,  1.8371e-01]],\n",
       "              \n",
       "                       [[-1.9014e-01, -1.6418e-01, -3.9257e-01,  ..., -7.9676e-02,\n",
       "                         -4.5493e-01,  3.7891e-01],\n",
       "                        [-1.0331e-01,  3.9554e-01, -1.2823e-01,  ...,  2.0256e-01,\n",
       "                         -9.3130e-02,  3.1498e-01],\n",
       "                        [ 2.4490e-01,  3.1144e-01,  6.3074e-01,  ...,  8.5702e-01,\n",
       "                          2.8472e-01,  5.1317e-01],\n",
       "                        ...,\n",
       "                        [-2.9996e-01,  6.8361e-01,  1.2271e+00,  ...,  3.7819e-01,\n",
       "                          2.1921e-01,  2.1288e-01],\n",
       "                        [ 2.6015e-01,  1.3184e+00,  1.2651e+00,  ...,  1.1742e-03,\n",
       "                          5.5129e-01,  1.0867e+00],\n",
       "                        [ 6.0323e-01,  2.1325e+00,  2.0986e+00,  ...,  3.7722e-01,\n",
       "                          8.7845e-01,  1.0769e+00]],\n",
       "              \n",
       "                       [[-6.9043e-01, -9.1665e-02,  6.7821e-02,  ..., -3.1753e-01,\n",
       "                          1.2908e-01,  5.9732e-01],\n",
       "                        [-6.4826e-01,  1.8252e-01,  5.4596e-01,  ...,  6.1147e-01,\n",
       "                          9.8260e-01,  1.1496e+00],\n",
       "                        [-7.9203e-01,  4.0190e-01, -2.8135e-01,  ...,  2.5329e-01,\n",
       "                          7.5300e-01,  5.0764e-01],\n",
       "                        ...,\n",
       "                        [-1.4809e+00, -4.9529e-01, -4.7891e-01,  ..., -2.4788e-01,\n",
       "                          1.1538e-01,  3.3786e-01],\n",
       "                        [-1.4629e+00, -1.4426e+00, -4.4455e-01,  ...,  1.8282e-01,\n",
       "                          1.1021e-02,  1.9584e-01],\n",
       "                        [-6.7977e-01, -4.5405e-01, -1.0277e-01,  ...,  3.5755e-01,\n",
       "                          1.1750e-01,  7.0216e-01]],\n",
       "              \n",
       "                       [[-4.4328e-03,  5.4331e-01,  5.1264e-01,  ...,  3.2425e-01,\n",
       "                          2.9698e-01, -2.6950e-01],\n",
       "                        [-9.7634e-02, -4.8624e-01, -1.6891e-01,  ..., -1.1064e+00,\n",
       "                         -6.1964e-01, -3.3575e-01],\n",
       "                        [ 6.3835e-01, -2.3533e-01, -5.9266e-01,  ..., -7.0922e-01,\n",
       "                         -5.5635e-01, -5.4694e-01],\n",
       "                        ...,\n",
       "                        [ 5.0356e-01, -7.5742e-01, -7.6203e-01,  ..., -9.3704e-01,\n",
       "                         -1.1793e+00, -3.7278e-01],\n",
       "                        [-4.5237e-01, -1.1845e+00, -1.3505e+00,  ...,  7.0696e-02,\n",
       "                         -9.2490e-01, -6.8261e-02],\n",
       "                        [ 8.7253e-01,  2.6779e-01, -1.4501e-01,  ...,  1.1677e+00,\n",
       "                          8.8643e-01, -1.6407e-01]]]], grad_fn=<ConvolutionBackward0>)),\n",
       "             ('feat3',\n",
       "              tensor([[[[-2.8874e-01, -2.2276e-01, -3.8304e-01, -2.4295e-01, -3.5041e-01,\n",
       "                         -2.4153e-01, -6.5777e-01,  6.9771e-02],\n",
       "                        [-5.8159e-01, -9.2228e-01, -1.0400e+00, -8.7284e-01, -6.6970e-01,\n",
       "                         -4.0442e-01, -5.9494e-01, -3.7480e-01],\n",
       "                        [-6.8877e-01, -6.7432e-01, -8.9043e-01, -1.9639e-01, -9.9080e-01,\n",
       "                         -5.9162e-01, -8.8178e-01, -6.0240e-01],\n",
       "                        [-8.2693e-01, -4.4837e-01, -9.2648e-01, -7.7054e-01, -1.2257e+00,\n",
       "                         -1.1067e-01, -1.3667e+00, -9.1597e-03],\n",
       "                        [-9.7302e-01, -6.8912e-01, -1.2097e+00, -4.4555e-01, -7.1897e-01,\n",
       "                         -1.0317e+00, -4.8158e-01, -6.7882e-01],\n",
       "                        [-8.0409e-01, -4.5036e-01,  4.8329e-02, -7.0340e-01, -1.0655e+00,\n",
       "                         -5.1758e-01, -1.1960e+00, -4.8788e-01],\n",
       "                        [-8.2313e-01, -6.6361e-01, -1.0613e+00, -7.8586e-01, -5.8351e-01,\n",
       "                         -8.9468e-01, -1.0915e+00, -8.5448e-01],\n",
       "                        [-6.4070e-01, -4.5867e-01, -4.3224e-01, -5.8518e-01, -6.6837e-01,\n",
       "                         -7.0784e-01, -9.8576e-01,  1.4557e-02]],\n",
       "              \n",
       "                       [[-3.9968e-01, -6.7951e-01, -3.7101e-02, -5.9991e-01, -6.8091e-01,\n",
       "                         -6.0446e-01, -7.9494e-01, -3.2561e-01],\n",
       "                        [ 5.0599e-01,  8.7972e-02,  5.2346e-01,  1.1415e-01, -1.0149e-01,\n",
       "                          1.2449e-01, -1.2265e-01, -7.4945e-02],\n",
       "                        [ 2.0567e-01, -3.0807e-02, -2.1787e-01, -2.6581e-02, -1.5607e-01,\n",
       "                          1.1049e-01,  2.7278e-01, -3.7792e-02],\n",
       "                        [ 6.4684e-01,  1.2713e-01, -2.7874e-01,  4.4691e-01, -3.8186e-03,\n",
       "                          1.8082e-01, -8.0807e-02,  2.7791e-01],\n",
       "                        [ 2.7915e-01,  3.1683e-01, -1.5861e-01,  3.3136e-02, -2.2338e-01,\n",
       "                         -3.1305e-01, -1.2926e-01, -9.7007e-02],\n",
       "                        [ 3.5112e-01,  2.5498e-01, -1.8532e-01, -5.6975e-01, -3.1606e-01,\n",
       "                         -2.4860e-01, -1.4100e-01,  3.4120e-01],\n",
       "                        [ 1.0163e-01, -9.3440e-02, -1.7757e-02,  7.3701e-02,  4.6703e-01,\n",
       "                          3.7238e-01,  6.9386e-01,  5.3925e-01],\n",
       "                        [ 2.6343e-01,  4.3834e-01, -6.1088e-02,  1.7633e-01, -1.2902e-01,\n",
       "                          3.0297e-01,  2.8894e-01,  5.1168e-01]],\n",
       "              \n",
       "                       [[-6.8676e-02,  6.0822e-01, -7.7878e-02,  3.9836e-02,  6.3387e-01,\n",
       "                         -1.4000e-01,  2.6515e-01, -2.1326e-01],\n",
       "                        [ 3.7375e-01,  2.6949e-01,  9.4511e-02,  8.0981e-02,  1.7669e-01,\n",
       "                         -1.9057e-01,  9.2003e-01,  3.9352e-01],\n",
       "                        [-4.3951e-02, -2.7200e-01,  5.0824e-01,  1.3231e-01,  4.8295e-01,\n",
       "                         -2.5074e-01,  2.7287e-01,  1.8617e-01],\n",
       "                        [-4.4838e-01,  5.2833e-01,  7.8973e-01,  2.7389e-01,  3.6471e-01,\n",
       "                          1.5844e-01, -1.5481e-01,  1.4131e-02],\n",
       "                        [ 2.0741e-01,  3.2294e-01,  2.2939e-01,  6.6524e-02,  5.6415e-01,\n",
       "                          1.6167e-01,  4.2358e-01,  1.8817e-01],\n",
       "                        [ 6.3459e-02, -9.8572e-02,  2.6159e-01,  3.9043e-01,  2.5657e-01,\n",
       "                         -4.2722e-01,  3.8625e-01,  5.6028e-02],\n",
       "                        [ 5.2971e-02,  1.4209e-01,  8.2037e-01,  1.3158e-01, -3.3500e-01,\n",
       "                          4.0593e-01,  1.6757e-02,  5.1574e-01],\n",
       "                        [-1.0005e-01,  6.0240e-02,  5.9779e-01, -6.1668e-02,  1.3741e-01,\n",
       "                          2.8261e-01, -5.6418e-02,  4.9936e-01]],\n",
       "              \n",
       "                       [[ 1.3703e-01, -2.0366e-01, -2.5322e-01, -1.5589e-01, -4.2823e-01,\n",
       "                         -3.6472e-01,  4.7253e-04, -1.9252e-01],\n",
       "                        [-2.3838e-01, -1.5146e-01, -6.6108e-01, -8.0034e-01, -7.6699e-02,\n",
       "                         -4.7165e-01, -6.0114e-01, -4.9989e-01],\n",
       "                        [-2.7868e-01, -6.9715e-01, -3.1942e-01, -3.8800e-01, -4.3677e-01,\n",
       "                         -7.3560e-01, -3.9836e-01, -6.3876e-01],\n",
       "                        [-4.8928e-01, -8.1613e-01, -3.3260e-01, -9.0506e-01, -1.9451e-01,\n",
       "                         -6.4661e-01, -6.6611e-01, -3.6382e-01],\n",
       "                        [-4.7075e-01, -7.0676e-01, -3.9286e-01, -1.1942e+00, -3.7660e-01,\n",
       "                         -6.0977e-01, -3.2889e-01, -4.6632e-01],\n",
       "                        [-3.5595e-01, -5.6389e-01, -8.8521e-01, -8.5002e-01, -6.1210e-01,\n",
       "                         -3.0098e-01, -7.2063e-01, -2.3527e-01],\n",
       "                        [-6.4030e-01, -8.8976e-01, -5.1042e-01, -1.3868e-01, -6.5054e-01,\n",
       "                         -5.0046e-01, -2.1001e-01, -4.3701e-01],\n",
       "                        [-4.1798e-01, -7.9194e-01, -6.1039e-01, -3.9954e-01, -8.7290e-02,\n",
       "                         -4.6569e-02, -1.9503e-01,  9.9226e-02]],\n",
       "              \n",
       "                       [[ 3.7162e-01,  5.4013e-01,  6.0619e-01,  2.8003e-01,  3.6680e-01,\n",
       "                         -8.3289e-02, -6.0576e-02, -4.1560e-01],\n",
       "                        [ 1.0384e+00,  5.6891e-01,  7.9417e-01,  2.9500e-01,  3.8786e-01,\n",
       "                          4.4341e-01,  6.6960e-01,  6.6912e-01],\n",
       "                        [ 1.2105e+00,  3.1149e-01,  5.8467e-01,  6.5764e-01,  1.0214e-01,\n",
       "                          2.5588e-01,  8.4963e-01,  8.0353e-01],\n",
       "                        [ 1.0731e+00,  7.8519e-01,  4.5299e-01,  1.1584e+00,  8.1042e-01,\n",
       "                          6.6112e-01,  5.2571e-01,  6.5583e-01],\n",
       "                        [ 7.7312e-01,  6.8071e-01,  6.6579e-01,  8.3232e-01,  7.0434e-01,\n",
       "                          7.6747e-01,  1.0043e+00,  3.7484e-01],\n",
       "                        [ 1.1412e+00,  7.9162e-01,  8.4955e-01,  7.3521e-01,  1.0017e+00,\n",
       "                          8.3194e-01,  1.0010e+00,  4.7049e-01],\n",
       "                        [ 9.8399e-01,  7.7165e-01,  5.7194e-01,  4.3298e-01,  7.6182e-01,\n",
       "                          9.7778e-01,  4.0487e-01,  5.8800e-01],\n",
       "                        [ 8.9305e-01,  1.3038e+00,  1.0443e+00,  7.9219e-01,  8.3838e-01,\n",
       "                          6.6010e-01,  9.0881e-01,  8.6897e-01]]]],\n",
       "                     grad_fn=<ConvolutionBackward0>))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuVwkwv74L9X",
    "tags": []
   },
   "source": [
    "## Classical Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KwfCDA84L9X"
   },
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u9SGDNv4L9X"
   },
   "source": [
    "#### `nn.Linear(in_dim, out_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `torch.nn.Flatten(start_dim=1, end_dim=- 1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 預設會從 dim = 1 開始，是因為 dim = 0 是 batch 的軸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1776, 0.9160, 0.8642, 0.5876, 0.0466],\n",
      "          [0.5273, 0.0673, 0.7782, 0.5226, 0.8699],\n",
      "          [0.7431, 0.8379, 0.7179, 0.9059, 0.1476],\n",
      "          [0.2583, 0.5090, 0.8046, 0.1763, 0.6814],\n",
      "          [0.6311, 0.2884, 0.9314, 0.7833, 0.7727]],\n",
      "\n",
      "         [[0.5804, 0.8342, 0.6768, 0.5435, 0.4607],\n",
      "          [0.7297, 0.7294, 0.0220, 0.5833, 0.4130],\n",
      "          [0.5099, 0.3951, 0.7678, 0.2135, 0.0866],\n",
      "          [0.8996, 0.9625, 0.3403, 0.5967, 0.0765],\n",
      "          [0.2447, 0.3609, 0.6060, 0.6795, 0.3156]],\n",
      "\n",
      "         [[0.2016, 0.7268, 0.8850, 0.7330, 0.0563],\n",
      "          [0.9394, 0.0015, 0.0673, 0.2740, 0.2546],\n",
      "          [0.0829, 0.2890, 0.8807, 0.0301, 0.4578],\n",
      "          [0.7551, 0.1646, 0.1234, 0.4990, 0.0452],\n",
      "          [0.7401, 0.1458, 0.2748, 0.1216, 0.0635]]],\n",
      "\n",
      "\n",
      "        [[[0.6772, 0.1119, 0.0980, 0.6105, 0.6992],\n",
      "          [0.8782, 0.8777, 0.9903, 0.8512, 0.5776],\n",
      "          [0.8834, 0.0357, 0.4094, 0.5686, 0.8500],\n",
      "          [0.7925, 0.5945, 0.0673, 0.4228, 0.6723],\n",
      "          [0.1631, 0.3959, 0.3661, 0.7034, 0.0851]],\n",
      "\n",
      "         [[0.7672, 0.1114, 0.8194, 0.1772, 0.8017],\n",
      "          [0.0814, 0.3402, 0.6552, 0.7820, 0.1323],\n",
      "          [0.8027, 0.1131, 0.3673, 0.6294, 0.2033],\n",
      "          [0.1771, 0.3085, 0.8691, 0.2127, 0.8360],\n",
      "          [0.9168, 0.4087, 0.4621, 0.3283, 0.7204]],\n",
      "\n",
      "         [[0.8540, 0.8420, 0.6497, 0.7415, 0.0710],\n",
      "          [0.1066, 0.8486, 0.9006, 0.3340, 0.1622],\n",
      "          [0.3209, 0.2985, 0.2924, 0.6111, 0.8170],\n",
      "          [0.5262, 0.9499, 0.4510, 0.2628, 0.7200],\n",
      "          [0.0460, 0.8964, 0.7780, 0.7176, 0.9792]]]])\n",
      "torch.Size([2, 3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(2, 3, 5, 5) # batch_size = 2, 每個 batch，都有 num_channel, Height, Width 的 image\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2404, 0.7937, 0.4494, 0.3026, 0.4312, 0.8841, 0.5139, 0.7175, 0.4840,\n",
      "         0.2230, 0.7278, 0.4127, 0.0997, 0.9097, 0.2046, 0.2216, 0.8711, 0.5727,\n",
      "         0.2523, 0.8165, 0.6370, 0.6532, 0.1003, 0.0487, 0.5645, 0.0552, 0.3277,\n",
      "         0.0192, 0.1828, 0.5912, 0.2608, 0.2675, 0.2025, 0.0099, 0.5579, 0.3849,\n",
      "         0.3101, 0.6909, 0.5483, 0.7962, 0.2805, 0.0691, 0.3280, 0.6559, 0.4262,\n",
      "         0.8914, 0.2734, 0.6694, 0.1777, 0.3344, 0.6636, 0.3904, 0.5210, 0.6946,\n",
      "         0.7324, 0.8554, 0.1056, 0.6247, 0.9505, 0.5251, 0.3029, 0.5655, 0.6137,\n",
      "         0.1051, 0.3008, 0.9149, 0.6882, 0.7984, 0.5965, 0.3351, 0.5486, 0.9809,\n",
      "         0.7653, 0.3887, 0.5888],\n",
      "        [0.0939, 0.3101, 0.8238, 0.5930, 0.2046, 0.5058, 0.1250, 0.4880, 0.8498,\n",
      "         0.0114, 0.1792, 0.1898, 0.4208, 0.6207, 0.8486, 0.7073, 0.3133, 0.7857,\n",
      "         0.8234, 0.3309, 0.7181, 0.0229, 0.5755, 0.5876, 0.4981, 0.2573, 0.9795,\n",
      "         0.8049, 0.6150, 0.0538, 0.9121, 0.6169, 0.2563, 0.2236, 0.5475, 0.5064,\n",
      "         0.6025, 0.6887, 0.9408, 0.2192, 0.6901, 0.9923, 0.7835, 0.7694, 0.5130,\n",
      "         0.4790, 0.9041, 0.3275, 0.6307, 0.2808, 0.9228, 0.6131, 0.1735, 0.6140,\n",
      "         0.8222, 0.5546, 0.1240, 0.9193, 0.3230, 0.9113, 0.0417, 0.5647, 0.8687,\n",
      "         0.2223, 0.5476, 0.1715, 0.0361, 0.9582, 0.3656, 0.1617, 0.2577, 0.7713,\n",
      "         0.0747, 0.8483, 0.3002]])\n",
      "torch.Size([2, 75])\n"
     ]
    }
   ],
   "source": [
    "my_flatten = torch.nn.Flatten()\n",
    "y = my_flatten(X)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `nn.Dropout(p=0.2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dropout layer 的作用是：  \n",
    "  * input: `nn.Dropout()` 通常都是接在全連階層後的 layer，所以他預設是接受到 (batch_size, n_neuron) 這種 shape. \n",
    "  * output: \n",
    "    * 對每個 neuron，都有 p 的機率，拔掉這個 neuron <-> 有 p 的機率，讓這個 neuron 的輸出值變成 0  \n",
    "    * 對於沒有被拔掉的 neuron，他的輸出值會被縮放(原始值/ (1-p))  \n",
    "    * 也就是說，隨機讓一些 neuron 的影響力變成 0, 但加強剩餘 neuron 的權重\n",
    "* 這樣做的話\n",
    "  * 每次 forward，都有 p 比例的輸出值變成 0，那算 gradient 時 (微分後，evaluate在這個輸出值上）就會是 0，就無法更新此 neuron 的參數，就等於停止這個 neuron 的學習。\n",
    "  * 由於每次 forawrd，都隨機的讓 p 比例的 neuron 不能學，但整體的 loss 又希望他一直變小。所以可以讓 NN 學到不要依賴某幾個 neuron 來做決策，讓結果可以比較 robust 一點。有點像一個球隊，總是隨機的讓某些球員不能上場，但又希望球隊贏球，所以 optimize 球隊的實力，就不會只依賴在某幾個明星球員上，而是會均分到各個球員。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3376, 0.8209, 0.5483, 0.5076, 0.9084],\n",
       "        [0.2590, 0.4310, 0.4476, 0.4791, 0.1276]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= torch.rand(2, 5) # batch_size = 2, 每個 batch 在上一個 layer 結束，都得到 5 個 ouptut\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      " tensor([[0.3376, 0.8209, 0.5483, 0.5076, 0.9084],\n",
      "        [0.2590, 0.4310, 0.4476, 0.4791, 0.1276]]) \n",
      "\n",
      "no_dropout: \n",
      " tensor([[0.3376, 0.8209, 0.5483, 0.5076, 0.9084],\n",
      "        [0.2590, 0.4310, 0.4476, 0.4791, 0.1276]]) \n",
      "\n",
      "all_dropout: \n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]) \n",
      "\n",
      "half_dropout: \n",
      " tensor([[0.6752, 1.6417, 1.0966, 0.0000, 1.8167],\n",
      "        [0.0000, 0.0000, 0.8952, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "no_dropout = nn.Dropout(p = 0)\n",
    "all_dropout = nn.Dropout(p = 1)\n",
    "half_dropout = nn.Dropout(p = 0.5)\n",
    "\n",
    "y1 = no_dropout(X)\n",
    "y2 = all_dropout(X)\n",
    "y3 = half_dropout(X)\n",
    "\n",
    "print(\"input: \\n\", X, \"\\n\")\n",
    "print(\"no_dropout: \\n\", y1, \"\\n\")\n",
    "print(\"all_dropout: \\n\", y2, \"\\n\")\n",
    "print(\"half_dropout: \\n\", y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，最後有 dropout 掉的結果，剩餘的 output 值都被放大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 實際應用時，大概會這樣用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout1, dropout2 = 0.2, 0.5\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # 不管進來的是啥 input, 先轉成 (bach_size, n_neuron) 這種 shape\n",
    "    nn.Flatten(),\n",
    "    # 一般的 linear 層, activation 層\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    # dropout 來了\n",
    "    nn.Dropout(dropout1),\n",
    "    # 同樣的概念再來一次    \n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    # 在第二个全连接层之后添加一个dropout层\n",
    "    nn.Dropout(dropout2),\n",
    "    nn.Linear(256, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如果要自己寫 dropout layer，會長這樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super().__init__()\n",
    "        assert 0 <= dropout_rate <= 1\n",
    "        self.dropout_rate = dropout_rate\n",
    "    def forward(self, x):\n",
    "        if self.dropout_rate == 1:\n",
    "            mask = torch.ones_like(x)\n",
    "            return mask * x\n",
    "        elif self.dropout_rate == 0:\n",
    "            mask = torch.zeros_like(x)\n",
    "            return mask * x\n",
    "        else:\n",
    "            mask = (torch.rand(x.shape) > self.dropout_rate).float()\n",
    "            return mask * x / (1.0 - self.dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4401, 0.0847, 0.2464, 0.0768, 0.6978],\n",
      "        [0.6341, 0.5199, 0.0806, 0.3622, 0.9605]])\n",
      "tensor([[0.8802, 0.0000, 0.0000, 0.0000, 1.3957],\n",
      "        [1.2681, 0.0000, 0.0000, 0.0000, 1.9211]])\n"
     ]
    }
   ],
   "source": [
    "X= torch.rand(2, 5) # batch_size = 2, 每個 batch 在上一個 layer 結束，都得到 5 個 ouptut\n",
    "print(X)\n",
    "my_dropout = MyDropout(0.5)\n",
    "print(my_dropout(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `nn.Dropout2d(p=0.2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input 是 (Batch_size, Channel, Height, Width)  \n",
    "* dropout 過程\n",
    "  * 概念：隨機將 某個batch的某個channel下的 feature map 全設為 0\n",
    "  * 實際操作：此函數在背後，是先用 Ber(p)，生成 Batch_size x Channel 個 (H,W) 的矩陣 (所以要嘛全 0 ，要嘛全 1)，然後去乘上 input tensor\n",
    "* output 就還是一樣的 shape: (Batch_size, Channel, Height, Width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 看個例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.9790, -0.9097],\n",
      "          [ 1.4508,  1.6278]],\n",
      "\n",
      "         [[ 0.9969,  0.4001],\n",
      "          [-0.5196, -0.2183]],\n",
      "\n",
      "         [[ 0.9921, -1.2808],\n",
      "          [ 0.7432, -0.5854]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5112,  1.0735],\n",
      "          [-0.2309, -0.3365]],\n",
      "\n",
      "         [[ 1.1461,  2.6715],\n",
      "          [-1.6048,  1.2848]],\n",
      "\n",
      "         [[-0.7704, -1.1662],\n",
      "          [-0.0082, -0.4357]]]])\n",
      "tensor([[[[-1.9581, -1.8193],\n",
      "          [ 2.9015,  3.2556]],\n",
      "\n",
      "         [[ 0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000]],\n",
      "\n",
      "         [[ 2.2921,  5.3431],\n",
      "          [-3.2096,  2.5696]],\n",
      "\n",
      "         [[-1.5407, -2.3324],\n",
      "          [-0.0164, -0.8714]]]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(2, 3, 2, 2)\n",
    "print(input)\n",
    "\n",
    "m = nn.Dropout2d(p=0.5) # 有 0.5 的機率，將 該batch該channel 下的 feature map 設為 0 <=> 有 50% 的 feature map 被設為 0\n",
    "\n",
    "out = m(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `nn.BatchNorm2d()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input shape: (B, C, H, W). \n",
    "* batch normalization 的過程：  \n",
    "  * by channel 做。例如 C = 3 的話，就把 channel = 1 的所有 batch, H, W 拉成向量，算 mean 和 sd. 然後做標準化. \n",
    "  * 同樣的步驟對 channel = 2, channel =3 做。所以會得到 3 個 mean 和 sd. \n",
    "  * output 就會是一樣的 shape. \n",
    "* output shape: (B, C, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.BatchNorm2d(\n",
    "    num_features, # 在影像中，一個 channel 被當成一個 feature, 所以這邊寫 channel 數\n",
    "    eps=1e-05, # 標準化的時候，分母加上的小數字，避免 sd = 0 時掛掉\n",
    "    momentum=0.1, # 每一個 batch 都在做 EMA, mean_new = momentum * mean_old + (1 - momentum) * this_batch_mean\n",
    "    affine=True, # 下面解釋\n",
    "    track_running_stats=True, # 會紀錄最新 EMA 結果的 mean 和 std, 稱為 running_mean 和 running_std; 這樣 inference 時就可以用\n",
    "    device=None, \n",
    "    dtype=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 補充一下上面的 affine = True 在講啥  \n",
    "* batch normalization 實際在做時，他的公式長這樣： $y = \\frac{x-E(x)}{\\sqrt{var(x)+\\epsilon}}\\times \\gamma + \\beta$\n",
    "* 也就是說，標準化完，本來變 $Normal(0,1)$，但他可以從資料中學習，需要的話，會變成 $Normal(\\beta, \\gamma)$  \n",
    "* 而 `affine = True` 的意思就是，我會先給 $\\gamma$ 和 $\\beta$ 起始值，分別為 1 和 0 (所以目前還沒改變分配)，但 $\\gamma$ 和 $\\beta$ 是可以學習的，他後續就會估這個參數\n",
    "* 以下，開始實際使用吧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[16., 17., 18., 19.],\n",
       "          [20., 21., 22., 23.],\n",
       "          [24., 25., 26., 27.],\n",
       "          [28., 29., 30., 31.]]],\n",
       "\n",
       "\n",
       "        [[[32., 33., 34., 35.],\n",
       "          [36., 37., 38., 39.],\n",
       "          [40., 41., 42., 43.],\n",
       "          [44., 45., 46., 47.]],\n",
       "\n",
       "         [[48., 49., 50., 51.],\n",
       "          [52., 53., 54., 55.],\n",
       "          [56., 57., 58., 59.],\n",
       "          [60., 61., 62., 63.]]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(64, dtype=torch.float32).reshape((2, 2, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.4113, -1.3513, -1.2912, -1.2312],\n",
       "          [-1.1711, -1.1111, -1.0510, -0.9909],\n",
       "          [-0.9309, -0.8708, -0.8108, -0.7507],\n",
       "          [-0.6907, -0.6306, -0.5705, -0.5105]],\n",
       "\n",
       "         [[-1.4113, -1.3513, -1.2912, -1.2312],\n",
       "          [-1.1711, -1.1111, -1.0510, -0.9909],\n",
       "          [-0.9309, -0.8708, -0.8108, -0.7507],\n",
       "          [-0.6907, -0.6306, -0.5705, -0.5105]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5105,  0.5705,  0.6306,  0.6907],\n",
       "          [ 0.7507,  0.8108,  0.8708,  0.9309],\n",
       "          [ 0.9909,  1.0510,  1.1111,  1.1711],\n",
       "          [ 1.2312,  1.2912,  1.3513,  1.4113]],\n",
       "\n",
       "         [[ 0.5105,  0.5705,  0.6306,  0.6907],\n",
       "          [ 0.7507,  0.8108,  0.8708,  0.9309],\n",
       "          [ 0.9909,  1.0510,  1.1111,  1.1711],\n",
       "          [ 1.2312,  1.2912,  1.3513,  1.4113]]]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_norm = nn.BatchNorm2d(2)\n",
    "my_norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3500, 3.9500])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_norm.running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.5000)\n",
      "tensor(39.5000)\n"
     ]
    }
   ],
   "source": [
    "print(X[:,0,:,:].mean())\n",
    "print(X[:,1,:,:].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到實際的 mean 和 running_mean 差了 10 倍，是因為 running_mean 已經先把目前的 mean x 0.1, 準備等等下一個 batch 進來時，用這個 running_mean + (1-0.1) x new_batch_mean 來得到 Exponential Moving Average (EMA) 的 mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6527, -1.2206,  0.6258, -0.2745],\n",
       "         [ 1.5530,  0.2956, -0.7750,  0.2132],\n",
       "         [-0.2742,  0.4927, -0.8178,  0.5907]],\n",
       "\n",
       "        [[ 0.5856,  0.7579, -0.3784,  0.1930],\n",
       "         [-0.1841,  1.4684, -0.8964,  1.9392],\n",
       "         [ 1.3827, -1.5651,  0.5822, -0.6323]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, sentence_length, embedding_dim = 2, 3, 4\n",
    "data = torch.randn(batch, sentence_length, embedding_dim)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1615, -0.6705,  1.4275,  0.4045],\n",
       "         [ 1.4901, -0.0316, -1.3272, -0.1313],\n",
       "         [-0.4708,  0.8564, -1.4115,  1.0260]],\n",
       "\n",
       "        [[ 0.6781,  1.0727, -1.5298, -0.2210],\n",
       "         [-0.6592,  0.7631, -1.2722,  1.1682],\n",
       "         [ 1.2777, -1.3363,  0.5678, -0.5092]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "layer_norm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1615, -0.6705,  1.4275,  0.4045],\n",
       "        [ 1.4901, -0.0316, -1.3272, -0.1313],\n",
       "        [-0.4708,  0.8564, -1.4115,  1.0260]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data[0] - data[0].mean(dim = -1, keepdim = True))/torch.sqrt(torch.var(data[0], dim = -1, keepdim = True, unbiased = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7847, -1.2869,  0.8404, -0.1969],\n",
       "         [ 1.9086,  0.4600, -0.7734,  0.3651],\n",
       "         [-0.1965,  0.6871, -0.8228,  0.8000]],\n",
       "\n",
       "        [[ 0.7941,  0.9926, -0.3165,  0.3418],\n",
       "         [-0.0927,  1.8112, -0.9133,  2.3536],\n",
       "         [ 1.7125, -1.6838,  0.7902, -0.6091]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data - data[0,:,:].mean())/torch.sqrt(torch.var(data[0,:,:], unbiased = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N, C, H, W = 2, 2, 3, 3\n",
    "input = torch.randn(N, C, H, W)\n",
    "# Normalize over the last three dimensions (i.e. the channel and spatial dimensions)\n",
    "# as shown in the image below\n",
    "layer_norm = nn.LayerNorm([C, H, W])\n",
    "output = layer_norm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2504,  0.5595, -1.3587],\n",
       "          [ 2.0666, -0.3750,  1.0990],\n",
       "          [-1.5351,  0.0377, -2.0293]],\n",
       "\n",
       "         [[ 0.4013,  1.1532,  0.3073],\n",
       "          [ 0.0422,  0.4210, -0.5679],\n",
       "          [-0.8564,  0.8200, -0.4358]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1713, -0.7803, -0.6681],\n",
       "          [ 0.4865, -0.5627, -1.4878],\n",
       "          [ 0.6812, -0.9151,  0.4169]],\n",
       "\n",
       "         [[ 2.7738,  0.2237,  0.3362],\n",
       "          [-0.4778, -0.7063,  0.9934],\n",
       "          [ 1.2262, -1.1143, -0.5968]]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kA1p_DR4L9X"
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zjSWiDF4L9X"
   },
   "source": [
    "#### convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `nn.Conv2d()`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 完整語法是： `nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 最重要的就是學 output shape 的算法\n",
    "* 先講實務運作時的重要結論：\n",
    "  * conv 的 padding 固定設為 (kernel_size - 1)/ 2\n",
    "  * stride 如果要 same padding, 就設為 1, 要高寬減半, 就設為 2  \n",
    "* 舉例來說：  \n",
    "  * 我的 kernel_size = 3, 我想要 same padding，那 padding 設為 (3-1)/2 = 1, stride 設為 1\n",
    "  * 我的 kernel_size = 3, 我想要高寬減半，那 padding 設為 (3-1)/2 = 1, stride 設為 2\n",
    "  * 1x1 conv 是比較特殊的應用，主要是用調整通道數，所以通常都是要 same padding，那一樣： padding = (1-1)/2 = 0, stride = 1, 合理吧～\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 接下來講 general case，以及解釋為什麼要這樣設. \n",
    "* 符號定義：  \n",
    "  * $n_h$: input 的高\n",
    "  * $k_h$: kernel 的高\n",
    "  * $p_h$: 高的方向的 padding 為多少\n",
    "  * $s_h$: 高的方向的步幅 (stride) 有多少\n",
    "* 那 output shape 為: $\\left \\lfloor{\\frac{n_h+2p_h-k_h}{s_h}+1}\\right \\rfloor \\times \\left \\lfloor{\\frac{n_w+2p_w-k_w}{s_w}+1}\\right \\rfloor$\n",
    "* 括號是高斯符號，floor，例如 $\\left \\lfloor{3.5}\\right \\rfloor = 3$，意思就是 kernel 如果除不盡，最後那一步就不走了。\n",
    "---\n",
    "* 講一下公式的推導。舉例來說， input size = 6x6, kernel size = 3, padding = 1, stride = 1\n",
    "  * 只看 width 方向就好。先把兩個 padding 放到 width 旁邊，現在整張圖的 width 變成 $n + 2p = 8$\n",
    "  * 一個 kernel 疊上來，吃掉 k 個寬，所以現在剩下 $n+2p-k = 6+2-3=5$ 個 widht 可以用\n",
    "  * stride 每次 1 格，表示剩下的寬，每移動1次kernel，被吃掉1格，所以可以移動 $\\frac{n+2p-k}{s} = 5/1 = 5$ 格\n",
    "  * 加上最一開始疊上來，就已經算 1 格了，所以最終 output 的 width 為 $\\frac{n+2p-k}{s} + 1 = 5/1 + 1 = 6$, 所以是 same padding\n",
    "---\n",
    "* 解釋一下實務上這樣設的原因： \n",
    "  * 我想要做 same padding: \n",
    "    * padding設為 $\\frac{k-1}{2}$, stride設為1，那結果就會是 same padding (帶進公式推一下就知道了)\n",
    "    * 例如 kernel_size = 3, 那 padding 就設 (3-1)/2 = 1, stride 設為 1，就會是 same padding\n",
    "    * 又例如 kernel_size = 7, 那 padding 就設 (7-1)/2 = 3, stride 設為 1，就會是 same padding. \n",
    "  * 我想要高寬減半:\n",
    "    * padding設為 $\\frac{k-1}{2}$, stride設為2，那結果就會是高寬減半。但是是用到 floor. \n",
    "    * 帶進公式看一下\n",
    "      * 原本的 $\\frac{n_h+2p_h-k_h}{s_h}+1$，可以把 1 放回分子 $\\frac{n_h+2p_h-k_h+s_h}{s_h}$  \n",
    "      * 把 $p_h = \\frac{k-1}{2}$ 帶入，變成 $\\frac{n_h+s_h-1}{s_h}$，再拆成 $\\frac{n_h}{s_h} + \\frac{s_h-1}{s_h}$  \n",
    "      * 前面項，如果 $s_h = 2$，那就做到高寬減半了; 後面那項一定 < 1，所以高斯取 floor 後，就被捨棄掉   \n",
    "    * 來個例子： kernel_size = 3, 那 padding 就設 (3-1)/2 = 1, stride 設為 2，就會是高寬減半"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* 來點實際例子吧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape: torch.Size([10, 3, 244, 244])\n",
      "same padding shape: torch.Size([10, 3, 244, 244])\n",
      "高寬減半: torch.Size([10, 3, 122, 122])\n"
     ]
    }
   ],
   "source": [
    "input_img = torch.rand(10, 3, 244, 244)\n",
    "print(\"original shape:\", input_img.shape)\n",
    "\n",
    "# same padding, kernel_size = 3\n",
    "k = 3\n",
    "p = int((k-1)/2)\n",
    "s = 1\n",
    "\n",
    "my_conv = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = k, padding = p, stride = s)\n",
    "out = my_conv(input_img)\n",
    "print(\"same padding shape:\", out.shape)\n",
    "\n",
    "# 高寬減半\n",
    "k = 3\n",
    "p = int((k-1)/2)\n",
    "s = 2\n",
    "\n",
    "my_conv2 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = k, padding = p, stride = s)\n",
    "out = my_conv2(input_img)\n",
    "print(\"高寬減半:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 來個比較有趣的， 1x1 的 convolution (主要用來改變通道)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape: torch.Size([10, 64, 512, 512])\n",
      "same padding shape: torch.Size([10, 128, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# 通道數加倍，same padding\n",
    "input_img = torch.rand(10, 64, 512, 512)\n",
    "print(\"original shape:\", input_img.shape)\n",
    "\n",
    "# same padding, kernel_size = 1\n",
    "k = 1\n",
    "p = int((k-1)/2)\n",
    "s = 1\n",
    "\n",
    "my_conv = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = k, padding = p, stride = s)\n",
    "out = my_conv(input_img)\n",
    "print(\"same padding shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcENycuX4L9Y"
   },
   "source": [
    "##### 1d convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuwfPgtG4L9Y"
   },
   "source": [
    "#### pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pooling 的重點，也是對 output shape 的掌握。\n",
    "* 原理就和 conv 那邊介紹的一樣，output shape = $\\left \\lfloor{\\frac{n_h+2p_h-k_h}{s_h}+1}\\right \\rfloor \\times \\left \\lfloor{\\frac{n_w+2p_w-k_w}{s_w}+1}\\right \\rfloor$  \n",
    "---\n",
    "* 但從實務角度來看，pooling 的使用，大概分為兩類：  \n",
    "  * 高寬降 k 倍的 pooling (e.g. k=2, 就是最常用的高寬減半).  \n",
    "    * 最常見的，就是 conv 時先做 same padding, 然後在 pooling 時再讓他高寬減半來降維  \n",
    "    * 那作法就是 kernel_size 設為 2, stride 設為 2, padding 設為 0。\n",
    "    * 如果是要降為 k 倍，那就 kernel_size 設為 k, stride 也設為 k, padding 設為 0. \n",
    "    * 也因為這種特性，常見的 layer (e.g. `nn.MaxPool2d()`, `nn.AvgPool2d()`)，你都只要設 kernel_size 就好，stride 他會直接幫你愈設為 kernel_size。  \n",
    "  * 指定 shape 的 pooling\n",
    "    * 例如 Resnet 最後會用到的 global average pooling，就是 by 通道數，將整個 feature map 統整為 1 個值. \n",
    "    * 那就等於我指定 output shape 要是 (1,1) 的 pooling. \n",
    "    * 這可以用 `nn.AdaptiveAvgPool2d(output_size=(1, 1))` 來搞定  \n",
    "    * 那在 object detection 的 model 中，有些步驟會需要不管 input feature map 的 shape 是多少，統一幫我做成 (k, k) output 的 pooling 結果 (例如 input feature map 是 (224,224), 我希望做完 average pooling 後，可以得到 (8,8) 的結果。那我其實就想做 kernel_size = 28, stride = 28 的 pooling，但你用 nn.AdaptiveAvgPool2d((8,8))，他就會自動幫你算出 kernel_size = 28, stride = 28, 你就只要收割就好)\n",
    "---\n",
    "* 來上例子吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `nn.MaxPool2d()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 高寬減半的 pooling\n",
    "my_pool = nn.MaxPool2d(2) # kernel_size = 2, stride 預設就是 kernel_size, 所以 stride = 2\n",
    "my_pool(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `nn.AvgPool2d()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.5000,  4.5000],\n",
       "          [10.5000, 12.5000]]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 高寬減半的 pooling\n",
    "my_pool = nn.AvgPool2d(2) # kernel_size = 2, stride 預設就是 kernel_size, 所以 stride = 2\n",
    "my_pool(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### `nn.AdaptiveAvgPool2d()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[7.5000]]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global average pooling -> 希望的 output shape 是 (1,1)\n",
    "my_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "my_pool(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.5000,  4.5000],\n",
       "          [10.5000, 12.5000]]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定 output shape 為 (2,2) 的 average pooling\n",
    "my_pool = nn.AdaptiveAvgPool2d((2,2))\n",
    "my_pool(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### `nn.AdaptiveMaxPool2d()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定 output shape 為 (2,2) 的 max pooling\n",
    "my_pool = nn.AdaptiveMaxPool2d((2,2))\n",
    "my_pool(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9OhratU4L9Y"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRQUC1jl4L9d"
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3No7oWdD4L9d"
   },
   "source": [
    "### overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZKQZIon4L9d"
   },
   "source": [
    "* 官網連結: https://pytorch.org/docs/stable/nn.html#loss-functions  \n",
    "* 常用的整理："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJpYa7UJ4L9e"
   },
   "source": [
    "| loss function                    | `torch.nn as nn`                    | `torch.nn.functional as F` |\n",
    "|:--------------------------------:| ----------------------------------- | -------------------------- |\n",
    "| Binary cross entropy             | `nn.BCELoss()`                      | `F.binary_cross_entropy`   |\n",
    "| Binary cross entropy with logits | `nn.BCEWithLogitsLoss()`            | `F.binary_cross_entropy_with_logits` |\n",
    "| categorical cross entropy        | `nn.CrossEntropyLoss()`             | `F.relu`                   |\n",
    "| mse                              | `nn.MSELoss()`                      | `F.leaky_relu`             |\n",
    "| mae                              | `nn.L1Loss()`                       | `F.tanh`                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEOS33cC4L9e"
   },
   "source": [
    "* 概念講一下：\n",
    "  * loss 在統計的定義，是對 \"單一\" sample 算 loss，例如 square error loss = $(y_i - \\hat{y_i})^2$\n",
    "  * 然後 mse 是 cost，不是 loss，所以 mse cost = $\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y_i})^2$  \n",
    "  * 但在 pytorch/tensorflow 中，這兩個已經被混用了，都叫做 loss. \n",
    "  * 至於，如何區別這兩者呢？靠 class/function 中的參數定義來決定。  \n",
    "  * 例如： `my_mse = nn.MSELoss()`, 然後 `my_mse(y_hat_vector, y_true_vector)`，算出來的就是 mse cost. \n",
    "  * 但如果 `my_mse = nn.MSELoss(reduction = 'none')`, 然後 `my_mse(y_hat_vector, y_true_vector)`，算出來的是 n 維的 mse loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm1Ie_tU4L9e"
   },
   "source": [
    "* y 不一定要是向量，可以是矩陣或陣列：  \n",
    "  * 在統計上，學 loss 或 cost，都是從回歸的角度去學的，也就 向量 vs 向量， e.g. y = 100 維向量(100個sample)，y_hat 也是 100 維，那就可以算出 1 個 cost 和 100 個 loss. \n",
    "  * 但在 deep learning 裡面，y不一定是向量，y可以是矩陣，甚至多維陣列。\n",
    "  * 例如做 autoencoder 時\n",
    "    * y就是矩陣，比如 100 張圖片，每張圖片都是 8x8 的矩陣，那 y 可以定義成 (100, 8x8) 的 矩陣，(把圖片拉成 8x8 的向量)。\n",
    "    * y_hat 是這些影像 reconstruct 後的結果，所以也是 100 x 64 的矩陣。\n",
    "    * 那我照樣用剛剛定義好的 loss function，我就可以算出 1 個 cost 和 100x8x8 = 6400 個 loss。\n",
    "    * 所以關鍵在：他都是 `by element` 算 loss, 然後紀錄有多少 `個數`, 最後再用 `sum` 或 `mean` 回給你一個 cost。\n",
    "    * 這樣，就不需要管 y 的 shape 了。\n",
    "    * 例如：我這次不要把 8x8 拉成向量，所以 y 就是 (100, 8, 8) 的 array，那也無所謂，放入我的 loss function，他就可以算出 6400 個 loss，然後依照你的 reduction 的設定 (none or sum or mean)，回給你 6400 個 loss 或是 1 個 cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-ea1fdV4L9e"
   },
   "source": [
    "* batch loss 是拿來更新參數用的， epoch loss 是用來檢查有無 overfitting 的\n",
    "  * deep learning 在 training or testing 時，都是一個 batch 一個 batch 做，最後再整合成一個 epoch  \n",
    "  * 每個 batch 在做的時候，都要算這個 batch 的 cost，他的目的是用來更 gradient 時，要對這個 cost 做偏微分。所以每個 batch 結束，會得到一個 cost\n",
    "  * 每個 epoch 結束，要算的 cost 是跨所有樣本的。他的目的，是要去比較 training sample 的 cost 和 validation sample 的 cost，來判斷是否 overfitting 了，要不要做 early stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7x2Fnjh4L9e"
   },
   "source": [
    "* epoch loss 的算法設計.   \n",
    "  * 最常見的設計，是直接拿 batch cost 的結果來 summarise，因位省時省力：  \n",
    "    * 舉例來說，我有 10 個 batch，每個 batch 有 32 個 batch size. \n",
    "    * 每個 batch 結束時，其實都拿到該 batch 的 1 個 cost 或 32 個 loss. \n",
    "    * 那算 epoch cost 時，我就可以把 10 個 batch cost 取平均，或是 10x32 = 320 個 loss 取平均，就得到 epoch cost。\n",
    "    * pseudo code 就是  \n",
    "      * 先定義 `epoch_cost = 0`  \n",
    "      * 然後 for 迴圈去 loop 10 個 batch  \n",
    "      * 每次 batch 結束，就讓 `epoch_cost += batch_cost`. \n",
    "      * 迴圈結束後，用 epoch_cost / 10，得到 mean cost。\n",
    "      * 如果要用 loss 的寫法也很簡單。最外面就是定義 `loss_list = []`，然後每個回圈都是算出 batch_size 個 epoch_loss，然後 `loss_list.append(epoch_loss)`，最終再對 loss_list 取平均就好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrgCMpXn4L9f",
    "tags": []
   },
   "source": [
    "### mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sKEmsrV4L9f",
    "outputId": "1c68e431-c560-43d4-f360-71972d268657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6104,  0.4109,  1.3322,  1.2199, -0.6273], requires_grad=True)\n",
      "tensor([ 0.7016, -1.6683,  1.0668,  0.9080, -1.2761])\n"
     ]
    }
   ],
   "source": [
    "y_hat = torch.randn(5, requires_grad=True)\n",
    "y_true = torch.randn(5)\n",
    "print(y_hat)\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ip7_JjqP4L9f"
   },
   "source": [
    "#### class 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hp1oodqd4L9f",
    "outputId": "5425e20e-e33c-4226-a386-879772c3b5a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.3454, 4.3230, 0.0705, 0.0973, 0.4209], grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 算 loss\n",
    "my_loss = nn.MSELoss(reduction = \"none\")\n",
    "loss = my_loss(y_hat, y_true)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uMC33QP4L9g",
    "outputId": "fc06f4fd-c01a-4a46-e189-a05e1be7e7bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0514, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0514, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 算 cost (i.e mean loss)\n",
    "my_loss = nn.MSELoss()\n",
    "cost = my_loss(y_hat, y_true)\n",
    "print(cost)\n",
    "print(loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2njHObn4L9g",
    "outputId": "aaf4c131-c20b-4a05-8265-bbd3094fc83b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.2571, grad_fn=<MseLossBackward>)\n",
      "tensor(10.2571, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 算 cost (i.e sum loss)\n",
    "my_loss = nn.MSELoss(reduction = \"sum\")\n",
    "sum_cost = my_loss(y_hat, y_true)\n",
    "print(sum_cost)\n",
    "print(loss.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl-GKJ0N4L9g"
   },
   "source": [
    "#### function 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNwShra64L9g",
    "outputId": "98ac32bf-0d5d-4729-ed74-ddad594cc3f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7780, -0.4672, -1.0941, -1.0928, -1.0654], requires_grad=True)\n",
      "tensor([ 0.5094, -0.6637, -0.5560, -0.5600, -1.6072])\n",
      "tensor(0.5126, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 用 function 做 cost\n",
    "y_hat = torch.randn(5, requires_grad=True)\n",
    "y_true = torch.randn(5)\n",
    "\n",
    "print(y_hat)\n",
    "print(y_true)\n",
    "print(F.mse_loss(y_hat, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnFDY6354L9h",
    "outputId": "8784f3d4-d9e1-48aa-cf97-7c368cc279fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9525,  0.9356, -0.1469, -0.3822,  2.0675], requires_grad=True)\n",
      "tensor([-0.3021, -0.6323, -1.2846, -0.1762, -0.0629])\n",
      "tensor([0.4230, 2.4583, 1.2943, 0.0424, 4.5388], grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 用 function 做 loss\n",
    "y_hat = torch.randn(5, requires_grad=True)\n",
    "y_true = torch.randn(5)\n",
    "\n",
    "print(y_hat)\n",
    "print(y_true)\n",
    "print(F.mse_loss(y_hat, y_true, reduction=\"none\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHgYnmjw4L9h"
   },
   "source": [
    "### mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-nPmzMu4L9h"
   },
   "source": [
    "### binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ks-Q3HJv4L9h"
   },
   "outputs": [],
   "source": [
    "y_logit = torch.tensor([2.3552, -0.9071,  2.8323])\n",
    "y_hat = torch.tensor([0.9133, 0.2876, 0.9444]) # 就是 F.sigmoid(y_logit) 後的結果\n",
    "y = torch.tensor([0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIo5kVdk4L9i"
   },
   "source": [
    "* y 是 vector/matrix/array 都可 (常見是 vector，element 個數就是樣本數)，值不是 0 就是 1。 e.g. [0, 1, 0] 表示三個樣本的真值。\n",
    "* y_hat 的 shape 同 y，值介於 0~1 之間。e.g. [0.3, 0.8, 0.1]，表示三個樣本的預測值。\n",
    "* `binary cross entropy`： $-\\frac{1}{n}\\sum_{i = 1}^{n}\\left[ y_i log(\\hat{y_i}) + (1-y_i)(1-log(\\hat{y_i})\\right]$ \n",
    "* 這在這個定義式的中間這項就是 loss： $y_i log(\\hat{y_i}) + (1-y_i)(1-log(\\hat{y_i})$ 。可用 `reduction = \"none\"` 來設定，就可拿到 n 個 loss\n",
    "* 那算 cost，可以像定義式那樣，用平均來做，可用 `reduction = \"mean\"` 來設定。不寫也可，預設就是取 mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9x0jejd4L9i"
   },
   "source": [
    "#### class 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOqF1bJM4L9k",
    "outputId": "2251be81-76ef-4d99-c9e2-fadbae34b4a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4453, 0.3391, 2.8896])\n"
     ]
    }
   ],
   "source": [
    "# loss 版\n",
    "my_loss = nn.BCELoss(reduction = \"none\")\n",
    "loss = my_loss(y_hat, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obZG0qeD4L9k",
    "outputId": "3c59ee6b-ef1d-4a2b-9b12-af42b16e5f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8913)\n",
      "tensor(1.8913)\n"
     ]
    }
   ],
   "source": [
    "# cost 版\n",
    "my_loss = nn.BCELoss()\n",
    "cost = my_loss(y_hat, y)\n",
    "print(cost)\n",
    "print(loss.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVuqTBdu4L9k"
   },
   "source": [
    "#### function 版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RG7wJbLu4L9k",
    "outputId": "91a14819-2a76-4ee7-c561-61bdbf017bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4453, 0.3391, 2.8896])\n",
      "tensor(1.8913)\n"
     ]
    }
   ],
   "source": [
    "# loss 版\n",
    "print(F.binary_cross_entropy(y_hat, y, reduction = \"none\"))\n",
    "\n",
    "# cost 版\n",
    "print(F.binary_cross_entropy(y_hat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnV1YudC4L9l",
    "outputId": "b52dabe8-ad61-40fc-9845-b49fb1c26f3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8913)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自己照定義算\n",
    "-1*(torch.log(1-y_hat)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiRkrM5G4L9l"
   },
   "source": [
    "* 事實上，y和y_hat可以是任何shape，他都會幫你 by element 的去做 $ y_i log(\\hat{y_i}) + (1-y_i)(1-log(\\hat{y_i})$，然後最後取總平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K432Lc5r4L9l",
    "outputId": "6888de18-f669-4a58-e6e5-e2282e8a7dff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2420, 0.5219, 0.5408, 0.7095, 0.1231],\n",
      "        [0.3518, 0.2747, 0.9089, 0.8097, 0.4674],\n",
      "        [0.2304, 0.0615, 0.1389, 0.2419, 0.7572]])\n",
      "tensor([[1., 1., 0., 0., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "y_hat = torch.rand((3,5))\n",
    "y = np.random.randint(low = 0, high = 2, size = (3,5))\n",
    "y = torch.tensor(y, dtype = torch.float32)\n",
    "\n",
    "print(y_hat)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUSyaitf4L9l",
    "outputId": "81c0adbf-bad5-40a7-e884-a9ac9a2dc782"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7998)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XETBixQg4L9l"
   },
   "source": [
    "* 而且，y 也 不一定要是 0 or 1， y也可以是 0~1 的數，此時 binary entropy 就是在衡量 y 和 y_hat 的 distribution 像不像的一個指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFNaOGht4L9l",
    "outputId": "0c9580e6-203f-4e1a-cd72-e1d87f338220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1801, 0.4587, 0.9839, 0.5115, 0.7780],\n",
      "        [0.2146, 0.9854, 0.0592, 0.6360, 0.2658],\n",
      "        [0.6827, 0.2666, 0.0440, 0.6086, 0.8917]])\n",
      "tensor([[0.6732, 0.5078, 0.8481, 0.7030, 0.9484],\n",
      "        [0.7353, 0.8262, 0.2038, 0.2685, 0.1202],\n",
      "        [0.5659, 0.6011, 0.3651, 0.3918, 0.4598]])\n"
     ]
    }
   ],
   "source": [
    "y_hat = torch.rand((3,5))\n",
    "y = torch.rand((3,5))\n",
    "\n",
    "print(y_hat)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HloBQaGt4L9m",
    "outputId": "68185d3b-31a8-49a5-bd38-33f07a88e781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8157)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJPwkOiu4L9m"
   },
   "source": [
    "* 這其實就有用在 autoencoder 的 loss 上\n",
    "* y 是一張正規化後的影像(值都介於 0 到 1)，y_hat 是 reconstruct 後的影像，值也都介於 0 到 1 (因為最後有加 sigmoid)，此時算 loss 就可以用 binary_cross_entropy loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfekcdKh4L9m"
   },
   "source": [
    "### binary cross entropy with logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDg24NRy4L9m"
   },
   "outputs": [],
   "source": [
    "y_logit = torch.tensor([2.3552, -0.9071,  2.8323])\n",
    "y_hat = torch.tensor([0.9133, 0.2876, 0.9444]) # 就是 F.sigmoid(y_logit) 後的結果\n",
    "y = torch.tensor([0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkkrMe_Y4L9m"
   },
   "source": [
    "* $\\hat{y_i}$ 在轉成機率值前叫 logit (從 deep learning 的角度，就是還沒做 sigmoid 前的值; 從統計角度，sigmoid 在做的就是 logit transform 的逆變換)。  \n",
    "* 所以他會幫你把這個 $\\hat{y_i}$ 轉成 $\\frac{1}{1+e^{-\\hat{y_i}}}$ (這就是 sigmoid function 在做的事，也就是 logit transform 的逆變換)，再丟進去 binary cross entropy 裡面。\n",
    "* 補充以下以前學過的統計知識：  \n",
    "  * logit transform 是把 0 到 1 的機率值，轉成實數域。 e.g. p 介於 0 到 1， $y = log\\left(\\frac{p}{1-p}\\right)$，此時 y 介於 -無窮 到 +無窮，此時的 y 被稱為 logits\n",
    "  * logit transform 逆變換，是把時數域壓到 0 到 1 之間。 e.g. y 介於 -無窮 到 +無窮. $p = \\frac{1}{1+e^{-y}}$，此時 p 介於 0 到 1, 此時的 p 被解釋為機率值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpgkhGt84L9n"
   },
   "source": [
    "### cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqS_2zMv4L9n"
   },
   "source": [
    "* 先寫結論和用法，晚點補詳細的：  \n",
    "  * input 的 y_hat 必須是 logit (還沒經過 softmax), y可以是 [0,c) 的 integer，或是 one-hot encoding(必須是 float32，為了通用於 blended-label). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hhkl52E84L9n"
   },
   "source": [
    "* y_hat 需要是 logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_JWTUpy4L9n"
   },
   "outputs": [],
   "source": [
    "y_hat_logit_mat = np.array(\n",
    "    [[-2.3, 2, 1.5],\n",
    "     [-1, 2, 3]]\n",
    ")\n",
    "y_hat_logit_mat = torch.tensor(y_hat_logit_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSKr_kFv4L9o",
    "outputId": "843da3cd-4bc4-4fc9-9e45-2fb9525da897"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3000,  2.0000,  1.5000],\n",
       "        [-1.0000,  2.0000,  3.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_logit_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgWSIrwn4L9o"
   },
   "source": [
    "* 熟悉的 softmax 是這樣："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9h1yvaG4L9p",
    "outputId": "73474212-1d57-4ccc-8ef2-446eefe1cfb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0084, 0.6172, 0.3744],\n",
       "        [0.0132, 0.2654, 0.7214]], dtype=torch.float64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_mat = torch.nn.functional.softmax(y_hat_logit_mat, dim = 1)\n",
    "y_hat_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ufs9QDJh4L9q"
   },
   "source": [
    "* y 可以是 int 或 one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2GrY84D4L9q"
   },
   "outputs": [],
   "source": [
    "y_int = torch.tensor([1,1])\n",
    "y_one_hot = torch.tensor([[0,1,0],[0,1,0]], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwvuEdOC4L9q"
   },
   "source": [
    "* 算 cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3gFiXQW4L9q",
    "outputId": "272b3f67-254d-49fc-87e0-26991a4da1c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4825, 1.3266], dtype=torch.float64)\n",
      "tensor([0.4825, 1.3266], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "print(loss(y_hat_logit_mat, y_int))\n",
    "print(loss(y_hat_logit_mat, y_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaPyHokI4L9r",
    "outputId": "2ca07faf-9c8b-4105-d8fd-a5dbc48576d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9045, dtype=torch.float64)\n",
      "tensor(0.9045, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "print(loss(y_hat_logit_mat, y_int))\n",
    "print(loss(y_hat_logit_mat, y_one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pONWEvRH4L9r"
   },
   "source": [
    "### 自訂 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAswDLa_4L9r"
   },
   "source": [
    "### 對比學習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HhgkUXk4L9r"
   },
   "source": [
    "### autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjIMFaNK4L9r"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 官網很清楚： https://pytorch.org/docs/1.8.1/optim.html#how-to-adjust-learning-rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxGOncFe4L9r"
   },
   "source": [
    "### 建立 optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不同 learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXMcYqun4L9r"
   },
   "source": [
    "### learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ydCVEIN4L9r"
   },
   "source": [
    "## Training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8HxS-7_4L9s"
   },
   "source": [
    "### 完整版 (了解概念用)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4Qavpxp4L9s"
   },
   "source": [
    "* Data 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9RpJqOk4L9s"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "X = np.random.rand(1000, 100, 100, 1)   # 虛構 1000 張 100 x 100 單色圖片\n",
    "Y = np.random.randint(0, 7, [1000, 10]) # 虛構 1000 個 labels\n",
    "\n",
    "X, Y = X.astype(np.float32), Y.astype(np.float32)\n",
    "tsrX, tsrY = torch.tensor(X), torch.tensor(Y)\n",
    "tsrdataset = TensorDataset(tsrX, tsrY)\n",
    "\n",
    "tsrdataloader = DataLoader(\n",
    "    tsrdataset, batch_size=4,\n",
    "    shuffle=True, num_workers=4)\n",
    "\n",
    "# Validation\n",
    "vX = np.random.rand(100, 100, 100, 1)   # 虛構 100 張 100 x 100 單色圖片\n",
    "vY = np.random.randint(0, 7, [100, 10]) # 虛構 100 個 labels\n",
    "\n",
    "vX, vY = vX.astype(np.float32), vY.astype(np.float32)\n",
    "vtsrX, vtsrY = torch.tensor(vX), torch.tensor(vY)\n",
    "vtsrdataset = TensorDataset(tsrX, tsrY)\n",
    "\n",
    "vtsrdataloader = DataLoader(\n",
    "    vtsrdataset, batch_size=4,\n",
    "    shuffle=False, num_workers=4) # Validation 不需要 shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io1Ey6mJ4L9s"
   },
   "source": [
    "* model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBRu5MWk4L9s"
   },
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(10000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # 傳入 model 的函數會經過 forward 做 inference\n",
    "        # x = x.view(x.size(0), -1) # flatten 的意思，原本的 x.size = (batch_size, 100, 100, 1) -> 改成 (batch_size, 100*100*1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iry9GjJB4L9s"
   },
   "source": [
    "* 確定 device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wma47Zft4L9s",
    "outputId": "3dd82151-0556-42a0-b13c-f759d85c9bd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBeH2CER4L9t"
   },
   "outputs": [],
   "source": [
    "# model structure\n",
    "simpleNN = SimpleNN()\n",
    "simpleNN.to(device)                           # 把 model 移到 GPU 計算\n",
    "\n",
    "# optimizer\n",
    "optim = torch.optim.Adam(\n",
    "    simpleNN.parameters(), lr=1e-4)\n",
    "\n",
    "# loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWxqWVU-4L9t"
   },
   "source": [
    "* tensorboard 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oY9KV_Dm4L9t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# tensorboard setting\n",
    "%load_ext tensorboard\n",
    "\n",
    "logs_base_dir = \"runs\" # training 的紀錄，放在這個路徑下\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQN8RxJG4L9t"
   },
   "source": [
    "* 開始 Training， 本質就是跑一個迴圈，在每一次（叫一個 **epoch**）要做的事有——\n",
    "  1. 載入資料\n",
    "  2. 經過 model 跑一次\n",
    "  3. 比對資料的正確性，算誤差（loss）\n",
    "  4. 把梯度清掉，然後根據這次誤差算新的梯度\n",
    "  5. 根據 optimizer 更新參數\n",
    "  6. 為了方便觀察，將本次 epoch 訓練的變化顯示出來，包括\n",
    "     - 進度條（觀察訓練快慢）\n",
    "     - batch loss （這個有時候會輸出太多東西）\n",
    "     - epoch loss （記得累計並除掉資料數量）\n",
    "     - 記錄到其他變數中（方便作圖）\n",
    "     - 記錄到 Tensorboard 中（SummaryWriter）\n",
    "\n",
    "* 為了避免 overfit，我們每個 epoch 還會進行一次 validation，事情少一些，變成——\n",
    "  1. 載入資料\n",
    "  2. 經過 model 跑一次\n",
    "  3. 比對資料的正確性，算誤差（loss）\n",
    "  4. 為了方便觀察，將本次 epoch validate 的結果顯示出來，包括\n",
    "     - 進度條（觀察訓練快慢）\n",
    "     - batch loss （這個有時候會輸出太多東西）\n",
    "     - epoch loss （記得累計並除掉資料數量）\n",
    "     - 記錄到其他變數中（方便作圖）\n",
    "     - 記錄到 Tensorboard 中（SummaryWriter）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWzueQdz4L9t"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {logs_base_dir} # 開啟 tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhG8zkLP4L9t",
    "outputId": "0fb3e03b-3512-480d-bf67-4ac958fa495e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   Epoch  1: Loss = 1.0920\n",
      "Validation Epoch  1: Loss = 0.9851\n",
      "Training   Epoch  2: Loss = 0.9812\n",
      "Validation Epoch  2: Loss = 0.9269\n",
      "Training   Epoch  3: Loss = 0.9155\n",
      "Validation Epoch  3: Loss = 0.8980\n",
      "Training   Epoch  4: Loss = 0.8593\n",
      "Validation Epoch  4: Loss = 0.7541\n",
      "Training   Epoch  5: Loss = 0.7777\n",
      "Validation Epoch  5: Loss = 0.6951\n",
      "Training   Epoch  6: Loss = 0.7110\n",
      "Validation Epoch  6: Loss = 0.6363\n",
      "Training   Epoch  7: Loss = 0.6274\n",
      "Validation Epoch  7: Loss = 0.5730\n",
      "Training   Epoch  8: Loss = 0.5564\n",
      "Validation Epoch  8: Loss = 0.4740\n",
      "Training   Epoch  9: Loss = 0.4752\n",
      "Validation Epoch  9: Loss = 0.3994\n",
      "Training   Epoch 10: Loss = 0.3927\n",
      "Validation Epoch 10: Loss = 0.3577\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    # training step (training data)\n",
    "    simpleNN.train() # 切換 simpleNN 為 training 模式，dropout 之類的操作會開啟\n",
    "    epoch_loss = 0.0\n",
    "    for x, y in tsrdataloader:\n",
    "        y_hat = simpleNN(x.to(device))        # 把 x tensor 移到 GPU 計算\n",
    "        loss = criterion(y.to(device), y_hat) # 把 y tensor 移到 GPU 計算，\n",
    "                                              ##  y_hat 因為是從 GPU model input GPU Tensor 出來的\n",
    "                                              ##  所以不用再次 .to(device) 當然要也是沒差啦 =_=|||\n",
    "        optim.zero_grad() # 把 trainable variable/weights/parameters 的 gradient 給 歸 0\n",
    "        loss.backward() # 利用 loss，計算出每個 trainable variable/weights/parameters 所對應的 gradient\n",
    "        optim.step() # 更新 trainable variable/weights/parameters 的值： parameters_new = parameters_old - learning_rate * gradient\n",
    "        epoch_loss += loss.item()\n",
    "    average_epoch_loss = epoch_loss / len(tsrdataset)\n",
    "    print(f\"Training   Epoch {epoch + 1:2d}: Loss = {average_epoch_loss:.4f}\")\n",
    "    tb.add_scalar(\"Loss/train\", average_epoch_loss, epoch + 1) # 寫進 tensorboard\n",
    "    \n",
    "\n",
    "    # evaluation step (validation data)\n",
    "    simpleNN.eval() # 將 simpleNN 切換到 evaluation mode， dropout 之類的操作會關閉\n",
    "    vepoch_loss = 0.0\n",
    "    for x, y in vtsrdataloader:\n",
    "        y_hat = simpleNN(x.to(device))\n",
    "        loss = criterion(y.to(device), y_hat)\n",
    "        vepoch_loss += loss.item()\n",
    "    vaverage_epoch_loss = vepoch_loss / len(vtsrdataset)\n",
    "    print(f\"Validation Epoch {epoch + 1:2d}: Loss = {vaverage_epoch_loss:.4f}\")\n",
    "    tb.add_scalar(\"Loss/val\", vaverage_epoch_loss, epoch + 1) # 寫進 tensorboard\n",
    "tb.close() # 加這個"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qaS4EmV4L9u"
   },
   "source": [
    "### 模組版 (實際做實驗, deploy 時用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-lBCVh14L9u"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):  #@save\n",
    "    \"\"\"計算預測正確的數量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlZJRlsg4L9u"
   },
   "outputs": [],
   "source": [
    "# 計算 metric 時用的\n",
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个變量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n # [0.0, 0.0, ..., 0.0], 共 n 個 0.0\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-s7sWEV4L9u"
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter):  #@save\n",
    "    \"\"\"計算在指定數據集上，模型的準確度\"\"\"\n",
    "    if isinstance(model, torch.nn.Module):\n",
    "        model.eval()  # 将模型设置为评估模式\n",
    "    metric = Accumulator(2)  # 正确预测数、预测总数\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBnPalve4L9u"
   },
   "outputs": [],
   "source": [
    "# 單一 epoch 裡要做的事\n",
    "def train_epoch(model, train_iter, loss, optimizer):  #@save\n",
    "    \n",
    "    model.train()\n",
    "    # 訓練損失總和、訓練準確度總和、樣本數\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        y_hat = model(X)\n",
    "        l = loss(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.mean().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    # 返回訓練損失 & 訓練準確度\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CT31ehqb4L9u"
   },
   "outputs": [],
   "source": [
    "class Animator:  #@save\n",
    "    \"\"\"在動畫中繪製數據\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: d2l.set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrGUZb1E4L9v"
   },
   "outputs": [],
   "source": [
    "def train(model, train_iter, valid_iter, loss, num_epochs, optimizer):  #@save\n",
    "    #animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "    #                    legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch(model, train_iter, loss, updater)\n",
    "        valid_acc = evaluate_accuracy(model, valid_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (valid_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert valid_acc <= 1 and valid_acc > 0.7, valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdbpVFeZ4L9v"
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1BId39k4L9v"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_93ERddN4L9v"
   },
   "source": [
    "* data 準備 (testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0z8Q71904L9v"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "tX = np.random.rand(100, 100, 100, 1)   # 虛構 100 張 100 x 100 單色圖片\n",
    "tY = np.random.randint(0, 7, [100, 10]) # 虛構 100 個 labels\n",
    "\n",
    "tX, tY = tX.astype(np.float32), tY.astype(np.float32)\n",
    "ttsrX, ttsrY = torch.tensor(tX), torch.tensor(tY)\n",
    "ttsrdataset = TensorDataset(tsrX, tsrY)\n",
    "\n",
    "ttsrdataloader = DataLoader(\n",
    "    ttsrdataset, batch_size=4,\n",
    "    shuffle=False, num_workers=4) # Testing 不需要 shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y86AgtMd4L9w"
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "simpleNN.eval()\n",
    "y_hat = [simpleNN(x) for x, y in ttsrdataloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9M5xpWNl4L9w"
   },
   "outputs": [],
   "source": [
    "def predict(model, test_iter, n=6):  #@save\n",
    "    for X, y in test_iter:\n",
    "        break\n",
    "    trues = d2l.get_fashion_mnist_labels(y)\n",
    "    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))\n",
    "    titles = [true +'\\n' + pred for true, pred in zip(trues, preds)]\n",
    "    d2l.show_images(\n",
    "        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])\n",
    "\n",
    "predict_ch3(net, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9K_XAaZ4L9w"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqb0XiGk4L9w"
   },
   "source": [
    "## Save/ load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只存 weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假設我們 train 好的 model 是 VGG\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# 只存 weight\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# 之後做預測，要先 initialize model\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# load weight\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n",
    "# 開始做 inference\n",
    "model.eval() # 關閉 batch normalization layer, dropout layer 等\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 存 weight 和 model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如果把 model structure 和 weight 一起存起來，就可以 load 後，直接 predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# 存檔時，存整個 model\n",
    "torch.save(model, 'model.pth')\n",
    "\n",
    "# 讀檔時，直接讀整個 model\n",
    "model = torch.load('model.pth') # 不需要 initialize 一個 model，再去讀 weight 了\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNInaCoS4L9w"
   },
   "source": [
    "### checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n",
    "* 看官網這篇就 ok 了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-3knBui4L9w"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "simpleNN.cpu() # 先移回 CPU\n",
    "torch.save(simpleNN.state_dict(), \"randmodel.model\")\n",
    "\n",
    "# Load model\n",
    "model2 = SimpleNN()\n",
    "model2.load_state_dict(torch.load(\"randmodel.model\"))\n",
    "\n",
    "# 確認是同一個 model\n",
    "torch.equal(model2(x), simpleNN(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 修改自：https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "        self.if_break = False\n",
    "\n",
    "    def monitor(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "        self.if_break = True if self.counter >= self.patience else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 然後這樣用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "for epoch in np.arange(n_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader)\n",
    "    validation_loss = validate_one_epoch(model, validation_loader)\n",
    "    \n",
    "    early_stopper.monitor(validation_loss)\n",
    "    if early_stopper.counter == 0:\n",
    "        torch.save(model, 'model.pth')\n",
    "    if early_stopper.if_break:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BoTTxEi4L9w"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tensorboard 參考這篇：\n",
    "  * https://pytorch.org/docs/stable/tensorboard.html\n",
    "  * https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEi7Oqoi4L9x"
   },
   "source": [
    "## Explaianation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXBgYeF64L9z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}