

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>08. Milestone Project 2: PyTorch Paper Replicating &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'classical_network/vision_transformer/08_pytorch_paper_replicating_video';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    <p class="title logo__title">My sample book</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Cheat Sheet</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../cheatsheet/pytorch_cheatsheet.html">1. Pytorch Cheatsheet</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Classical Network</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../vgg/vgg16.html">2. VGG16</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resnet/resnet.html">3. Resnet</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fclassical_network/vision_transformer/08_pytorch_paper_replicating_video.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/classical_network/vision_transformer/08_pytorch_paper_replicating_video.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>08. Milestone Project 2: PyTorch Paper Replicating</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-setup">0. Get setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-data">1. Get data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-datasets-and-dataloaders">2. Create Datasets and DataLoaders</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-a-single-a-image">2.3 Visualize a single a image</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#replicating-vit-overview">3. Replicating ViT: Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vit-overview-pieces-of-the-puzzle">3.1 ViT overview: pieces of the puzzle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#figure-1">Figure 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#four-equations">Four equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-1-describes-the-various-equations">Section 3.1 describes the various equations:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#table-1">Table 1</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding">4. Equation 1: Split data into patches and creating the class, position and patch embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-input-and-output-shapes-by-hand">4.1 Calculate input and output shapes by hand</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turning-a-single-image-into-patches">4.2 Turning a single image into patches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-image-patches-and-turning-them-into-patch-embeddings">4.3 Creating image patches and turning them into patch embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flattening-the-patch-embedding-with-torch-nn-flatten">4.4 Flattening the patch embedding with <code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turning-the-vit-patch-embedding-layer-into-a-pytorch-module">4.5 Turning the ViT patch embedding layer into a PyTorch module</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-class-token-embedding">4.6 Creating the class token embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-position-embedding">4.7 Creating the position embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together-from-image-to-embedding">4.8 Putting it all together: from image to embedding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-2-multihead-self-attention-msa-block">Equation 2: Multihead Self-Attention (MSA block)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-3-multilayer-perceptron-mlp-block">6. Equation 3: Multilayer Perceptron (MLP block)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-transformer-encoder">7. Creating the Transformer Encoder</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-custom-transformer-encoder-block">7.1 Create a custom Transformer Encoder block</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-transformer-encoder-layer-with-in-built-pytorch-layers">7.2 Create a Transformer Encoder layer with in-built PyTorch layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together-to-create-vit">8. Putting it all together to create ViT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-visual-summary-of-our-vit-model">8.1 Getting a visual summary of our ViT model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-training-code-for-our-custom-vit">9. Setting up training code for our custom ViT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-an-optimizer">9.1 Creating an optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-loss-function">9.2 Creating a loss function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-our-vit-model">9.3 Training our ViT Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-our-training-setup-is-missing">9.4 What our training setup is missing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-loss-curves-for-our-model">9.5 Plotting loss curves for our model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-a-pretrained-vit-from-torchvision-models">10. Using a pretrained ViT from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-a-pretrained-model">10.1 Why use a pretrained model?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-a-pretrained-vit-for-use-with-foodvision-mini-turn-it-into-a-feature-extractor">10.2 Prepare a pretrained ViT for use with FoodVision Mini (turn it into a feature extractor)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-data-for-the-pretrained-vit-model">10.3 Preparing data for the pretrained ViT model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-feature-extractor-vit-model">10.4 Train feature extractor ViT model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-loss-curves-of-our-pretrained-vit-feature-extractor-model">10.5 Plot the loss curves of our pretrained ViT feature extractor model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-our-best-performing-vit-model">10.6 Save our best performing ViT model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-on-a-custom-image">11. Predicting on a custom image</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises-and-extra-curriculum">Exercises and extra-curriculum</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><a href="https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/video_notebooks/08_pytorch_paper_replicating_video.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="milestone-project-2-pytorch-paper-replicating">
<h1>08. Milestone Project 2: PyTorch Paper Replicating<a class="headerlink" href="#milestone-project-2-pytorch-paper-replicating" title="Permalink to this heading">#</a></h1>
<p>這篇的重點，是要將 Vision Transformer (ViT, <a class="reference external" href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>) 這篇 paper，用 pytorch 重現</p>
<section id="get-setup">
<h2>0. Get setup<a class="headerlink" href="#get-setup" title="Permalink to this heading">#</a></h2>
<p>Let’s import code we’ve previously written + required libraries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from google.colab import drive
drive.mount(&#39;/content/drive&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import os
os.chdir(&quot;/content/drive/MyDrive/0. codepool_python/pytorch_udemy&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+
try:
    import torch
    import torchvision
    assert int(torch.__version__.split(&quot;.&quot;)[1]) &gt;= 12, &quot;torch version should be 1.12+&quot;
    assert int(torchvision.__version__.split(&quot;.&quot;)[1]) &gt;= 13, &quot;torchvision version should be 0.13+&quot;
    print(f&quot;torch version: {torch.__version__}&quot;)
    print(f&quot;torchvision version: {torchvision.__version__}&quot;)
except:
    print(f&quot;[INFO] torch/torchvision versions not as required, installing nightly versions.&quot;)
    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113
    import torch
    import torchvision
    print(f&quot;torch version: {torch.__version__}&quot;)
    print(f&quot;torchvision version: {torchvision.__version__}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch version: 1.13.1+cu116
torchvision version: 0.14.1+cu116
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Continue with regular imports
import matplotlib.pyplot as plt
import torch
import torchvision

from torch import nn
from torchvision import transforms

# Try to get torchinfo, install it if it doesn&#39;t work
try:
    from torchinfo import summary
except:
    print(&quot;[INFO] Couldn&#39;t find torchinfo... installing it.&quot;)
    !pip install -q torchinfo
    from torchinfo import summary

# Try to import the going_modular directory, download it from GitHub if it doesn&#39;t work
try:
    from going_modular.going_modular import data_setup, engine
    from helper_functions import download_data, set_seeds, plot_loss_curves
except:
    # Get the going_modular scripts
    print(&quot;[INFO] Couldn&#39;t find going_modular or helper_functions scripts... downloading them from GitHub.&quot;)
    !git clone https://github.com/mrdbourke/pytorch-deep-learning
    !mv pytorch-deep-learning/going_modular .
    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script
    !rm -rf pytorch-deep-learning
    from going_modular.going_modular import data_setup, engine
    from helper_functions import download_data, set_seeds, plot_loss_curves
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] Couldn&#39;t find torchinfo... installing it.
[INFO] Couldn&#39;t find going_modular or helper_functions scripts... downloading them from GitHub.
Cloning into &#39;pytorch-deep-learning&#39;...
remote: Enumerating objects: 3435, done.
remote: Counting objects: 100% (133/133), done.
remote: Compressing objects: 100% (87/87), done.
remote: Total 3435 (delta 55), reused 97 (delta 41), pack-reused 3302
Receiving objects: 100% (3435/3435), 643.58 MiB | 19.11 MiB/s, done.
Resolving deltas: 100% (1962/1962), done.
Updating files: 100% (222/222), done.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Setup device agnostic code
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
device
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cuda&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>!nvidia-smi
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sun Feb 26 03:58:23 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   47C    P0    28W /  70W |    571MiB / 15360MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
</pre></div>
</div>
</div>
</div>
</section>
<section id="get-data">
<h2>1. Get data<a class="headerlink" href="#get-data" title="Permalink to this heading">#</a></h2>
<p>我們要用 food 資料集 (裡面有 pizze, steak, sushi 三類)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Download pizza, steak, sushi images from GitHub
image_path = download_data(source=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip&quot;,
                           destination=&quot;pizza_steak_sushi&quot;)
image_path
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] Did not find data/pizza_steak_sushi directory, creating one...
[INFO] Downloading pizza_steak_sushi.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip...
[INFO] Unzipping pizza_steak_sushi.zip data...
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PosixPath(&#39;data/pizza_steak_sushi&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Setup directory paths to train and test images
train_dir = image_path / &quot;train&quot;
test_dir = image_path / &quot;test&quot;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train_dir, test_dir
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(PosixPath(&#39;data/pizza_steak_sushi/train&#39;),
 PosixPath(&#39;data/pizza_steak_sushi/test&#39;))
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-datasets-and-dataloaders">
<h2>2. Create Datasets and DataLoaders<a class="headerlink" href="#create-datasets-and-dataloaders" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from torchvision import transforms
from going_modular.going_modular import data_setup

# Create image size 
IMG_SIZE = 224 # comes from Table 3 of the ViT paper

# Create transforms pipeline
manual_transforms = transforms.Compose([
                                        transforms.Resize((IMG_SIZE, IMG_SIZE)),
                                        transforms.ToTensor()
])

print(f&quot;Manually created transforms: {manual_transforms}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Manually created transforms: Compose(
    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)
    ToTensor()
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create a batch size of 32 (gthe paper uses 4096 but this may be too big for our smaller hardware... can always scale up later)
BATCH_SIZE = 32 

# Create DataLoaders
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(
    train_dir=train_dir,
    test_dir=test_dir,
    transform=manual_transforms,
    batch_size=BATCH_SIZE
)

len(train_dataloader), len(test_dataloader), class_names
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(8, 3, [&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;])
</pre></div>
</div>
</div>
</div>
<section id="visualize-a-single-a-image">
<h3>2.3 Visualize a single a image<a class="headerlink" href="#visualize-a-single-a-image" title="Permalink to this heading">#</a></h3>
<p>As always, let’s adhere to the motto, <em>visualize, visualize, visualize</em>!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get a batch of images
image_batch, label_batch = next(iter(train_dataloader))

# Get a single image and label from the batch
image, label = image_batch[0], label_batch[0]

# View the single image and label shapes
image.shape, label
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([3, 224, 224]), tensor(1))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Plot the image with matplotlib
import matplotlib.pyplot as plt

plt.imshow(image.permute(1, 2, 0)) # (color_channels, height, width) -&gt; (height, width, color_channels)
plt.title(class_names[label])
plt.axis(False);
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c947ee3c107d70c1769da42acbb1e1a7e42506767422fe3cb813beea4ae72cb6.png" src="../../_images/c947ee3c107d70c1769da42acbb1e1a7e42506767422fe3cb813beea4ae72cb6.png" />
</div>
</div>
</section>
</section>
<section id="replicating-vit-overview">
<h2>3. Replicating ViT: Overview<a class="headerlink" href="#replicating-vit-overview" title="Permalink to this heading">#</a></h2>
<p>要去 implement 一篇 paper，要抓住以下四個重點，就不會 overwhelm:</p>
<ul class="simple">
<li><p><strong>Inputs</strong> - What goes into the model? (in our case, image tensors)</p></li>
<li><p><strong>Outputs</strong> - What comes out of the model/layer/block? (in our case, we want the model to output image classification labels)</p></li>
<li><p><strong>Layers</strong> - Takes an input, manipulates it with a function (for example could be self-attention).</p></li>
<li><p><strong>Blocks</strong> - A collection of layers.</p></li>
<li><p><strong>Model (or architecture)</strong> - A collection of blocks.</p></li>
</ul>
<section id="vit-overview-pieces-of-the-puzzle">
<h3>3.1 ViT overview: pieces of the puzzle<a class="headerlink" href="#vit-overview-pieces-of-the-puzzle" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Figure 1: Visual overview of the architecture</p></li>
<li><p>Four equations: math equations which define the functions of each layer/block</p></li>
<li><p>Table 1/3: different hyperparameters for the architecture/training.</p></li>
<li><p>Text descriptions (especially section 3.1)</p></li>
</ul>
</section>
<section id="figure-1">
<h3>Figure 1<a class="headerlink" href="#figure-1" title="Permalink to this heading">#</a></h3>
<img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-figure-1-architecture-overview.png" width=600 alt="figure 1 from vision transformer paper"/>
<ul class="simple">
<li><p>這邊我先把所有重點快速講一遍(尤其是 shape 的部分)</p></li>
<li><p>input:</p>
<ul>
<li><p>首先，一張 img 進來, shape 為 (224, 224, 3).</p></li>
<li><p>第一段要做的，是把影像，變成 sequence, 這個 sequence 有 N + 1 個 element，其中：</p>
<ul>
<li><p>N 是指 N 個 patch。以此圖為例，原圖被切成 9 個 patch，所以 N = 9.</p></li>
<li><p>每個 patch，我都要把他從 shape = (p, p, 3) 的影像，轉成 n_embedding 維的向量。e.g. 764 維的向量。這就是圖中的粉紅橢圓的意思。至於怎麼做？就是先把此 patch 從 shape = (p,p,3) ，flatten 成向量，再餵入一個 Linear layer (這邊叫他 embedding layer)，輸出 764 維，就搞定這張 patch img 的 representation 了。</p></li>
<li><p>N + 1 的 1，是指第一張 patch 之前，加了一個 start token，這個 start token 被命名為 class token。他也是個 764 維的向量，一開始是隨機產生，但是是可學習的。</p>
<ul>
<li><p>這邊觀察一下，其實就跟語言 model 一樣，語言 model 都會有一個 start token，然後接下來的 N 個 sequence，其實就是一句話裡面依序出來的 N 個 token。</p></li>
<li><p>也因為語言模型中，不管你今天是講哪句話，前面的 start token 都一樣，都只是 start 而已，所以他的 embedding 也應該跨不同語句都一樣</p></li>
<li><p>那同樣的，在 image 中也是這樣，這個最開始的 start token / class token，其實從頭到尾就是 n_embedding 維的向量，不會每張 image 出來都要有自己的 start token</p></li>
</ul>
</li>
<li><p>小結一下，當一張原始影像餵進來後，現在會被轉成 (N + 1) 個 element 的 sequence。且每個 element，都是 764 維的 embedding。</p>
<ul>
<li><p>第一個 embedding (start token / class token)，是先 random 產出一個 764 維的向量，後續逐步學習後產生。且這個 embedding 是跨所有影像都是一樣的.</p></li>
<li><p>N 個 patch 的 embedding，是透過 Linear projection of flattened patches 得來的。所以要學習的就是這個 linear layer，他的 input 是 flatten 後的 patch image (i.e. PxPx3 維)，output 是 n_embedding 維 (i.e. 764).</p></li>
</ul>
</li>
<li><p>接著，positional embedding (紫色)，也是生出 (N + 1) 個 element 的 sequence，每個 element 也都是 764 維的向量。他代表的意義，是這個 patch 所處的 position。</p>
<ul>
<li><p>這邊講一下，最簡單的 embedding 就是不需學習的 one-hot embedding。</p></li>
<li><p>如果 (N+1) = 10，那就生個 10 維的向量，第1個position 的 embedding 就是 (1,0,…,0)，第 2 個position 的 embedding 就是 (0,1,0,…,0) 以此類推。</p></li>
<li><p>但這樣做有兩個問題：</p>
<ul>
<li><p>第一，太簡單，這樣我只知道 position 0 在 position 1 的前面，但我不知道其實在空間中， position 0 在 position 1 的左邊，在 position 3 的上面，在 position 4 的左上角，在 position 8 更遠的左上角…等。</p></li>
<li><p>第二，這個維度和剛剛做出來的維度，size 不同 (10 vs 764)，所以不容易做相加的運算。</p></li>
</ul>
</li>
<li><p>所以，我希望 position embedding 也是 “可以學習” 的，而且維度要和剛剛的一樣 (i.e. 764)。所以這邊就會先 random 生出 shape 為(N+1, n_embedding) 的矩陣，每個 column 就是該 patch 的 position representation (i.e. position embedding).</p></li>
<li><p>最後，特別注意，因為 position embedding 是 “跨 image 都一樣的”，因為不管你是哪張 image ，切成 patch 後，第 i 個 patch 的 position 意義一樣。所以，這個 position embedding 實際上就是 shape 為 (N+1, n_embedding) 的可學習的矩陣。</p></li>
</ul>
</li>
<li><p>最後，會把剛剛的 (N + 1) 個 embedding，和 (N + 1) 個 positional embedding，做相加。所以，仍然是 (N+1)個 embedding，但裡面融合了 patch img 的 embedding 和 positional embedding (你可以理解為，這個 embedding 融合了 patch img 的內容訊息和位置訊息).</p></li>
</ul>
</li>
</ul>
</li>
<li><p>output:</p>
<ul>
<li><p>經過剛剛的處理，我們現在統一口徑，一張影像餵進來時，他的 input 會變成一個 sequence，這個 sequence 有 N+1 個 element/token, 每個 element 都是 n_embedding 的向量。</p></li>
<li><p>接著，這個 input 會被餵進灰色的 transformer encoder。</p></li>
<li><p>他的 output，會和 input 維持一模一樣的 shape。也就是一樣是一個 sequence，裡面有 N+1 個 element，每個 element 都是 n_embedding 維的向量。</p></li>
<li><p>也正因為如此，我們才能把 trandformer encoder 重複做 L 次，L 任選.</p></li>
<li><p>當做到最後一次時，就要準備做預測了，這邊注意：</p>
<ul>
<li><p>只取 output 的 sequence 的 第0個 element 出來，所以此時是 n_embedding 維的向量.</p></li>
<li><p>經過 MLP (Multi-layer perceptron)，輸出為 n_class 維的向量，這就是預測結果了。</p></li>
<li><p>可想而知，我們可以用 cross entropy 當 loss 來做 training</p></li>
</ul>
</li>
</ul>
</li>
<li><p>layer, block, model:</p>
<ul>
<li><p>我把 layer 理解成 torch.module。在這張圖中，我們有四種顏色的 layer/module 要做：</p>
<ul>
<li><p>前處理時的 embedding layer (粉紅色)，要去做出可以 input 一張 patch img，output 出一個 n_embedding 維的向量 的 layer.</p></li>
<li><p>layer normalization (黃色)，這個其實 pytorch 有現成的了，就叫 torch.nn.LayerNorm.</p></li>
<li><p>Multi-head attention (綠色)，這個其實 pytorch 也有現成的了，就叫 torch.nn.MulltiheadAttention.</p></li>
<li><p>MLP (藍色): 就是簡單的 multi-layer perceptron。裡面的結構，就是 linear-&gt;activation-&gt;drop_out-&gt;linear -&gt; activation -&gt; drop_out。要注意的是，最後 output 的維度，要和input一樣即可.</p></li>
<li><p>MLP head (橘色)：這是最終的 classification layer，從 paper 來看，他是做 layer_normalization -&gt; linear_classifier。</p></li>
</ul>
</li>
<li><p>block:</p>
<ul>
<li><p>我們會把原始影像進來，到產出 N+1 sequence 的過程，包成一個 block。裡面包括剛剛定義好的 embedding layer，以及 start token, position encoding 這些</p></li>
<li><p>另外，最主要要做的 block，就是灰色的 transformer encoder。</p>
<ul>
<li><p>作法蠻簡單的，就是把剛剛的 norm_layer, multihead_attention, mlp 給串在一起。</p></li>
<li><p>其實，pytorch 內建也有了，就叫 <code class="docutils literal notranslate"><span class="pre">torch.nn.TransformerEncoderLayer()</span></code>，argument 就可以定義 multi-head attention 的參數, MLP 的參數等，就幫你做出整個 transformer encoder.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>model: 把剛剛的所有拼圖拼起來，</p>
<ul>
<li><p>前處理的 part: 原始影像進來 -&gt; (N + 1) sequence 出去.</p></li>
<li><p>transformer encoder 的 part: (N+1) sequence 進來， (N+1) sequence 出去.</p></li>
<li><p>重複 L 次 transformer encoding.</p></li>
<li><p>MLP head: 把最後一次的 output，的第0個element抓出來，進 classifier，output 出 class probability.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Embedding = learnable representation (start with random numbers and improve over time)</p></li>
</ul>
</section>
<section id="four-equations">
<h3>Four equations<a class="headerlink" href="#four-equations" title="Permalink to this heading">#</a></h3>
<img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-four-equations.png" width=600 alt="four equations from vision transformer paper"/>
<section id="section-3-1-describes-the-various-equations">
<h4>Section 3.1 describes the various equations:<a class="headerlink" href="#section-3-1-describes-the-various-equations" title="Permalink to this heading">#</a></h4>
<p><strong>Equation 1:</strong>
An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle <span class="math notranslate nohighlight">\(2 \mathrm{D}\)</span> images, we reshape the image <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{H \times W \times C}\)</span> into a sequence of flattened <span class="math notranslate nohighlight">\(2 \mathrm{D}\)</span> patches <span class="math notranslate nohighlight">\(\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}\)</span>, where <span class="math notranslate nohighlight">\((H, W)\)</span> is the resolution of the original image, <span class="math notranslate nohighlight">\(C\)</span> is the number of channels, <span class="math notranslate nohighlight">\((P, P)\)</span> is the resolution of each image patch, and <span class="math notranslate nohighlight">\(N=H W / P^{2}\)</span> is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size <span class="math notranslate nohighlight">\(D\)</span> through all of its layers, so we flatten the patches and map to <span class="math notranslate nohighlight">\(D\)</span> dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.</p>
<p><strong>Equation 1:</strong>
Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable <span class="math notranslate nohighlight">\(1 \mathrm{D}\)</span> position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.</p>
<p>In pseudocode:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Equation 1</span>
<span class="n">x_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">class_token_embedding</span><span class="p">,</span> <span class="n">image_patch_1_embedding</span><span class="p">,</span> <span class="n">image_patch_2_embedding</span><span class="p">,</span> <span class="o">...</span> <span class="n">image_patch_N_embedding</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">class_token_pos_embedding</span><span class="p">,</span> <span class="n">image_patch_1_pos_embedding</span><span class="p">,</span> <span class="n">image_patch_2_pos_embedding</span><span class="p">,</span> <span class="o">...</span> <span class="n">image_patch_N_pos_embedding</span><span class="p">]</span>
</pre></div>
</div>
<hr class="docutils" />
<p><strong>Equations 2&amp;3:</strong>
The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski &amp; Auli, 2019).</p>
<p>In pseudocode:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Equation 2 </span>
<span class="n">x_output_MSA_block</span> <span class="o">=</span> <span class="n">MSA_layer</span><span class="p">(</span><span class="n">LN_layer</span><span class="p">(</span><span class="n">x_input</span><span class="p">))</span> <span class="o">+</span> <span class="n">x_input</span>

<span class="c1"># Equation 3 </span>
<span class="n">x_output_MLP_block</span> <span class="o">=</span> <span class="n">MLP_layer</span><span class="p">(</span><span class="n">LN_layer</span><span class="p">(</span><span class="n">x_output_MSA_block</span><span class="p">))</span> <span class="o">+</span> <span class="n">x_output_MSA_block</span> 
</pre></div>
</div>
<hr class="docutils" />
<p><strong>Equation 4:</strong>
Similar to BERT’s [ class ] token, we prepend a learnable embedding to the sequence of embedded patches <span class="math notranslate nohighlight">\(\left(\mathbf{z}_{0}^{0}=\mathbf{x}_{\text {class }}\right)\)</span>, whose state at the output of the Transformer encoder <span class="math notranslate nohighlight">\(\left(\mathbf{z}_{L}^{0}\right)\)</span> serves as the image representation y (Eq. 4). Both during pre-training and fine-tuning, a classification head is attached to <span class="math notranslate nohighlight">\(\mathbf{z}_{L}^{0}\)</span>. The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.</p>
<ul class="simple">
<li><p>MLP = multilayer perceptron = a neural network with X number of layers</p></li>
<li><p>MLP = one hidden layer at training time</p></li>
<li><p>MLP = single linear layer at fine-tuning time</p></li>
</ul>
<p>In pseudocode:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Equation 4</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Linear_layer</span><span class="p">(</span><span class="n">LN_layer</span><span class="p">(</span><span class="n">x_output_MLP_block</span><span class="p">))</span> 
</pre></div>
</div>
</section>
</section>
<section id="table-1">
<h3>Table 1<a class="headerlink" href="#table-1" title="Permalink to this heading">#</a></h3>
<img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-table-1.png" width=600 alt="table 1 from vision transformer paper"/>
<ul class="simple">
<li><p>ViT-Base, ViT-Large and ViT-Huge are all different sizes of the same model architecture</p></li>
<li><p>ViT-B/16 = ViT-Base with image patch size 16x16</p></li>
<li><p>Layers - the number of transformer encoder layers</p></li>
<li><p>Hidden size <span class="math notranslate nohighlight">\(D\)</span> - the embedding size throughout the architecture</p></li>
<li><p>MLP size - the number of hidden units/neurons in the MLP</p></li>
<li><p>Heads - the number of multi-head self-attention</p></li>
</ul>
</section>
</section>
<section id="equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding">
<h2>4. Equation 1: Split data into patches and creating the class, position and patch embedding<a class="headerlink" href="#equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding" title="Permalink to this heading">#</a></h2>
<p>Layers = input -&gt; function -&gt; output</p>
<p>What’s the input shape?</p>
<p>What’s the output shape?</p>
<ul class="simple">
<li><p>Input shape: (224, 224, 3) -&gt; single image -&gt; (height, width, color channels)</p></li>
<li><p>Output shape: ???</p></li>
</ul>
<section id="calculate-input-and-output-shapes-by-hand">
<h3>4.1 Calculate input and output shapes by hand<a class="headerlink" href="#calculate-input-and-output-shapes-by-hand" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p><strong>Equation 1:</strong>
An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle <span class="math notranslate nohighlight">\(2 \mathrm{D}\)</span> images, we reshape the image <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{H \times W \times C}\)</span> into a sequence of flattened <span class="math notranslate nohighlight">\(2 \mathrm{D}\)</span> patches <span class="math notranslate nohighlight">\(\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}\)</span>, where <span class="math notranslate nohighlight">\((H, W)\)</span> is the resolution of the original image, <span class="math notranslate nohighlight">\(C\)</span> is the number of channels, <span class="math notranslate nohighlight">\((P, P)\)</span> is the resolution of each image patch, and <span class="math notranslate nohighlight">\(N=H W / P^{2}\)</span> is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size <span class="math notranslate nohighlight">\(D\)</span> through all of its layers, so we flatten the patches and map to <span class="math notranslate nohighlight">\(D\)</span> dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.</p>
</div></blockquote>
<blockquote>
<div><p><strong>Equation 1:</strong>
Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable <span class="math notranslate nohighlight">\(1 \mathrm{D}\)</span> position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.</p>
</div></blockquote>
<ul class="simple">
<li><p>Input shape: <span class="math notranslate nohighlight">\(H\times{W}\times{C}\)</span> (height x width x color channels)</p></li>
<li><p>Output shape: <span class="math notranslate nohighlight">\({N \times\left(P^{2} \cdot C\right)}\)</span></p></li>
<li><p>H = height</p></li>
<li><p>W = width</p></li>
<li><p>C = color channels</p></li>
<li><p>P = patch size</p></li>
<li><p>N = number of patches = (height * width) / p^2</p></li>
<li><p>D = constant latent vector size = embedding dimension (see Table 1)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create example values
height = 224
width = 224
color_channels = 3
patch_size = 16 

# Calculate the number of patches
number_of_patches = int((height * width) / patch_size**2)
number_of_patches
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>196
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Input shape
embedding_layer_input_shape = (height, width, color_channels)

# Output shape
embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)

print(f&quot;Input shape (single 2D image): {embedding_layer_input_shape}&quot;)
print(f&quot;Output shape (single 1D sequence of patches): {embedding_layer_output_shape} -&gt; (number_of_patches, embedding_dimension)&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input shape (single 2D image): (224, 224, 3)
Output shape (single 1D sequence of patches): (196, 768) -&gt; (number_of_patches, embedding_dimension)
</pre></div>
</div>
</div>
</div>
</section>
<section id="turning-a-single-image-into-patches">
<h3>4.2 Turning a single image into patches<a class="headerlink" href="#turning-a-single-image-into-patches" title="Permalink to this heading">#</a></h3>
<p>Let’s <em>visualize, visualize, visualize!</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># View a single image
plt.imshow(image.permute(1, 2, 0))
plt.title(class_names[label])
plt.axis(False);
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/6766fe2e96b6643401db9de493d7d3a2294c566ec742731083e9b15086e28fa0.png" src="../../_images/6766fe2e96b6643401db9de493d7d3a2294c566ec742731083e9b15086e28fa0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>image.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 224, 224])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get the top row of the image
image_permuted = image.permute(1, 2, 0) # convert image to color channels last (H, W, C)

# Index to plot the top row of pixels
patch_size = 16
plt.figure(figsize=(patch_size, patch_size))
plt.imshow(image_permuted[:patch_size, :, :])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x7fc62c2838d0&gt;
</pre></div>
</div>
<img alt="../../_images/77574bbc0567148e7a5b05345094ced8a7e38af18ec88667c57e08fcf7c8ce6a.png" src="../../_images/77574bbc0567148e7a5b05345094ced8a7e38af18ec88667c57e08fcf7c8ce6a.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Setup code to plot top row as patches
img_size = 224
patch_size = 16
num_patches = img_size/patch_size 
assert img_size % patch_size == 0, &quot;Image size must be divisible by patch size&quot;
print(f&quot;Number of patches per row: {num_patches}\nPatch size: {patch_size} pixels x {patch_size} pixels&quot;)

# Create a series of subplots
fig, axs = plt.subplots(nrows=1,
                        ncols=img_size // patch_size, # one column for each patch
                        sharex=True,
                        sharey=True,
                        figsize=(patch_size, patch_size))

# Iterate through number of patches in the top row
for i, patch in enumerate(range(0, img_size, patch_size)):
  axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); 
  axs[i].set_xlabel(i+1) # set the patch label
  axs[i].set_xticks([])
  axs[i].set_yticks([])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of patches per row: 14.0
Patch size: 16 pixels x 16 pixels
</pre></div>
</div>
<img alt="../../_images/092471678830661e0cf288ef1ffc63cb8026d8958c87cfb5cc6a31a7acf2467c.png" src="../../_images/092471678830661e0cf288ef1ffc63cb8026d8958c87cfb5cc6a31a7acf2467c.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Setup code to plot whole image as patches
img_size = 224
patch_size = 16
num_patches = img_size/patch_size 
assert img_size % patch_size == 0, &quot;Image size must be divisible by patch size&quot;
print(f&quot;Number of patches per row: {num_patches}\
  \nNumber of patches per column: {num_patches}\
  \nTotal patches: {num_patches*num_patches}\
  \nPatch size: {patch_size} pixels x {patch_size} pixels&quot;)

# Create a series of subplots
fig, axs = plt.subplots(nrows=img_size // patch_size,
                        ncols=img_size // patch_size,
                        figsize=(num_patches, num_patches),
                        sharex=True,
                        sharey=True)

# Loop through height and width of image
for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height
  for j, patch_width in enumerate(range(0, img_size, patch_size)):
    # Plot the permuted image on the different axes 
    axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height
                                    patch_width:patch_width+patch_size, # iterate through width
                                    :]) # get all color channels
    # Set up label information for each subplot (patch)
    axs[i, j].set_ylabel(i+1,
                         rotation=&quot;horizontal&quot;,
                         horizontalalignment=&quot;right&quot;,
                         verticalalignment=&quot;center&quot;)
    axs[i, j].set_xlabel(j+1)
    axs[i, j].set_xticks([])
    axs[i, j].set_yticks([])
    axs[i, j].label_outer()

# Set up a title for the plot
fig.suptitle(f&quot;{class_names[label]} -&gt; Patchified&quot;, fontsize=14)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of patches per row: 14.0  
Number of patches per column: 14.0  
Total patches: 196.0  
Patch size: 16 pixels x 16 pixels
</pre></div>
</div>
<img alt="../../_images/cf87730509b7e2dc787a62a716ecea678ac4426914007a41583fc229ddbc063d.png" src="../../_images/cf87730509b7e2dc787a62a716ecea678ac4426914007a41583fc229ddbc063d.png" />
</div>
</div>
</section>
<section id="creating-image-patches-and-turning-them-into-patch-embeddings">
<h3>4.3 Creating image patches and turning them into patch embeddings<a class="headerlink" href="#creating-image-patches-and-turning-them-into-patch-embeddings" title="Permalink to this heading">#</a></h3>
<p>Perhaps we could create the image patches and image patch embeddings in a single step using <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d()</span></code> and setting the kernel size and stride parameters to <code class="docutils literal notranslate"><span class="pre">patch_size</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create conv2d layer to turn image into patches of learnable feature maps (embeddings)
from torch import nn

# Set the patch size 
patch_size = 16

# Create a conv2d lyaer with hyperparameters from the ViT paper
conv2d = nn.Conv2d(in_channels=3, # for color images
                   out_channels=768, # D size from Table 1 for ViT-Base
                   kernel_size=patch_size,
                   stride=patch_size,
                   padding=0)
conv2d
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># View single image
plt.imshow(image.permute(1, 2, 0))
plt.title(class_names[label])
plt.axis(False);
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/6766fe2e96b6643401db9de493d7d3a2294c566ec742731083e9b15086e28fa0.png" src="../../_images/6766fe2e96b6643401db9de493d7d3a2294c566ec742731083e9b15086e28fa0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Pass the image through the convolutional layer
image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension -&gt; (batch_size, color_channels, height, width)
print(image_out_of_conv.shape) 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 768, 14, 14])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>image_out_of_conv.requires_grad
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>Now we’ve passed a single image to our <code class="docutils literal notranslate"><span class="pre">conv2d</span></code> layer, it’s shape is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">768</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span> <span class="c1"># [batch_size, embedding_dim, feature_map_height, feature_map_width] </span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># plot random convolutional feature maps (embeddings)
import random
random_indexes = random.sample(range(0, 758), k=5)
print(f&quot;Showing random convolutional feature maps from indexes: {random_indexes}&quot;)

# Create plot 
fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))

# Plot random image feature maps
for i, idx in enumerate(random_indexes):
  image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the conv2d layer
  axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy()) # remove batch dimension, and remove from grad tracking/switch to numpy for matplotlib
  axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Showing random convolutional feature maps from indexes: [507, 71, 166, 241, 272]
</pre></div>
</div>
<img alt="../../_images/8fbd6fab46f070cdcb7ff97106685be1115ff07ec47fb6f1aa45e603fbb99cd2.png" src="../../_images/8fbd6fab46f070cdcb7ff97106685be1115ff07ec47fb6f1aa45e603fbb99cd2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get a single feature map in tensor form
single_feature_map = image_out_of_conv[:, 0, :, :] 
single_feature_map, single_feature_map.requires_grad
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[[ 0.1407,  0.1090,  0.1021,  0.1104,  0.0994, -0.0023,  0.0833,
            0.0281,  0.0831,  0.0941,  0.0986,  0.0203,  0.0206, -0.0152],
          [ 0.0804,  0.1664,  0.1979, -0.0078, -0.3030, -0.0536, -0.2039,
           -0.0388, -0.0347, -0.1243, -0.1810, -0.0476,  0.1051,  0.0399],
          [-0.0716,  0.0355, -0.0671, -0.0396, -0.2541, -0.1525, -0.1335,
           -0.1575,  0.0895, -0.0103, -0.1282, -0.0114, -0.0942, -0.0481],
          [-0.1791, -0.1164, -0.1725, -0.0481, -0.0625, -0.0877, -0.1679,
           -0.0722, -0.1618, -0.0711, -0.0335,  0.1048,  0.1567,  0.1100],
          [ 0.0778, -0.2123, -0.0797, -0.1068, -0.0471, -0.0960,  0.0561,
           -0.0643, -0.0510, -0.0651, -0.0275, -0.0313, -0.1188, -0.0143],
          [-0.1055, -0.0940,  0.0829, -0.0087, -0.0778, -0.1851, -0.1613,
            0.1131,  0.0694, -0.1660, -0.2467,  0.0015,  0.1099,  0.1271],
          [-0.0018,  0.0883, -0.0462, -0.0074,  0.0591, -0.1006, -0.0257,
           -0.2168, -0.1621,  0.1588,  0.0712,  0.0631, -0.1775, -0.1007],
          [ 0.0357, -0.0580, -0.1732, -0.2045, -0.0843, -0.1670, -0.0618,
            0.0191,  0.0565, -0.1508,  0.0103,  0.0300,  0.1253,  0.0356],
          [-0.0979, -0.1481, -0.1128,  0.1911,  0.1118,  0.1514, -0.1875,
           -0.2145,  0.0742,  0.0809,  0.2035,  0.0928,  0.1184,  0.1298],
          [-0.0954, -0.1065, -0.1638, -0.0156,  0.0218, -0.1327, -0.1192,
           -0.1086, -0.1809, -0.0102, -0.1456,  0.0052, -0.0445,  0.0401],
          [-0.0196,  0.0171,  0.0129, -0.1425, -0.0151, -0.0344, -0.0826,
           -0.0653, -0.1250,  0.0047, -0.0196, -0.0445,  0.0677,  0.0581],
          [-0.0221, -0.0131, -0.0670, -0.1478, -0.0123,  0.0458,  0.0050,
           -0.0714, -0.0824, -0.0079, -0.0218,  0.0119, -0.1344, -0.0957],
          [-0.1727, -0.0127, -0.0364, -0.0090, -0.0312, -0.0084,  0.0170,
           -0.0113, -0.1140, -0.1092, -0.1072, -0.0010,  0.0080, -0.1315],
          [-0.1228, -0.1214, -0.1354, -0.0643, -0.1033, -0.1008, -0.1099,
           -0.0781, -0.0485, -0.1678, -0.1026, -0.0602, -0.0902,  0.0780]]],
        grad_fn=&lt;SliceBackward0&gt;), True)
</pre></div>
</div>
</div>
</div>
</section>
<section id="flattening-the-patch-embedding-with-torch-nn-flatten">
<h3>4.4 Flattening the patch embedding with <code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten()</span></code><a class="headerlink" href="#flattening-the-patch-embedding-with-torch-nn-flatten" title="Permalink to this heading">#</a></h3>
<p>Right now we’ve a series of convolutional feature maps (patch embeddings) that we want to flatten into a sequence of patch embeddings to satisfy the input criteria of the ViT Transformer Encoder.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(f&quot;{image_out_of_conv.shape} -&gt; (batch_size, embedding_dim, feature_map_height, feature_map_width)&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 768, 14, 14]) -&gt; (batch_size, embedding_dim, feature_map_height, feature_map_width)
</pre></div>
</div>
</div>
</div>
<p>Want: (batch_size, number_of_patches, embedding_dim)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from torch import nn
flatten_layer = nn.Flatten(start_dim=2,
                           end_dim=3)

flatten_layer(image_out_of_conv).shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 768, 196])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Put everything together
plt.imshow(image.permute(1, 2, 0))
plt.title(class_names[label])
plt.axis(False)
print(f&quot;Original image shape: {image.shape}&quot;)

# Turn image into feature maps
image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension
print(f&quot;Image feature map (patches) shape: {image_out_of_conv.shape}&quot;)

# Flatten the feature maps
image_out_of_conv_flattened = flatten_layer(image_out_of_conv)
print(f&quot;Flattened image feature map shape: {image_out_of_conv_flattened.shape}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original image shape: torch.Size([3, 224, 224])
Image feature map (patches) shape: torch.Size([1, 768, 14, 14])
Flattened image feature map shape: torch.Size([1, 768, 196])
</pre></div>
</div>
<img alt="../../_images/6766fe2e96b6643401db9de493d7d3a2294c566ec742731083e9b15086e28fa0.png" src="../../_images/6766fe2e96b6643401db9de493d7d3a2294c566ec742731083e9b15086e28fa0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Rearrange output of flattened layer
image_out_of_conv_flattened_permuted = image_out_of_conv_flattened.permute(0, 2, 1)
print(f&quot;{image_out_of_conv_flattened_permuted.shape} -&gt; (batch_size, number_of_patches, embedding_dimension)&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 196, 768]) -&gt; (batch_size, number_of_patches, embedding_dimension)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get a single flattened feature map
single_flattened_feature_map = image_out_of_conv_flattened_permuted[:, :, 0]

# Plot the flattened feature map visually
plt.figure(figsize=(22, 22))
plt.imshow(single_flattened_feature_map.detach().numpy())
plt.title(f&quot;Flattened feature map shape: {single_flattened_feature_map.shape}&quot;)
plt.axis(False); 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/b2452cddf7b1498ac82121834652735c71137ad0e92bde7e31fdb754c3c38422.png" src="../../_images/b2452cddf7b1498ac82121834652735c71137ad0e92bde7e31fdb754c3c38422.png" />
</div>
</div>
</section>
<section id="turning-the-vit-patch-embedding-layer-into-a-pytorch-module">
<h3>4.5 Turning the ViT patch embedding layer into a PyTorch module<a class="headerlink" href="#turning-the-vit-patch-embedding-layer-into-a-pytorch-module" title="Permalink to this heading">#</a></h3>
<p>We want this module to do a few things:</p>
<ol class="arabic simple">
<li><p>Create a class called <code class="docutils literal notranslate"><span class="pre">PatchEmbedding</span></code> that inherits from <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.</p></li>
<li><p>Initialize with appropriate hyperparameters, such as channels, embedding dimension, patch size.</p></li>
<li><p>Create a layer to turn an image into embedded patches using <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code>.</p></li>
<li><p>Create a layer to flatten the feature maps of the output of the layer in 3.</p></li>
<li><p>Define a <code class="docutils literal notranslate"><span class="pre">foward()</span></code> that defines the forward computation (e.g. pass through layer from 3 and 4).</p></li>
<li><p>Make sure the output shape of the layer reflects the required output shape of the patch embedding.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 1. Create a class called PatchEmbedding
class PatchEmbedding(nn.Module):
  # 2. Initilaize the layer with appropriate hyperparameters
  def __init__(self,
               in_channels:int=3,
               patch_size:int=16,
               embedding_dim:int=768): # from Table 1 for ViT-Base
    super().__init__()

    self.patch_size = patch_size
  
    # 3. Create a layer to turn an image into embedded patches
    self.patcher = nn.Conv2d(in_channels=in_channels,
                             out_channels=embedding_dim,
                             kernel_size=patch_size,
                             stride=patch_size,
                             padding=0)
    
    # 4. Create a layer to flatten feature map outputs of Conv2d
    self.flatten = nn.Flatten(start_dim=2,
                              end_dim=3)
    
  # 5. Define a forward method to define the forward computation steps
  def forward(self, x):
    # Create assertion to check that inputs are the correct shape
    image_resolution = x.shape[-1]
    assert image_resolution % patch_size == 0, f&quot;Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {self.patch_size}&quot;

    # Perform the forward pass
    x_patched = self.patcher(x) 
    x_flattened = self.flatten(x_patched)
    # 6. Make the returned sequence embedding dimensions are in the right order (batch_size, number_of_patches, embedding_dimension)
    return x_flattened.permute(0, 2, 1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>set_seeds()

# Create an instance of patch embedding layer
patchify = PatchEmbedding(in_channels=3,
                          patch_size=16,
                          embedding_dim=768)

# Pass a single image through patch embedding layer
print(f&quot;Input image size: {image.unsqueeze(0).shape}&quot;)
patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension
print(f&quot;Output patch embedding sequence shape: {patch_embedded_image.shape}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input image size: torch.Size([1, 3, 224, 224])
Output patch embedding sequence shape: torch.Size([1, 196, 768])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>rand_image_tensor = torch.randn(1, 3, 224, 224)
rand_image_tensor_bad = torch.randn(1, 3, 250, 250)

# patchify(rand_image_tensor_bad)
</pre></div>
</div>
</div>
</div>
</section>
<section id="creating-the-class-token-embedding">
<h3>4.6 Creating the class token embedding<a class="headerlink" href="#creating-the-class-token-embedding" title="Permalink to this heading">#</a></h3>
<p>Want to: prepend a learnable class token to the start of the patch embedding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>patch_embedded_image
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-0.7665,  0.1767, -0.2405,  ...,  0.6293, -0.3423,  0.3247],
         [-0.7299,  0.1911, -0.1953,  ...,  0.5647, -0.3340,  0.2806],
         [-0.7201,  0.1827, -0.1611,  ...,  0.5443, -0.3373,  0.2574],
         ...,
         [-0.5255,  0.0765, -0.0469,  ...,  0.3121, -0.3001,  0.2329],
         [-0.4847,  0.1029, -0.0593,  ...,  0.2676, -0.2530,  0.0827],
         [-0.2441,  0.1201, -0.3067,  ...,  0.2972, -0.1575,  0.0815]]],
       grad_fn=&lt;PermuteBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get the batch size and embedding dimension
batch_size = patch_embedded_image.shape[0]
embedding_dimension = patch_embedded_image.shape[-1]
batch_size, embedding_dimension
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1, 768)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)
class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),
                           requires_grad=True)
class_token.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 1, 768])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>patch_embedded_image.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 196, 768])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Add the class token embedding to the front of the patch embedding 
patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),
                                                      dim=1) # number_of_patches dimension

print(patch_embedded_image_with_class_embedding)
print(f&quot;Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; (batch_size, class_token + number_of_patches, embedding_dim)&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],
         [-0.7665,  0.1767, -0.2405,  ...,  0.6293, -0.3423,  0.3247],
         [-0.7299,  0.1911, -0.1953,  ...,  0.5647, -0.3340,  0.2806],
         ...,
         [-0.5255,  0.0765, -0.0469,  ...,  0.3121, -0.3001,  0.2329],
         [-0.4847,  0.1029, -0.0593,  ...,  0.2676, -0.2530,  0.0827],
         [-0.2441,  0.1201, -0.3067,  ...,  0.2972, -0.1575,  0.0815]]],
       grad_fn=&lt;CatBackward0&gt;)
Sequence of patch embeddings with class token prepended shape: torch.Size([1, 197, 768]) -&gt; (batch_size, class_token + number_of_patches, embedding_dim)
</pre></div>
</div>
</div>
</div>
</section>
<section id="creating-the-position-embedding">
<h3>4.7 Creating the position embedding<a class="headerlink" href="#creating-the-position-embedding" title="Permalink to this heading">#</a></h3>
<p>Want to: create a series of 1D learnable position embeddings and to add them to the sequence of patch embeddings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Calculate N (number_of_patches)
number_of_patches = int((height * width) / patch_size**2)

# Get the embedding dimension
embedding_dimension = patch_embedded_image_with_class_embedding.shape[-1]

# Create the learnable 1D position embedding
position_embedding = nn.Parameter(torch.ones(1,
                                             number_of_patches+1,
                                             embedding_dimension),
                                  requires_grad=True)

position_embedding, position_embedding.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(Parameter containing:
 tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          ...,
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.]]], requires_grad=True),
 torch.Size([1, 197, 768]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># View the sequence of patch embeddings with the prepended class embedding
patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],
          [-0.7665,  0.1767, -0.2405,  ...,  0.6293, -0.3423,  0.3247],
          [-0.7299,  0.1911, -0.1953,  ...,  0.5647, -0.3340,  0.2806],
          ...,
          [-0.5255,  0.0765, -0.0469,  ...,  0.3121, -0.3001,  0.2329],
          [-0.4847,  0.1029, -0.0593,  ...,  0.2676, -0.2530,  0.0827],
          [-0.2441,  0.1201, -0.3067,  ...,  0.2972, -0.1575,  0.0815]]],
        grad_fn=&lt;CatBackward0&gt;), torch.Size([1, 197, 768]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Add the position embedding to the patch and class token embedding
patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding
print(patch_and_position_embedding)
print(f&quot;Patch and position embedding shape: {patch_and_position_embedding.shape}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[2.0000, 2.0000, 2.0000,  ..., 2.0000, 2.0000, 2.0000],
         [0.2335, 1.1767, 0.7595,  ..., 1.6293, 0.6577, 1.3247],
         [0.2701, 1.1911, 0.8047,  ..., 1.5647, 0.6660, 1.2806],
         ...,
         [0.4745, 1.0765, 0.9531,  ..., 1.3121, 0.6999, 1.2329],
         [0.5153, 1.1029, 0.9408,  ..., 1.2676, 0.7470, 1.0827],
         [0.7559, 1.1201, 0.6933,  ..., 1.2972, 0.8425, 1.0815]]],
       grad_fn=&lt;AddBackward0&gt;)
Patch and position embedding shape: torch.Size([1, 197, 768])
</pre></div>
</div>
</div>
</div>
</section>
<section id="putting-it-all-together-from-image-to-embedding">
<h3>4.8 Putting it all together: from image to embedding<a class="headerlink" href="#putting-it-all-together-from-image-to-embedding" title="Permalink to this heading">#</a></h3>
<p>We’ve written code to turn an image into a flattened sequence of patch embeddings.</p>
<p>Now let’s it all in one cell.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Set seeds
set_seeds()

# 1. Set the patch size
patch_size = 16

# 2. Print shapes of the original image tensor and get the image dimensions
print(f&quot;Image tensor shape: {image.shape}&quot;)
height, width = image.shape[1], image.shape[2]

# 3. Get image tensor and add a batch dimension
x = image.unsqueeze(0)
print(f&quot;Input image shape: {x.shape}&quot;)

# 4. Create patch embedding layer
patch_embedding_layer = PatchEmbedding(in_channels=3,
                                       patch_size=patch_size, 
                                       embedding_dim=768)

# 5. Pass input image through PatchEmbedding
patch_embedding = patch_embedding_layer(x)
print(f&quot;Patch embedding shape: {patch_embedding.shape}&quot;)

# 6. Create class token embedding
batch_size = patch_embedding.shape[0]
embedding_dimension = patch_embedding.shape[-1]
class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),
                           requires_grad=True) # make sure it&#39;s learnable
print(f&quot;Class token embedding shape: {class_token.shape}&quot;)

# 7. Prepend the class token embedding to patch embedding
patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)
print(f&quot;Patch embedding with class token shape: {patch_embedding_class_token.shape}&quot;)

# 8. Create position embedding
number_of_patches = int((height*width) / patch_size**2)
position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),
                                  requires_grad=True)

# 9. Add the position embedding to patch embedding with class token
patch_and_position_embedding = patch_embedding_class_token + position_embedding
print(f&quot;Patch and position embedding shape: {patch_and_position_embedding.shape} &quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Image tensor shape: torch.Size([3, 224, 224])
Input image shape: torch.Size([1, 3, 224, 224])
Patch embedding shape: torch.Size([1, 196, 768])
Class token embedding shape: torch.Size([1, 1, 768])
Patch embedding with class token shape: torch.Size([1, 197, 768])
Patch and position embedding shape: torch.Size([1, 197, 768]) 
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="equation-2-multihead-self-attention-msa-block">
<h2>Equation 2: Multihead Self-Attention (MSA block)<a class="headerlink" href="#equation-2-multihead-self-attention-msa-block" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Multihead self-attention</strong> = which part of a sequence should pay the most attention to itself?</p>
<ul>
<li><p>In our case, we have a series of embedded image patches, which patch significantly relates to another patch.</p></li>
<li><p>We want our neural network (ViT) to learn this relationship/representation.</p></li>
</ul>
</li>
<li><p>To replicate MSA in PyTorch we can use: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html">https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html</a></p></li>
<li><p><strong>LayerNorm</strong> = Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy.</p>
<ul>
<li><p>Normalization = make everything have the same mean and same standard deviation.</p></li>
<li><p>In PyTorch = <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html</a>, normalizes values over <span class="math notranslate nohighlight">\(D\)</span> dimension, in our case, the <span class="math notranslate nohighlight">\(D\)</span> dimension is the embedding dimension.</p>
<ul>
<li><p>When we normalize along the embedding dimension, it’s like making all of the stairs in a staircase the same size.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class MultiHeadSelfAttentionBlock(nn.Module): 
  &quot;&quot;&quot;Creates a multi-head self-attention block (&quot;MSA block&quot; for short).
  &quot;&quot;&quot;
  def __init__(self, 
               embedding_dim:int=768, # Hidden size D (embedding dimension) from Table 1 for ViT-Base
               num_heads:int=12, # Heads from Table 1 for ViT-Base
               attn_dropout:int=0):
    super().__init__()
    
    # Create the norm layer (LN)
    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)

    # Create multihead attention (MSA) layer
    self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,
                                                num_heads=num_heads,
                                                dropout=attn_dropout, 
                                                batch_first=True) # is the batch first? (batch, seq, feature) -&gt; (batch, number_of_patches, embedding_dimension)
  
  def forward(self, x):
    x = self.layer_norm(x)
    attn_output, _ = self.multihead_attn(query=x,
                                         key=x,
                                         value=x,
                                         need_weights=False)
    return attn_output
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create an instance MSA block
multihead_self_attention_block = MultiHeadSelfAttentionBlock(embedding_dim=768,
                                                             num_heads=12,
                                                             attn_dropout=0)

# Pass the patch and position image embedding sequence through MSA block
patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)
print(f&quot;Input shape of MSA block: {patch_and_position_embedding.shape}&quot;)
print(f&quot;Output shape of MSA block: {patched_image_through_msa_block.shape}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input shape of MSA block: torch.Size([1, 197, 768])
Output shape of MSA block: torch.Size([1, 197, 768])
</pre></div>
</div>
</div>
</div>
</section>
<section id="equation-3-multilayer-perceptron-mlp-block">
<h2>6. Equation 3: Multilayer Perceptron (MLP block)<a class="headerlink" href="#equation-3-multilayer-perceptron-mlp-block" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>MLP</strong> = The MLP contains two layers with a GELU non-linearity (section 3.1).</p>
<ul>
<li><p>MLP = a quite broad term for a block with a series of layer(s), layers can be multiple or even only one hidden layer.</p></li>
<li><p>Layers can mean: fully-connected, dense, linear, feed-forward, all are often similar names for the same thing. In PyTorch, they’re often called <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear()</span></code> and in TensorFlow they might be called <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dense()</span></code></p></li>
<li><p>GELU in PyTorch - <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU">https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU</a></p></li>
<li><p>MLP number of hidden units = MLP Size in Table 1</p></li>
</ul>
</li>
<li><p><strong>Dropout</strong> =  Dropout, when used, is applied after
every dense layer except for the the qkv-projections and directly after adding positional- to patch
embeddings. Hybrid models are trained with the exact setup as their ViT counterparts.</p>
<ul>
<li><p>Value for Dropout available in Table 3</p></li>
</ul>
</li>
</ul>
<p>In pseudocode:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># MLP</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">linear</span> <span class="o">-&gt;</span> <span class="n">non</span><span class="o">-</span><span class="n">linear</span> <span class="o">-&gt;</span> <span class="n">dropout</span> <span class="o">-&gt;</span> <span class="n">linear</span> <span class="o">-&gt;</span> <span class="n">dropout</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class MLPBlock(nn.Module):
  def __init__(self,
               embedding_dim:int=768,
               mlp_size:int=3072,
               dropout:int=0.1):
    super().__init__()
    
    # Create the norm layer (LN) 
    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)

    # Create the MLP
    self.mlp = nn.Sequential(
        nn.Linear(in_features=embedding_dim,
                  out_features=mlp_size),
        nn.GELU(),
        nn.Dropout(p=dropout),
        nn.Linear(in_features=mlp_size,
                  out_features=embedding_dim),
        nn.Dropout(p=dropout) 
    )
  
  def forward(self, x):
    x = self.layer_norm(x) 
    x = self.mlp(x)
    return x
    # return self.mlp(self.layer_norm(x)) # same as above 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create an instance of MLPBlock
mlp_block = MLPBlock(embedding_dim=768,
                     mlp_size=3072,
                     dropout=0.1)

# Pass output the MSABlock through MLPBlock
patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)
print(f&quot;Input shape of MLP block: {patched_image_through_msa_block.shape}&quot;)
print(f&quot;Output shape of MLP block: {patched_image_through_mlp_block.shape}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input shape of MLP block: torch.Size([1, 197, 768])
Output shape of MLP block: torch.Size([1, 197, 768])
</pre></div>
</div>
</div>
</div>
</section>
<section id="creating-the-transformer-encoder">
<h2>7. Creating the Transformer Encoder<a class="headerlink" href="#creating-the-transformer-encoder" title="Permalink to this heading">#</a></h2>
<p>The Transformer Encoder is a combination of alternating blocks of MSA (equation 2) and MLP (equation 3).</p>
<p>And there are residual connections between each block.</p>
<ul class="simple">
<li><p>Encoder = turn a sequence into learnable representation</p></li>
<li><p>Decoder = go from learn representation back to some sort of sequence</p></li>
<li><p>Residual connections = add a layer(s) input to its subsequent output, this enables the creation of deeper networks (prevents weights from getting too small)</p></li>
</ul>
<p>In pseudocode:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Transformer Encoder</span>
<span class="n">x_input</span> <span class="o">-&gt;</span> <span class="n">MSA_block</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="n">MSA_block_output</span> <span class="o">+</span> <span class="n">x_input</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="n">MLP_block</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="n">MLP_block_output</span> <span class="o">+</span> <span class="n">MSA_block_output</span> <span class="o">+</span> <span class="n">x_input</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="o">...</span> 
</pre></div>
</div>
<section id="create-a-custom-transformer-encoder-block">
<h3>7.1 Create a custom Transformer Encoder block<a class="headerlink" href="#create-a-custom-transformer-encoder-block" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class TransformerEncoderBlock(nn.Module):
  def __init__(self,
               embedding_dim:int=768, # Hidden size D from table 1, 768 for ViT-Base
               num_heads:int=12, # from table 1
               mlp_size:int=3072, # from table 1
               mlp_dropout:int=0.1, # from table 3
               attn_dropout:int=0):
    super().__init__()

    # Create MSA block (equation 2)
    self.msa_block = MultiHeadSelfAttentionBlock(embedding_dim=embedding_dim,
                                                 num_heads=num_heads,
                                                 attn_dropout=attn_dropout)
    
    # Create MLP block (equation 3)
    self.mlp_block = MLPBlock(embedding_dim=embedding_dim, 
                              mlp_size=mlp_size,
                              dropout=mlp_dropout)
    
  def forward(self, x):
    x = self.msa_block(x) + x # residual/skip connection for equation 2
    x = self.mlp_block(x) + x # residual/skip connection for equation 3
    return x 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create an instance of TransformerEncoderBlock()
transformer_encoder_block = TransformerEncoderBlock()

# Get a summary using torchinfo.summary
summary(model=transformer_encoder_block,
        input_size=(1, 197, 768), # (batch_size, number_of_patches, embedding_dimension)
        col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],
        col_width=20,
        row_settings=[&quot;var_names&quot;])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>============================================================================================================================================
Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable
============================================================================================================================================
TransformerEncoderBlock (TransformerEncoderBlock)            [1, 197, 768]        [1, 197, 768]        --                   True
├─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        --                   True
│    └─LayerNorm (layer_norm)                                [1, 197, 768]        [1, 197, 768]        1,536                True
│    └─MultiheadAttention (multihead_attn)                   --                   [1, 197, 768]        1,771,776            True
│    │    └─NonDynamicallyQuantizableLinear (out_proj)       --                   --                   590,592              True
├─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        --                   True
│    └─LayerNorm (layer_norm)                                [1, 197, 768]        [1, 197, 768]        1,536                True
│    └─Sequential (mlp)                                      [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─Linear (0)                                       [1, 197, 768]        [1, 197, 3072]       2,362,368            True
│    │    └─GELU (1)                                         [1, 197, 3072]       [1, 197, 3072]       --                   --
│    │    └─Dropout (2)                                      [1, 197, 3072]       [1, 197, 3072]       --                   --
│    │    └─Linear (3)                                       [1, 197, 3072]       [1, 197, 768]        2,360,064            True
│    │    └─Dropout (4)                                      [1, 197, 768]        [1, 197, 768]        --                   --
============================================================================================================================================
Total params: 7,087,872
Trainable params: 7,087,872
Non-trainable params: 0
Total mult-adds (M): 4.73
============================================================================================================================================
Input size (MB): 0.61
Forward/backward pass size (MB): 8.47
Params size (MB): 21.26
Estimated Total Size (MB): 30.34
============================================================================================================================================
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-a-transformer-encoder-layer-with-in-built-pytorch-layers">
<h3>7.2 Create a Transformer Encoder layer with in-built PyTorch layers<a class="headerlink" href="#create-a-transformer-encoder-layer-with-in-built-pytorch-layers" title="Permalink to this heading">#</a></h3>
<p>So far we’ve created a transformer encoder by hand.</p>
<p>But because of how good the Transformer architecture is, PyTorch has implemented ready to use Transformer Encoder layers: <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#transformer-layers">https://pytorch.org/docs/stable/nn.html#transformer-layers</a></p>
<p>We can create a Transformer Encoder with pure PyTorch layers: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer">https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create the same as above with torch.nn.TransformerEncoderLayer()
torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # embedding size from table 1
                                                             nhead=12, # heads from table 1
                                                             dim_feedforward=3072, # MLP size from table
                                                             dropout=0.1,
                                                             activation=&quot;gelu&quot;,
                                                             batch_first=True,
                                                             norm_first=True)

torch_transformer_encoder_layer
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (linear1): Linear(in_features=768, out_features=3072, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=3072, out_features=768, bias=True)
  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get a summary using torchinfo.summary
summary(model=torch_transformer_encoder_layer,
        input_size=(1, 197, 768), # (batch_size, number_of_patches, embedding_dimension)
        col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],
        col_width=20,
        row_settings=[&quot;var_names&quot;])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==================================================================================================================================
Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable
==================================================================================================================================
TransformerEncoderLayer (TransformerEncoderLayer)  [1, 197, 768]        [1, 197, 768]        3,072                True
├─LayerNorm (norm1)                                [1, 197, 768]        [1, 197, 768]        (recursive)          True
├─MultiheadAttention (self_attn)                   [1, 197, 768]        [1, 197, 768]        2,362,368            True
├─Dropout (dropout1)                               [1, 197, 768]        [1, 197, 768]        --                   --
├─LayerNorm (norm2)                                [1, 197, 768]        [1, 197, 768]        (recursive)          True
├─Linear (linear1)                                 [1, 197, 768]        [1, 197, 3072]       2,362,368            True
├─LayerNorm (norm1)                                [1, 197, 768]        [1, 197, 768]        (recursive)          True
├─LayerNorm (norm2)                                [1, 197, 768]        [1, 197, 768]        (recursive)          True
├─Dropout (dropout)                                [1, 197, 3072]       [1, 197, 3072]       --                   --
├─Dropout (dropout2)                               [1, 197, 768]        [1, 197, 768]        --                   --
├─Linear (linear2)                                 [1, 197, 3072]       [1, 197, 768]        2,360,064            True
├─Dropout (dropout2)                               [1, 197, 768]        [1, 197, 768]        --                   --
==================================================================================================================================
Total params: 7,087,872
Trainable params: 7,087,872
Non-trainable params: 0
Total mult-adds (M): 4.73
==================================================================================================================================
Input size (MB): 0.61
Forward/backward pass size (MB): 6.05
Params size (MB): 18.89
Estimated Total Size (MB): 25.55
==================================================================================================================================
</pre></div>
</div>
</div>
</div>
<p>Why spend all this time recreating the transformer encoder when we could’ve just made it with a single PyTorch layer?</p>
<p>Practice. Practice. Practice.</p>
<p>Now we know how things are implemented behind the scenes, we can tweak them if necessary.</p>
<p>What are the benefits of using a pre-built PyTorch layer?</p>
<ul class="simple">
<li><p>Less prone to errors (goes through a bunch of testing)</p></li>
<li><p>Potential benefit of speed ups (performance boosts)</p></li>
</ul>
</section>
</section>
<section id="putting-it-all-together-to-create-vit">
<h2>8. Putting it all together to create ViT<a class="headerlink" href="#putting-it-all-together-to-create-vit" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create a ViT class 
class ViT(nn.Module): 
  def __init__(self,
               img_size:int=224, # Table 3 from the ViT paper
               in_channels:int=3,
               patch_size:int=16, 
               num_transformer_layers:int=12, # Table 1 for &quot;Layers&quot; for ViT-Base
               embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base
               mlp_size:int=3072, # Table 1
               num_heads:int=12, # Table 1
               attn_dropout:int=0,
               mlp_dropout:int=0.1,
               embedding_dropout:int=0.1, # Dropout for patch and position embeddings
               num_classes:int=1000): # number of classes in our classification problem
    super().__init__()

    # Make an assertion that the image size is compatible with the patch size
    assert img_size % patch_size == 0,  f&quot;Image size must be divisible by patch size, image: {img_size}, patch size: {patch_size}&quot;

    # Calculate the number of patches (height * width/patch^2)
    self.num_patches = (img_size * img_size) // patch_size**2

    # Create learnable class embedding (needs to go at front of sequence of patch embeddings)
    self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),
                                        requires_grad=True)
    
    # Create learnable position embedding 
    self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim))

    # Create embedding dropout value
    self.embedding_dropout = nn.Dropout(p=embedding_dropout)

    # Create patch embedding layer
    self.patch_embedding = PatchEmbedding(in_channels=in_channels,
                                          patch_size=patch_size,
                                          embedding_dim=embedding_dim)
    
    # Create the Transformer Encoder block
    self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,
                                                                       num_heads=num_heads,
                                                                       mlp_size=mlp_size,
                                                                       mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])
    
    # Create classifier head
    self.classifier = nn.Sequential(
        nn.LayerNorm(normalized_shape=embedding_dim),
        nn.Linear(in_features=embedding_dim,
                  out_features=num_classes)
    )
  
  def forward(self, x):
    # Get the batch size
    batch_size = x.shape[0]

    # Create class token embedding and expand it to match the batch size (equation 1)
    class_token = self.class_embedding.expand(batch_size, -1, -1) # &quot;-1&quot; means to infer the dimensions
    
    # Create the patch embedding (equation 1)
    x = self.patch_embedding(x)
    
    # Concat class token embedding and patch embedding (equation 1)
    x = torch.cat((class_token, x), dim=1) # (batch_size, number_of_patches, embedding_dim)

    # Add position embedding to class token and patch embedding
    x = self.position_embedding + x

    # Apply dropout to patch embedding (&quot;directly after adding positional- to patch embeddings&quot;)
    x = self.embedding_dropout(x)

    # Pass position and patch embedding to Transformer Encoder (equation 2 &amp; 3)
    x = self.transformer_encoder(x)

    # Put 0th index logit through classifier (equation 4)
    x = self.classifier(x[:, 0])
    
    return x 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>batch_size=32
embedding_dim=768
class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),
                              requires_grad=True)
class_embedding_expanded = class_embedding.expand(batch_size, -1, -1)
print(class_embedding.shape)
print(class_embedding_expanded.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 1, 768])
torch.Size([32, 1, 768])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>set_seeds()

# Create a random image tensor with same shape as a single image
random_image_tensor = torch.randn(1, 3, 224, 224)

# Create an instance of ViT with the number of classes we&#39;re working with (pizza, steak and sushi)
vit = ViT(num_classes=len(class_names))

# Pass the random image tensor to our ViT instance
vit(random_image_tensor)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.2377,  0.7360,  1.2137]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<section id="getting-a-visual-summary-of-our-vit-model">
<h3>8.1 Getting a visual summary of our ViT model<a class="headerlink" href="#getting-a-visual-summary-of-our-vit-model" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from torchinfo import summary

summary(model=ViT(num_classes=len(class_names)),
        input_size=(1, 3, 224, 224), # (batch_size, color_channels, height, width)
        col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],
        col_width=20,
        row_settings=[&quot;var_names&quot;])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>======================================================================================================================================================
Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable
======================================================================================================================================================
ViT (ViT)                                                              [1, 3, 224, 224]     [1, 3]               152,064              True
├─Dropout (embedding_dropout)                                          [1, 197, 768]        [1, 197, 768]        --                   --
├─PatchEmbedding (patch_embedding)                                     [1, 3, 224, 224]     [1, 196, 768]        --                   True
│    └─Conv2d (patcher)                                                [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True
│    └─Flatten (flatten)                                               [1, 768, 14, 14]     [1, 768, 196]        --                   --
├─Dropout (embedding_dropout)                                          [1, 197, 768]        [1, 197, 768]        --                   --
├─Sequential (transformer_encoder)                                     [1, 197, 768]        [1, 197, 768]        --                   True
│    └─TransformerEncoderBlock (0)                                     [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
│    └─TransformerEncoderBlock (1)                                     [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
│    └─TransformerEncoderBlock (2)                                     [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
│    └─TransformerEncoderBlock (3)                                     [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
│    └─TransformerEncoderBlock (4)                                     [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
│    └─TransformerEncoderBlock (5)                                     [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
│    └─TransformerEncoderBlock (6)                                     [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
│    └─TransformerEncoderBlock (7)                                     [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
│    └─TransformerEncoderBlock (8)                                     [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
│    └─TransformerEncoderBlock (9)                                     [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
│    └─TransformerEncoderBlock (10)                                    [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
│    └─TransformerEncoderBlock (11)                                    [1, 197, 768]        [1, 197, 768]        --                   True
│    │    └─MultiHeadSelfAttentionBlock (msa_block)                    [1, 197, 768]        [1, 197, 768]        2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [1, 197, 768]        [1, 197, 768]        4,723,968            True
├─Sequential (classifier)                                              [1, 768]             [1, 3]               --                   True
│    └─LayerNorm (0)                                                   [1, 768]             [1, 768]             1,536                True
│    └─Linear (1)                                                      [1, 768]             [1, 3]               2,307                True
======================================================================================================================================================
Total params: 85,800,963
Trainable params: 85,800,963
Non-trainable params: 0
Total mult-adds (M): 172.47
======================================================================================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 102.88
Params size (MB): 257.55
Estimated Total Size (MB): 361.03
======================================================================================================================================================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span> # Number of parameters in pretrained ViT
 num_params = 85,800,963
 num_params
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(85, 800, 963)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="setting-up-training-code-for-our-custom-vit">
<h2>9. Setting up training code for our custom ViT<a class="headerlink" href="#setting-up-training-code-for-our-custom-vit" title="Permalink to this heading">#</a></h2>
<p>We’ve replicated the ViT architecture, now let’s see how it performs on our FoodVision Mini data.</p>
<section id="creating-an-optimizer">
<h3>9.1 Creating an optimizer<a class="headerlink" href="#creating-an-optimizer" title="Permalink to this heading">#</a></h3>
<p>The paper states it uses the Adam optimizer (section 4, Training &amp; fine-tuning) with <span class="math notranslate nohighlight">\(B1\)</span> value of 0.9, <span class="math notranslate nohighlight">\(B2\)</span> of 0.999 (defaults) and a weight decay of 0.1.</p>
<p>Weight decay = Weight decay is a regularization technique by adding a small penalty, usually the L2 norm of the weights (all the weights of the model), to the loss function.</p>
<p>Regularization technique = prevents overfitting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># vit
</pre></div>
</div>
</div>
</div>
</section>
<section id="creating-a-loss-function">
<h3>9.2 Creating a loss function<a class="headerlink" href="#creating-a-loss-function" title="Permalink to this heading">#</a></h3>
<p>The ViT paper doesn’t actually mention what loss function they used.</p>
<p>So since it’s a multi-class classification we’ll use the <code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss()</span></code>.</p>
</section>
<section id="training-our-vit-model">
<h3>9.3 Training our ViT Model<a class="headerlink" href="#training-our-vit-model" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>device
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cuda&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from going_modular.going_modular import engine

set_seeds()

optimizer = torch.optim.Adam(vit.parameters(), 
                             lr=1e-3,
                             betas=(0.9, 0.999),
                             weight_decay=0.1)

loss_fn = torch.nn.CrossEntropyLoss()

results = engine.train(model=vit,
                       train_dataloader=train_dataloader,
                       test_dataloader=test_dataloader,
                       epochs=10,
                       optimizer=optimizer,
                       loss_fn=loss_fn,
                       device=device)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "5cd792d85ff1491394316c87b2e931f8", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 4.9009 | train_acc: 0.2969 | test_loss: 1.0362 | test_acc: 0.5417
Epoch: 2 | train_loss: 1.5928 | train_acc: 0.2773 | test_loss: 1.5662 | test_acc: 0.1979
Epoch: 3 | train_loss: 1.4393 | train_acc: 0.2617 | test_loss: 1.2755 | test_acc: 0.1979
Epoch: 4 | train_loss: 1.2928 | train_acc: 0.2891 | test_loss: 1.6844 | test_acc: 0.1979
Epoch: 5 | train_loss: 1.2678 | train_acc: 0.2852 | test_loss: 1.7159 | test_acc: 0.2604
Epoch: 6 | train_loss: 1.1954 | train_acc: 0.4102 | test_loss: 1.9489 | test_acc: 0.1979
Epoch: 7 | train_loss: 1.1835 | train_acc: 0.4062 | test_loss: 3.0747 | test_acc: 0.1979
Epoch: 8 | train_loss: 1.3447 | train_acc: 0.4180 | test_loss: 1.9179 | test_acc: 0.2604
Epoch: 9 | train_loss: 1.5294 | train_acc: 0.2383 | test_loss: 1.4440 | test_acc: 0.5417
Epoch: 10 | train_loss: 1.4364 | train_acc: 0.3359 | test_loss: 1.2803 | test_acc: 0.2604
</pre></div>
</div>
</div>
</div>
</section>
<section id="what-our-training-setup-is-missing">
<h3>9.4 What our training setup is missing<a class="headerlink" href="#what-our-training-setup-is-missing" title="Permalink to this heading">#</a></h3>
<p>How is our training setup different to the ViT paper?</p>
<p>We’ve replicated model archirecture correctly.</p>
<p>But what was different between our training procedure (to get such poor results) and the ViT paper training procedure to get such great results?</p>
<p>The main things our training implementation is missing:</p>
<p>Prevent underfitting:</p>
<ul class="simple">
<li><p>Data - our setup uses far less data (225 vs millions)</p></li>
</ul>
<p>Prevent overfitting:</p>
<ul class="simple">
<li><p>Learning rate warmup - start with a low learning rate and increase to a base LR</p></li>
<li><p>Learning rate decay - as your model gets closer to convergence, start to lower the learning rate</p></li>
<li><p>Gradient clipping - prevent gradients from getting too big</p></li>
</ul>
<p>Search “pytorch [technique name]”</p>
</section>
<section id="plotting-loss-curves-for-our-model">
<h3>9.5 Plotting loss curves for our model<a class="headerlink" href="#plotting-loss-curves-for-our-model" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from helper_functions import plot_loss_curves

plot_loss_curves(results) 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/f7181ce3ac0015a3ac1e7843ee41f0b1af73de30f2105e9e0d2c39024547f16b.png" src="../../_images/f7181ce3ac0015a3ac1e7843ee41f0b1af73de30f2105e9e0d2c39024547f16b.png" />
</div>
</div>
<p>Hmm it looks like our model is underfitting and overfitting… I wonder what techniques we could use to take care of both at the same time?</p>
<p>See more here: <a class="reference external" href="https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like">https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like</a></p>
</section>
</section>
<section id="using-a-pretrained-vit-from-torchvision-models">
<h2>10. Using a pretrained ViT from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code><a class="headerlink" href="#using-a-pretrained-vit-from-torchvision-models" title="Permalink to this heading">#</a></h2>
<p>Generally, in deep learning if you can use a pretrained model from a large dataset on your own problem, it’s often a good place to start.</p>
<p>If you can find a pretrained model and use transfer learning, give it a go, it often achieves great results with little data.</p>
<section id="why-use-a-pretrained-model">
<h3>10.1 Why use a pretrained model?<a class="headerlink" href="#why-use-a-pretrained-model" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Sometimes data is limited</p></li>
<li><p>Limited training resources</p></li>
<li><p>Get better results faster (sometimes)…</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Cost of a TPUv3 for 30 days
cost = 30*24*8
print(f&quot;Cost of renting a TPUv3 for 30 straight days: ${cost}USD&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost of renting a TPUv3 for 30 straight days: $5760USD
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># The following requires torch v0.12+ and torchvision 0.13+
import torch
import torchvision
print(torch.__version__)
print(torchvision.__version__)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.12.0+cu113
0.13.0+cu113
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
device
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cuda&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="prepare-a-pretrained-vit-for-use-with-foodvision-mini-turn-it-into-a-feature-extractor">
<h3>10.2 Prepare a pretrained ViT for use with FoodVision Mini (turn it into a feature extractor)<a class="headerlink" href="#prepare-a-pretrained-vit-for-use-with-foodvision-mini-turn-it-into-a-feature-extractor" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get pretrained weights for ViT-Base
pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # &quot;DEFAULT&quot; = best available

# Setup a ViT model instance with pretrained weights
pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)

# Freeze the base parameters
for parameter in pretrained_vit.parameters():
  parameter.requires_grad = False

# Update the classifier head
set_seeds()
pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from torchinfo import summary

summary(model=pretrained_vit,
        input_size=(1, 3, 224, 224), # (batch_size, color_channels, height, width)
        col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],
        col_width=20,
        row_settings=[&quot;var_names&quot;])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>======================================================================================================================================================
Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable
======================================================================================================================================================
VisionTransformer (VisionTransformer)                                  [1, 3, 224, 224]     [1, 3]               768                  Partial
├─Conv2d (conv_proj)                                                   [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False
├─Encoder (encoder)                                                    [1, 197, 768]        [1, 197, 768]        151,296              False
│    └─Dropout (dropout)                                               [1, 197, 768]        [1, 197, 768]        --                   --
│    └─Sequential (layers)                                             [1, 197, 768]        [1, 197, 768]        --                   False
│    │    └─EncoderBlock (encoder_layer_0)                             [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_1)                             [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_2)                             [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_3)                             [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_4)                             [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_5)                             [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_6)                             [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_7)                             [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_8)                             [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_9)                             [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_10)                            [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_11)                            [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    └─LayerNorm (ln)                                                  [1, 197, 768]        [1, 197, 768]        (1,536)              False
├─Linear (heads)                                                       [1, 768]             [1, 3]               2,307                True
======================================================================================================================================================
Total params: 85,800,963
Trainable params: 2,307
Non-trainable params: 85,798,656
Total mult-adds (M): 172.47
======================================================================================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 104.09
Params size (MB): 257.55
Estimated Total Size (MB): 362.24
======================================================================================================================================================
</pre></div>
</div>
</div>
</div>
</section>
<section id="preparing-data-for-the-pretrained-vit-model">
<h3>10.3 Preparing data for the pretrained ViT model<a class="headerlink" href="#preparing-data-for-the-pretrained-vit-model" title="Permalink to this heading">#</a></h3>
<p>When using a pretrained model, you want to make sure your data is formatted in the same way that the model was trained on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get automtic transforms from pretrained ViT weights
vit_transforms = pretrained_vit_weights.transforms()
vit_transforms
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ImageClassification(
    crop_size=[224]
    resize_size=[256]
    mean=[0.485, 0.456, 0.406]
    std=[0.229, 0.224, 0.225]
    interpolation=InterpolationMode.BILINEAR
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train_dir, test_dir
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(PosixPath(&#39;data/pizza_steak_sushi/train&#39;),
 PosixPath(&#39;data/pizza_steak_sushi/test&#39;))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Setup dataloaders
from going_modular.going_modular import data_setup
train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,
                                                                                                     test_dir=test_dir,
                                                                                                     transform=vit_transforms,
                                                                                                     batch_size=32) # could set a higher batch size because using a pretrained model 
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-feature-extractor-vit-model">
<h3>10.4 Train feature extractor ViT model<a class="headerlink" href="#train-feature-extractor-vit-model" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from going_modular.going_modular import engine

# Create optimizer and loss function
optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),
                             lr=1e-3)
loss_fn = torch.nn.CrossEntropyLoss()

# Train the classifier head of pretrained ViT
set_seeds() 
pretrained_vit_results = engine.train(model=pretrained_vit,
                                      train_dataloader=train_dataloader_pretrained,
                                      test_dataloader=test_dataloader_pretrained,
                                      optimizer=optimizer,
                                      loss_fn=loss_fn,
                                      epochs=10,
                                      device=device)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "481d3b11bde044c1acc198d87e36ff99", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 0.7665 | train_acc: 0.7227 | test_loss: 0.5432 | test_acc: 0.8665
Epoch: 2 | train_loss: 0.3428 | train_acc: 0.9453 | test_loss: 0.3263 | test_acc: 0.8977
Epoch: 3 | train_loss: 0.2064 | train_acc: 0.9531 | test_loss: 0.2707 | test_acc: 0.9081
Epoch: 4 | train_loss: 0.1556 | train_acc: 0.9570 | test_loss: 0.2422 | test_acc: 0.9081
Epoch: 5 | train_loss: 0.1246 | train_acc: 0.9727 | test_loss: 0.2279 | test_acc: 0.8977
Epoch: 6 | train_loss: 0.1216 | train_acc: 0.9766 | test_loss: 0.2129 | test_acc: 0.9280
Epoch: 7 | train_loss: 0.0938 | train_acc: 0.9766 | test_loss: 0.2352 | test_acc: 0.8883
Epoch: 8 | train_loss: 0.0797 | train_acc: 0.9844 | test_loss: 0.2281 | test_acc: 0.8778
Epoch: 9 | train_loss: 0.1098 | train_acc: 0.9883 | test_loss: 0.2074 | test_acc: 0.9384
Epoch: 10 | train_loss: 0.0650 | train_acc: 0.9883 | test_loss: 0.1804 | test_acc: 0.9176
</pre></div>
</div>
</div>
</div>
</section>
<section id="plot-the-loss-curves-of-our-pretrained-vit-feature-extractor-model">
<h3>10.5 Plot the loss curves of our pretrained ViT feature extractor model<a class="headerlink" href="#plot-the-loss-curves-of-our-pretrained-vit-feature-extractor-model" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from helper_functions import plot_loss_curves

plot_loss_curves(pretrained_vit_results)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/a125becb66551dba518baf9d12f01641f34bc8b50894f1dfa10cac55ceb8fd4f.png" src="../../_images/a125becb66551dba518baf9d12f01641f34bc8b50894f1dfa10cac55ceb8fd4f.png" />
</div>
</div>
</section>
<section id="save-our-best-performing-vit-model">
<h3>10.6 Save our best performing ViT model<a class="headerlink" href="#save-our-best-performing-vit-model" title="Permalink to this heading">#</a></h3>
<p>Now we’ve got a model that performs quite well, how about we save it to file and then check it’s filesize.</p>
<p>We want to check the filesize because if we wanted to deploy a model to say a website/mobile application, we may limitations on the size of the model we can deploy.</p>
<p>E.g. a smaller model may be required due to compute restrictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Save the model
from going_modular.going_modular import utils

utils.save_model(model=pretrained_vit,
                 target_dir=&quot;models&quot;,
                 model_name=&quot;08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] Saving model to: models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from pathlib import Path

# Get the model size in bytes then convert to megabytes 
pretrained_vit_model_size = Path(&quot;models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth&quot;).stat().st_size // (1024*1024)
print(f&quot;Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Pretrained ViT feature extractor model size: 327 MB
</pre></div>
</div>
</div>
</div>
<p>Our pretrained ViT gets some of the best results we’ve seen so far on our FoodVision Mini problem, however, the model size is ~11x larger than our next best performing model.</p>
<p>Perhaps the larger model size might cause issues when we go to deploy it (e.g. hard to deploy such a large file/might not make predictions as fast as a smaller model).</p>
</section>
</section>
<section id="predicting-on-a-custom-image">
<h2>11. Predicting on a custom image<a class="headerlink" href="#predicting-on-a-custom-image" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import requests

# Import function to make predictions on images and plot them 
from going_modular.going_modular.predictions import pred_and_plot_image

# Setup custom image path
custom_image_path = image_path / &quot;04-pizza-dad.jpeg&quot;

# Download the image if it doesn&#39;t already exist
if not custom_image_path.is_file():
    with open(custom_image_path, &quot;wb&quot;) as f:
        # When downloading from GitHub, need to use the &quot;raw&quot; file link
        request = requests.get(&quot;https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg&quot;)
        print(f&quot;Downloading {custom_image_path}...&quot;)
        f.write(request.content)
else:
    print(f&quot;{custom_image_path} already exists, skipping download.&quot;)

# Predict on custom image
pred_and_plot_image(model=pretrained_vit,
                    image_path=custom_image_path,
                    class_names=class_names)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data/pizza_steak_sushi/04-pizza-dad.jpeg...
</pre></div>
</div>
<img alt="../../_images/c4a6c36955e7b9081ff2df386d47ec9acdbe19a41d3d3dcd31c5ff96d21e19b1.png" src="../../_images/c4a6c36955e7b9081ff2df386d47ec9acdbe19a41d3d3dcd31c5ff96d21e19b1.png" />
</div>
</div>
</section>
<section id="exercises-and-extra-curriculum">
<h2>Exercises and extra-curriculum<a class="headerlink" href="#exercises-and-extra-curriculum" title="Permalink to this heading">#</a></h2>
<p>See exercises and extra-curriculum here: <a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#exercises">https://www.learnpytorch.io/08_pytorch_paper_replicating/#exercises</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./classical_network/vision_transformer"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-setup">0. Get setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-data">1. Get data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-datasets-and-dataloaders">2. Create Datasets and DataLoaders</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-a-single-a-image">2.3 Visualize a single a image</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#replicating-vit-overview">3. Replicating ViT: Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vit-overview-pieces-of-the-puzzle">3.1 ViT overview: pieces of the puzzle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#figure-1">Figure 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#four-equations">Four equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-1-describes-the-various-equations">Section 3.1 describes the various equations:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#table-1">Table 1</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding">4. Equation 1: Split data into patches and creating the class, position and patch embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-input-and-output-shapes-by-hand">4.1 Calculate input and output shapes by hand</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turning-a-single-image-into-patches">4.2 Turning a single image into patches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-image-patches-and-turning-them-into-patch-embeddings">4.3 Creating image patches and turning them into patch embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flattening-the-patch-embedding-with-torch-nn-flatten">4.4 Flattening the patch embedding with <code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turning-the-vit-patch-embedding-layer-into-a-pytorch-module">4.5 Turning the ViT patch embedding layer into a PyTorch module</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-class-token-embedding">4.6 Creating the class token embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-position-embedding">4.7 Creating the position embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together-from-image-to-embedding">4.8 Putting it all together: from image to embedding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-2-multihead-self-attention-msa-block">Equation 2: Multihead Self-Attention (MSA block)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-3-multilayer-perceptron-mlp-block">6. Equation 3: Multilayer Perceptron (MLP block)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-transformer-encoder">7. Creating the Transformer Encoder</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-custom-transformer-encoder-block">7.1 Create a custom Transformer Encoder block</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-transformer-encoder-layer-with-in-built-pytorch-layers">7.2 Create a Transformer Encoder layer with in-built PyTorch layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together-to-create-vit">8. Putting it all together to create ViT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-visual-summary-of-our-vit-model">8.1 Getting a visual summary of our ViT model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-training-code-for-our-custom-vit">9. Setting up training code for our custom ViT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-an-optimizer">9.1 Creating an optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-loss-function">9.2 Creating a loss function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-our-vit-model">9.3 Training our ViT Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-our-training-setup-is-missing">9.4 What our training setup is missing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-loss-curves-for-our-model">9.5 Plotting loss curves for our model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-a-pretrained-vit-from-torchvision-models">10. Using a pretrained ViT from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-a-pretrained-model">10.1 Why use a pretrained model?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-a-pretrained-vit-for-use-with-foodvision-mini-turn-it-into-a-feature-extractor">10.2 Prepare a pretrained ViT for use with FoodVision Mini (turn it into a feature extractor)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-data-for-the-pretrained-vit-model">10.3 Preparing data for the pretrained ViT model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-feature-extractor-vit-model">10.4 Train feature extractor ViT model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-loss-curves-of-our-pretrained-vit-feature-extractor-model">10.5 Plot the loss curves of our pretrained ViT feature extractor model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-our-best-performing-vit-model">10.6 Save our best performing ViT model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-on-a-custom-image">11. Predicting on a custom image</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises-and-extra-curriculum">Exercises and extra-curriculum</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>